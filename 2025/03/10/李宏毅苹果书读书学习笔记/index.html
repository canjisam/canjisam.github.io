<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>李宏毅苹果书读书学习笔记 | 诒森的博客</title><meta name="author" content="CanJisam"><meta name="copyright" content="CanJisam"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#FFFFFF"><meta name="description" content="学习目标：  Task 1 《深度学习详解》- 1.1 通过案例了解机器学习  Task 2 《深度学习详解》- 1.2 了解线性模型  Task 3 《深度学习详解》- 2 机器学习框架&amp;实践攻略   学习内容： 欢迎去大家各大电商平台选购纸质版苹果书《深度学习详解》基于上述书籍拓展   引用内容为书本原话 图片基本上来源于书中我以自问自答的方式输出内容   Task 1 通过案例了解机">
<meta property="og:type" content="article">
<meta property="og:title" content="李宏毅苹果书读书学习笔记">
<meta property="og:url" content="https://canjisam.github.io/2025/03/10/%E6%9D%8E%E5%AE%8F%E6%AF%85%E8%8B%B9%E6%9E%9C%E4%B9%A6%E8%AF%BB%E4%B9%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/index.html">
<meta property="og:site_name" content="诒森的博客">
<meta property="og:description" content="学习目标：  Task 1 《深度学习详解》- 1.1 通过案例了解机器学习  Task 2 《深度学习详解》- 1.2 了解线性模型  Task 3 《深度学习详解》- 2 机器学习框架&amp;实践攻略   学习内容： 欢迎去大家各大电商平台选购纸质版苹果书《深度学习详解》基于上述书籍拓展   引用内容为书本原话 图片基本上来源于书中我以自问自答的方式输出内容   Task 1 通过案例了解机">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://canjisam.github.io/img/cover/9f13ef7ffc274ea1974a8c4bd3849b26_2.png">
<meta property="article:published_time" content="2025-03-10T02:29:35.000Z">
<meta property="article:modified_time" content="2025-03-10T15:34:57.898Z">
<meta property="article:author" content="CanJisam">
<meta property="article:tag" content="CanJisam, 前端, 后端, 架构, 运维, 数据库, 算法, 面试, 博客">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://canjisam.github.io/img/cover/9f13ef7ffc274ea1974a8c4bd3849b26_2.png"><script type="application/ld+json">{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "李宏毅苹果书读书学习笔记",
  "url": "https://canjisam.github.io/2025/03/10/%E6%9D%8E%E5%AE%8F%E6%AF%85%E8%8B%B9%E6%9E%9C%E4%B9%A6%E8%AF%BB%E4%B9%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/",
  "image": "https://canjisam.github.io/img/cover/9f13ef7ffc274ea1974a8c4bd3849b26_2.png",
  "datePublished": "2025-03-10T02:29:35.000Z",
  "dateModified": "2025-03-10T15:34:57.898Z",
  "author": [
    {
      "@type": "Person",
      "name": "CanJisam",
      "url": "https://canjisam.github.io/"
    }
  ]
}</script><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://canjisam.github.io/2025/03/10/%E6%9D%8E%E5%AE%8F%E6%AF%85%E8%8B%B9%E6%9E%9C%E4%B9%A6%E8%AF%BB%E4%B9%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#000000')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#FFFFFF')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":false,"top_n_per_article":1,"unescape":false,"languages":{"hits_empty":"未找到符合您查询的内容：${query}","hits_stats":"共找到 ${hits} 篇文章"}},
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":false,"highlightMacStyle":false},
  copy: {
    success: '复制成功',
    error: '复制失败',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyloadPlugin: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '李宏毅苹果书读书学习笔记',
  isHighlightShrink: false,
  isToc: true,
  pageType: 'post'
}</script><script>window.XF_API_KEY = "33ec326f4348794e214e464ff2990f32";
window.XF_API_SECRET = "NDQ3NGYyZWMwZWQ0ZWQ4ZWUzMTMwY2Y4";</script><link rel="stylesheet" href="/css/custom-music.css"><meta name="generator" content="Hexo 7.3.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src="/img/cover/avator.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data text-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">57</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">101</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">29</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/music/"><i class="fa-fw fas fa-music"></i><span> 音乐</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-user"></i><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url(/img/cover/9f13ef7ffc274ea1974a8c4bd3849b26_2.png);"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">诒森的博客</span></a><a class="nav-page-title" href="/"><span class="site-name">李宏毅苹果书读书学习笔记</span></a></span><div id="menus"><div id="search-button"><span class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></span></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/music/"><i class="fa-fw fas fa-music"></i><span> 音乐</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-user"></i><span> 关于</span></a></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav><div id="post-info"><h1 class="post-title">李宏毅苹果书读书学习笔记</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2025-03-10T02:29:35.000Z" title="发表于 2025-03-10 10:29:35">2025-03-10</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2025-03-10T15:34:57.898Z" title="更新于 2025-03-10 23:34:57">2025-03-10</time></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">总字数:</span><span class="word-count">22.7k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span class="min2read">81 分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-commentcount"><i class="far fa-comments fa-fw post-meta-icon"></i><span class="post-meta-label">评论数:</span><a href="/2025/03/10/%E6%9D%8E%E5%AE%8F%E6%AF%85%E8%8B%B9%E6%9E%9C%E4%B9%A6%E8%AF%BB%E4%B9%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/#post-comment"><span class="valine-comment-count" data-xid="/2025/03/10/%E6%9D%8E%E5%AE%8F%E6%AF%85%E8%8B%B9%E6%9E%9C%E4%B9%A6%E8%AF%BB%E4%B9%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"></span><span class="waline-comment-count" id="/2025/03/10/%E6%9D%8E%E5%AE%8F%E6%AF%85%E8%8B%B9%E6%9E%9C%E4%B9%A6%E8%AF%BB%E4%B9%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"></span><span class="twikoo-count"></span><span class="fb-comments-count"></span><span class="gitalk-comment-count"></span></a></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><h1 id="学习目标："><a href="#学习目标：" class="headerlink" title="学习目标："></a>学习目标：</h1><ul>
<li><input checked="" disabled="" type="checkbox"> Task 1 《深度学习详解》- 1.1 通过案例了解机器学习</li>
<li><input checked="" disabled="" type="checkbox"> Task 2 《深度学习详解》- 1.2 了解线性模型</li>
<li><input checked="" disabled="" type="checkbox"> Task 3 《深度学习详解》- 2 机器学习框架&amp;实践攻略</li>
</ul>
<hr>
<h1 id="学习内容："><a href="#学习内容：" class="headerlink" title="学习内容："></a>学习内容：</h1><blockquote>
<p>欢迎去大家各大电商平台选购纸质版苹果书《深度学习详解》<br>基于上述书籍拓展</p>
</blockquote>
<blockquote>
<p>引用内容为书本原话 图片基本上来源于书中<br>我以自问自答的方式输出内容</p>
</blockquote>
<hr>
<h1 id="Task-1-通过案例了解机器学习"><a href="#Task-1-通过案例了解机器学习" class="headerlink" title="Task 1 通过案例了解机器学习"></a>Task 1 通过案例了解机器学习</h1><hr>
<h2 id="机器学习（Machine-Learning，ML）和深度学习（Deep-Learning，DL）的基本概念"><a href="#机器学习（Machine-Learning，ML）和深度学习（Deep-Learning，DL）的基本概念" class="headerlink" title="机器学习（Machine Learning，ML）和深度学习（Deep Learning，DL）的基本概念"></a>机器学习（Machine Learning，ML）和深度学习（Deep Learning，DL）的基本概念</h2><blockquote>
<p>什么是机器学习</p>
</blockquote>
<p>人工智能的一个分支。机器学习范畴比人工智能概念略小，深度学习的底层是神经网络。机器学习是指用计算机模拟人类学习行为的的技术用来从已知的数据中获取新的知识。</p>
<blockquote>
<p>机器学习，顾名思义，<strong>机器具备有学习的能力</strong>。具体来讲，机器学习就是让机器<strong>具备找一个函数的能力</strong>。机器具备找函数的能力以后，它可以做很多事。</p>
</blockquote>
<blockquote>
<p>比如语音识别，机器听一段声音，产生这段声音对应的文字。我们需要的是一个函数，该函数的输入是声音信号，输出是这段声音信号的内容。</p>
</blockquote>
<p> 就是让机器的输入映射到某个函数之后可以得到输出</p>
<h2 id="什么是回归（regression）"><a href="#什么是回归（regression）" class="headerlink" title="什么是回归（regression）"></a>什么是回归（regression）</h2><blockquote>
<p><strong>随着要找的函数不同，机器学习有不同的类别</strong>。假设要找的函数的<strong>输出是一个数值，一个标量（scalar）</strong>，这种机器学习的任务称为回归</p>
</blockquote>
<blockquote>
<p>机器要找一个<strong>函数 f</strong>，其输入是可能是<strong>种种跟预测 PM2.5 有关的指数</strong>，包括今天的 PM2.5 的数值、平均温度、平均的臭氧浓度等等，<strong>输出是明天中午的 PM2.5的数值</strong>,找这个函数的任务称为回归（regression）</p>
</blockquote>
<p>机器要找一个函数f(x)，其输入是可能是与预测目标有关的数值x，输出是对于下一次的预测值f(x)，找这个函数的任务称为回归（regression）。</p>
<blockquote>
<p><strong>隐藏任务①：</strong> 找出本篇中形如回归（regression）加粗字体的术语，并用自己的话进行解释，列成表格，与学习群的其他小伙伴讨论你的理解和搜索到的相关案例</p>
</blockquote>
<table>
<thead>
<tr>
<th>术语</th>
<th>解释</th>
</tr>
</thead>
<tbody><tr>
<td>分类</td>
<td>将数据划分为多个离散的类别的任务，预测输入的样本所属的类别</td>
</tr>
<tr>
<td>回归</td>
<td>通过对输入数据进行学习，建立一个连续的函数关系，预测数值型的输出结果</td>
</tr>
<tr>
<td>机器学习</td>
<td>一种从数据中自动学习模式和模型的方法，使计算机能够根据之前的经验来进行预测或决策</td>
</tr>
<tr>
<td>深度学习</td>
<td>一种机器学习的子领域，通过模拟人脑的神经网络结构，对大规模数据进行学习和表达复杂模式</td>
</tr>
<tr>
<td>损失</td>
<td>衡量预测的输出与实际值之间的差异的函数，用于评估模型的训练效果</td>
</tr>
<tr>
<td>梯度下降</td>
<td>一种优化算法，通过反复迭代的方式，沿着目标函数的负梯度方向调整模型参数的值，以最小化损失函数</td>
</tr>
</tbody></table>
<h2 id="什么是分类（classification）"><a href="#什么是分类（classification）" class="headerlink" title="什么是分类（classification）"></a>什么是分类（classification）</h2><blockquote>
<p>分类任务要让机器<strong>做选择题。<strong>人类先</strong>准备好一些选项，这些选项称为类别（class）</strong>，现在要找的函数的输出就是从设定好的选项里面<strong>选择一个当作输出，该任务称为分类。</strong><br>举个例子，每个人都有邮箱账户，邮箱账户里面有一个函数，该函数可以检测一封邮件是否为垃圾邮件。<strong>分类不一定只有两个选项，也可以有多个选项。</strong></p>
</blockquote>
<p>根据某些特征把不同数据分成不同的类别。</p>
<h2 id="什么是结构化学习"><a href="#什么是结构化学习" class="headerlink" title="什么是结构化学习"></a>什么是结构化学习</h2><blockquote>
<p>机器不只是要做选择题或输出一个数字，而是产生一个有结构的物体，比如让机器画一张图，写一篇文章。这种叫机器产生有结构的东西的问题称为结构化学习。</p>
</blockquote>
<p>就是根据输入的东西的某种规律生产某种相似结构的东西</p>
<h2 id="机器学习找函数的三个步骤"><a href="#机器学习找函数的三个步骤" class="headerlink" title="机器学习找函数的三个步骤"></a>机器学习找函数的三个步骤</h2><blockquote>
<p>隐藏任务③：找出机器学习找函数的3个步骤！并查找资料，交叉佐证这些步骤。</p>
</blockquote>
<blockquote>
<p>机器学习找函数的过程，分成3个步骤。</p>
</blockquote>
<h3 id="第1个步骤是写出一个带有未知参数的函数f，其能预测未来观看次数。"><a href="#第1个步骤是写出一个带有未知参数的函数f，其能预测未来观看次数。" class="headerlink" title="第1个步骤是写出一个带有未知参数的函数f，其能预测未来观看次数。"></a>第1个步骤是写出一个带有未知参数的函数<code>f</code>，其能预测未来观看次数。</h3><blockquote>
<p>y &#x3D; b + w ∗ x1，而 b 跟 w 是未知的。<br><strong>带有未知的参数（parameter）的函数称为模型（model）。</strong><br>模型在机器学习里面，就是一个带有未知的参数的函数，特征（feature）  $x_1$ 是这个函数里面已知的，它是来自于后台的信息，2 月 25 日点击的总次数是已知的，而 w 跟 b 是未知的参数。<br><strong>w 称为权重（weight），b 称为偏置（bias）。</strong></p>
</blockquote>
<h3 id="第2个步骤是定义损失（loss），损失也是一个函数。"><a href="#第2个步骤是定义损失（loss），损失也是一个函数。" class="headerlink" title="第2个步骤是定义损失（loss），损失也是一个函数。"></a>第2个步骤是定义损失（loss），损失也是一个函数。</h3><blockquote>
<p>估测的值跟实际的值之间的差距，其实有不同的计算方法，计算 y 与 yˆ 之间绝对值的差距，如式 (1.6) 所示，称为平均绝对误差（Mean Absolute Error，MAE）</p>
<p><img src="/img/downloaded/aHR0cHM6_33d3ffa7ce6d47cbacd0f5fcc1211984.png" alt="在这里插入图片描述"></p>
</blockquote>
<blockquote>
<p>如果算 y 与 yˆ 之间平方的差距，如式 (1.7) 所示，则称为均方误差（Mean SquaredError，MSE）。<br><img src="/img/downloaded/aHR0cHM6_8882f24dbc85463d87b8b1ac62bf69e2.png" alt="在这里插入图片描述"></p>
<p>有一些任务中 y 和 yˆ 都是概率分布，这个时候可能会选择<strong>交叉熵</strong>（cross entropy），这个是机器学习的第 2 步。</p>
</blockquote>
<p><strong>交叉熵</strong>是信息论中用来度量两个概率分布之间差异的一种方法。在机器学习中，<strong>交叉熵</strong>经常被用来作为损失函数，用来度量预测结果与真实结果之间的差异。</p>
<p>对于分类问题，<strong>交叉熵</strong>可以用来度量预测结果的概率分布与真实结果的概率分布之间的差异。<strong>交叉熵</strong>的计算公式如下：</p>
<p>$$H(p,q) &#x3D; -∑ p(x) * log(q(x))$$</p>
<p>其中，<code>p(x)</code>表示真实结果的概率分布，<code>q(x)</code>表示预测结果的概率分布。</p>
<p><strong>交叉熵</strong>的值越小，表示预测结果与真实结果越接近，模型的性能也越好。因此，通过最小化<strong>交叉熵</strong>，可以优化模型的预测能力。</p>
<p>在深度学习中，<strong>交叉熵</strong>通常作为损失函数与激活函数一起使用，用来训练神经网络模型。通过反向传播算法，可以根据<strong>交叉熵</strong>的值来调整模型的参数，使得模型的预测结果与真实结果更加接近。</p>
<h3 id="机器学习的第-3-步：解一个最优化的问题。"><a href="#机器学习的第-3-步：解一个最优化的问题。" class="headerlink" title="机器学习的第 3 步：解一个最优化的问题。"></a>机器学习的第 3 步：解一个最优化的问题。</h3><blockquote>
<p>找一个 <code>w</code> 跟 <code>b</code>，把未知的参数找一个数值出来，看代哪一个数值进去可以让损失 L 的值最小，就是要找的 <code>w</code> 跟 <code>b</code>，这个可以让损失最小的 <code>w</code> 跟 <code>b</code> 称为 <code>w∗</code> 跟 <code>b∗</code> 代表它们是最好的一组 <code>w</code> 跟 <code>b</code>，可以让损失的值最小。</p>
</blockquote>
<p><strong>梯度下降</strong>（gradient descent）是经常会使用优化的方法。</p>
<blockquote>
<p>试了不同的参数，计算它的损失，画出来的等高线图称为误差表面（error surface）。<br>在这个等高线图上面，越偏红色系，代表计算出来的损失越大，就代表这一组 w 跟 b 越差。如果越偏蓝色系，就代表损失越小，就代表这一组 w 跟 b 越好，拿这一组 w 跟 b，放到函数里面，预测会越精准。<img src="/img/downloaded/aHR0cHM6_e19efe52835c4b9696362974134bfb53.png" alt="在这里插入图片描述"></p>
</blockquote>
<blockquote>
<p><strong>学习率（learning rate）η 也会影响步伐大小</strong>。<br>学习率是自己设定的，如果 η 设大一点，每次参数更新就会量大，学习可能就比较快。如果 η 设小一点，参数更新就很慢，每次只会改变一点点参数的数值。<br><strong>这种在做机器学习，需要自己设定，不是机器自己找出来的，称为超参数（hyperparameter）。</strong></p>
</blockquote>
<h2 id="为什么损失可以是负的？"><a href="#为什么损失可以是负的？" class="headerlink" title="为什么损失可以是负的？"></a>为什么损失可以是负的？</h2><p><img src="/img/downloaded/aHR0cHM6_54e8e9e426724a82a147d4b0b957959b.png" alt="在这里插入图片描述"></p>
<h2 id="梯度下降有一个很大的问题"><a href="#梯度下降有一个很大的问题" class="headerlink" title="梯度下降有一个很大的问题"></a>梯度下降有一个很大的问题</h2><blockquote>
<p><strong>梯度下降</strong>有一个很大的问题，没有找到真正最好的解，没有找到可以让损失最小的 w。<br>在图 1.4 所示的例子里面，把 w 设定在最右侧红点附近这个地方可以让损失最小。但如果在<strong>梯度下降</strong>中，$w^0$ 是随机初始的位置，也很有可能走到 wT 这里，训练就停住了，无法再移动 w 的位置。右侧红点这个位置是真的可以让损失最小的地方，称为全局最小值（global minima），而 wT 这个地方称为局部最小值（local minima），其左右两边都比这个地方的损失还要高一点，但是它不是整个误差表面上面的最低点。</p>
</blockquote>
<p><img src="/img/downloaded/aHR0cHM6_5ef2f64926e348afbbbeea25588742c7.png" alt="在这里插入图片描述"><br>推广到多参数(w,b)的话</p>
<blockquote>
<p>假设有两个参数，随机初始值为 $w^0$, $b^0$。要计算 w, b 跟损失的微分，计算在 w &#x3D; $w^0$ 的位置，b &#x3D; $b^0$ 的位置，要计算 w 对 L 的微分，计算 b 对 L 的微分计算完后更新 w 跟 b，把 $w^0$ 减掉学习率乘上微分的结果得到 ，把 $b^0$ 减掉学习率乘上微分的结果得到 $b^1$。</p>
</blockquote>
<p><img src="/img/downloaded/aHR0cHM6_a7b5317aebaf45f89cd8ec94c022e6f4.png" alt="在这里插入图片描述"></p>
<p><img src="/img/downloaded/aHR0cHM6_3452064a661c4ec0b08939074d5e2262.png"></p>
<blockquote>
<p>就是反复同样的步骤，就不断的更新 <code>w</code> 跟 <code>b</code>，期待最后，可以找到一个最好的 <code>w</code>，w∗ 跟最好的 <code>b∗</code>. 如图 1.5 所示，随便选一个初始的值，先计算一下 <code>w</code> 对 L 的微分，跟计算一下 <code>b</code> 对 L 的微分，接下来更新 <code>w</code> 跟 <code>b</code>，更新的方向就是 <code>∂L/∂w</code>，乘以 η 再乘以一个负号，∂L&#x2F;∂b，算出这个微分的值，就可以决定更新的方向，可以决定 <code>w</code> 要怎么更新。把 <code>w</code> 跟 <code>b</code> 更新的方向结合起来，就是一个向量，就是红色的箭头，再计算一次微分，再决定要走什么样的方向，把这个微分的值乘上学习率，再乘上负号，我们就知道红色的箭头要指向那里，就知道如何移动 <code>w</code> 跟 <code>b</code> 的位置，一直移动，期待最后可以找出一组不错的 <code>w</code>, <code>b</code>。</p>
</blockquote>
<h1 id="Task-2-了解线性模型"><a href="#Task-2-了解线性模型" class="headerlink" title="Task 2  了解线性模型"></a>Task 2  了解线性模型</h1><h2 id="线性模型"><a href="#线性模型" class="headerlink" title="线性模型"></a>线性模型</h2><p>书中举例了一个预测观看人数的例子 </p>
<blockquote>
<p>每隔 7 天它一个循环，如果一个模型参考前 7 天的数据，把 7天前的数据，直接复制到拿来当作预测的结果，也许预测的会更准也说不定，所以我们就要修改一下模型。通常一个模型的修改，往往来自于对这个问题的理解，即<strong>领域知识</strong>。</p>
</blockquote>
<p>机器学习领域的领域知识是指机器学习算法、技术和应用方面的专业知识。<br>包括机器学习算法、数据预处理、特征工程、模型评估和选择的知识。</p>
<blockquote>
<p>这些模型都是把输入的特征 x 乘上一个权重，再加上一个偏置就得到预测的结果，这样的模型称为<strong>线性模型（linear model）</strong>。</p>
</blockquote>
<h3 id="分段线性曲线"><a href="#分段线性曲线" class="headerlink" title="分段线性曲线"></a>分段线性曲线</h3><blockquote>
<p>红色的曲线可以看作是一个常数再加上一群 Hard <code>Sigmoid </code>函数。Hard <code>Sigmoid </code>函数的特性是当输入的值，当 x 轴的值小于某一个阈值（某个定值）的时候，大于另外一个定值阈值的时候，中间有一个斜坡。所以它是先水平的，再斜坡，再水平的。所以红色的线可以看作是一个常数项加一大堆的蓝色函数（Hard Sigmoid）。常数项设成红色的线跟 x 轴的交点一样大。</p>
</blockquote>
<p><img src="/img/downloaded/aHR0cHM6_3b3eda53be35441bb2e996c427b449c2.png" alt="在这里插入图片描述"></p>
<blockquote>
<p>常数项怎么加上蓝色函数后，变成红色的这一条线?</p>
<ol>
<li>蓝线 1 函数斜坡的起点，设在红色函数的起始的地方，第 2 个斜坡的终点（最高点）|（第 1 个蓝色函数斜坡的终点） 设在第一个转角处，让第 1 个蓝色函数的斜坡和红色函数的斜坡的斜率是一样的，这个时候把 线0+线1 就可以得到红色曲线左侧的线段。</li>
<li>再加第 2 个蓝色的函数，所以第2 个蓝色函数的斜坡就在红色函数的第一个转折点到第 2 个转折点之间，让第 2 个蓝色函数的斜率跟红色函数的斜率一样，这个时候把 线0+线1+线2，就可以得到红色函数左侧和中间的线段。</li>
<li>接下来第 3 个部分，第 2 个转折点之后的部分，就加第 3 个蓝色的函数，第 3 个蓝色的函数坡度的起始点设的跟红色函数转折点一样，蓝色函数的斜率设的跟红色函数斜率一样</li>
<li>接下来把 线0+线1+线2+线3全部加起来，就得到完整红色的线。<br>(线0、线1、线2、线3 为图1.8中线段）</li>
</ol>
</blockquote>
<p><img src="/img/downloaded/aHR0cHM6_e394f591a7d44639bf1f9ec056473cba.png" alt="在这里插入图片描述"></p>
<blockquote>
<p>所以红色线，即分段线性曲线（piecewise linear curve）可以看作是一个常数，再加上一堆蓝色的函数。</p>
</blockquote>
<p>大量不同的蓝色函数，加上一个常数以后就可以组出任意的分段线性曲线。<br>如果分段线性曲线越复杂，转折的点越多，所需的蓝色函数就越多。<br>反之，越多蓝色函数的话可以组成越复杂的分段线性曲线。</p>
<p><strong>可以在这样的曲线（图1.9）上面，先取一些点并连起来变成一个分段线性曲线。这个分段线性曲线跟非常接近原来的曲线，如果点取的够多或点取的位置适当，分段线性曲线就可以逼近这一个连续的曲线。</strong> </p>
<p><img src="/img/downloaded/aHR0cHM6_85ca75a8ba474157864729b726d34f20.png" alt="在这里插入图片描述"></p>
<blockquote>
<p>所以可以用分段线性曲线去逼近任何的连续的曲线，而每个分段线性曲线都可以用一大堆蓝色的函数组合起来。也就是说，只要有足够的蓝色函数把它加起来，就可以变成任何连续的曲线。</p>
</blockquote>
<p>我们可以用任意多的蓝色函数来模拟出曲线。<br>极限的思路来看：就是只有取得足够多的点并且相连接，就可以无限多的直线代替曲线。</p>
<h3 id="如何表示方程"><a href="#如何表示方程" class="headerlink" title="如何表示方程"></a>如何表示方程</h3><p><img src="/img/downloaded/aHR0cHM6_9db4a3d0ece343c4be8e5a2c18926e4a.png" alt="在这里插入图片描述"></p>
<blockquote>
<p>如果 x1 的值，趋近于无穷大的时候，e−(b+wx1) 这一项就会消失，当 x1 非常大的时候，这一条就会收敛在高度为 c 的地方。如果 x1 负的非常大的时候，分母的地方就会非常大，y的值就会趋近于 0。<br>所以可以用这样子的一个函数逼近这一个蓝色的函数，即 <code>Sigmoid </code>函数，<code>Sigmoid </code>函数就是 S 型的函数。<br>因为它长得是有点像是 S 型，所以叫它 <code>Sigmoid </code>函数。为了简洁，去掉了指数的部分，蓝色函数的表达式为</p>
</blockquote>
<p>$$y &#x3D; cσ(b + wx1) (1.15)$$</p>
<blockquote>
<p>所以可以用 <code>Sigmoid </code>函数逼近 Hard <code>Sigmoid </code>函数。</p>
</blockquote>
<p>$$<br>y &#x3D; \frac{c}{ 1+ e^{-(b+wx1)}}<br>$$</p>
<blockquote>
<p><strong>调整这里的 <code>b</code>、<code>w </code>和 <code>c</code> 可以制造各种不同形状的 <code>Sigmoid </code>函数，</strong> 用各种不同形状的 Sigmoid函数去逼近 Hard <code>Sigmoid </code>函数。</p>
</blockquote>
<blockquote>
<p>如图 1.11 所示，如果改 w，就会改变斜率，就会改变斜坡的坡度。如果改了 <code>b</code>，就可以把这一个 <code>Sigmoid </code>函数左右移动；如果改 <code>c</code>，就可以改变它的高度。所以只要有不同的 <code>w </code>不同的 <code>b</code> 不同的 <code>c</code>，就可以制造出不同的 <code>Sigmoid </code>函数，把不同的<code>Sigmoid </code>函数叠起来以后就可以去逼近各种不同的分段线性函数；分段线性函数可以拿来近似各种不同的连续的函数。</p>
</blockquote>
<p><img src="/img/downloaded/aHR0cHM6_f9f5ee0a313944d1bacb1b9e1d549fe2.png" alt="在这里插入图片描述"><br><img src="/img/downloaded/aHR0cHM6_a8f6973d038c490aa5f324f92c587580.png" alt="在这里插入图片描述"></p>
<blockquote>
<p>我们可以不只用一个特征<code> x1</code>，可以用多个特征代入不同的 <code>c, b, w</code>，组合出各种不同的函数，从而得到更有 <strong>灵活性（flexibility）</strong> 的函数，如图 1.13 所示。<br>用 <code>j </code>来代表特征的编号。如果要考虑前 28 天，<code>j </code>就是 1 到 28。</p>
</blockquote>
<p><img src="/img/downloaded/aHR0cHM6_a69bac4cc1c7482cadf10b21208428b5.png" alt="在这里插入图片描述"></p>
<blockquote>
<p>无论是拿行或拿列都可以，把 W 的每一列或每一行“拼”成一个长的向量，把 b, cT, b” 拼” 上来，这个长的向量直接用 θ 来表示。<br>所有的未知的参数，一律统称 θ。</p>
</blockquote>
<h4 id="优化是找一个可以让损失最小的参数，是否可以穷举所有可能的未知参数的值？"><a href="#优化是找一个可以让损失最小的参数，是否可以穷举所有可能的未知参数的值？" class="headerlink" title="优化是找一个可以让损失最小的参数，是否可以穷举所有可能的未知参数的值？"></a>优化是找一个可以让损失最小的参数，是否可以穷举所有可能的未知参数的值？</h4><p><img src="/img/downloaded/aHR0cHM6_7c50b0a3c97849b0ac539dbe34230d54.png" alt="在这里插入图片描述"></p>
<h4 id="刚才的例子里面有-3-个-Sigmoid，为什么是-3-个，能不能-4-个或更多？"><a href="#刚才的例子里面有-3-个-Sigmoid，为什么是-3-个，能不能-4-个或更多？" class="headerlink" title="刚才的例子里面有 3 个 Sigmoid，为什么是 3 个，能不能 4 个或更多？"></a>刚才的例子里面有 3 个 Sigmoid，为什么是 3 个，能不能 4 个或更多？</h4><p><img src="/img/downloaded/aHR0cHM6_5136ea488dba48da88cb96625968b6a2.png" alt="在这里插入图片描述"></p>
<h3 id="定义损失"><a href="#定义损失" class="headerlink" title="定义损失"></a>定义损失</h3><blockquote>
<p>之前是 <code>L(w, b)</code>，因为 w 跟 b 是未知的。<br>现在未知的参数很多了，再把它一个一个列出来太累了，所以直接用 θ 来统设所有的参数，所以损失函数就变成 <code>L(θ)</code>。</p>
</blockquote>
<blockquote>
<p>损失函数能够判断 <code>θ</code> 的好坏，其计算方法跟刚才只有两个参数的时候是一样的。<br>先给定 <code>θ</code> 的值，即某一组 <code>W, b, cT, b</code> 的值，再把一种特征 <code>x</code> 代进去，得到估测出来的 <code>y</code>，再计算一下跟真实的标签之间的误差 <code>e</code>。把所有的误差通通加起来，就得到损失。</p>
</blockquote>
<p><img src="/img/downloaded/aHR0cHM6_8ab5780e47174a3580be5690eb786de1.png" alt="在这里插入图片描述"><br><img src="/img/downloaded/aHR0cHM6_60bf1b0bfb3e4f319f3766d50d674c37.png" alt="在这里插入图片描述"></p>
<blockquote>
<p>要找到 $θ$让损失越小越好，可以让<br>损失最小的一组 $θ$称为 $θ_∗$。一开始要随机选一个初始的数值 $θ_0$。<br>接下来计算每一个未知的参数对 L 的微分，得到向量 $g$，即可以让损失变低的函数</p>
</blockquote>
<p><img src="/img/downloaded/aHR0cHM6_5aa8c959c29b4a72bfedbd274b00b887.png" alt="在这里插入图片描述"></p>
<blockquote>
<p>假设有 1000 个参数，这个向量的长度就是 1000，这个向量也称为梯度，$∇L$代表梯度。<br>L($θ_0$) 是指计算梯度的位置，是在 θ 等于 $θ_0$ 的地方。<br>计算出 g 后，接下来跟新参数，$θ_0$ 代表它是一个起始的值，它是一个随机选的起始的值，代表 $θ_1$ 更新过一次的结果，$θ^0_2$ 减掉微分乘以，减掉 η 乘上微分的值，得到 $θ^1_2$，以此类推，就可以把 1000 个参数都更新了。</p>
</blockquote>
<p><img src="/img/downloaded/aHR0cHM6_92abe1a1c1114610b3bbc58d68a7c638.png" alt="在这里插入图片描述"></p>
<blockquote>
<p>假设参数有 1000 个，$θ_0$ 就是 1000 个数值，1000 维的向量，g 是 1000 维的向量，$θ_1$ 也是 1000 维的向量。 整个操作就是这样，由 $θ_0$ 算梯度，根据梯度去把 $θ_0$ 更新成 $θ_1$，再算一次梯度，再根据梯度把 $θ_1$ 再更新成 $θ_2$，再算一次梯度把 $θ_2$ 更新成 $θ_3$，以此类推，直到不想做。<br><img src="/img/downloaded/aHR0cHM6_fdd986e113434fbcbd1d5c835d558c5f.png" alt="在这里插入图片描述"></p>
</blockquote>
<blockquote>
<p>或者计算出梯度为 0 向量，导致无法再更新参数为止，不过在实现上几乎不太可能梯度为 0，通常会停下来就是我们不想做了。</p>
</blockquote>
<h3 id="实现上的细节"><a href="#实现上的细节" class="headerlink" title="实现上的细节"></a>实现上的细节</h3><h4 id="批量（batch）"><a href="#批量（batch）" class="headerlink" title="批量（batch）"></a>批量（batch）</h4><p><img src="/img/downloaded/aHR0cHM6_6c121abd25e74f5da606124044251c7c.png" alt="在这里插入图片描述"></p>
<blockquote>
<p>实现上有个细节的问题，实际使用梯度下降的时候，如图 1.17 所示，会把 N 笔数据随机分成一个一个的<strong>批量（batch）</strong>，一组一组的。</p>
</blockquote>
<p>在深度学习中，<strong>批量（Batch）</strong> 指的是计算一次<strong>成本（cost）</strong> 需要的输入数据个数。当数据集比较大时，一次性处理所有样本在计算和存储上会有困难，因此会采用一次输入一定量的样本来进行训练。</p>
<p><strong>如果数据集比较小，可以将全体数据看做一个批量，即把数据集中每个样本都计算损失（loss）然后取其平均值当做成本（cost）。</strong></p>
<p><strong>批量学习的优点</strong>：能更好地代表样本总体从而更准确地确定下降方向，对梯度向量有更精确的估计等。</p>
<h4 id="回合（epoch）"><a href="#回合（epoch）" class="headerlink" title="回合（epoch）"></a>回合（epoch）</h4><blockquote>
<p>把所有的批量都看过一次，称为一个回合（epoch），每一次更新参数叫做一次更新。更新跟回合是不同的东西。每次更新一次参数叫做一次更新，把所有的批量都看过一遍，叫做一个回合。</p>
</blockquote>
<p><strong>回合（Epoch）</strong> 指的是遍历全部数据集一次。<br>在一个回合中，模型会对数据集中的所有样本都进行处理和学习。</p>
<h2 id="模型变形"><a href="#模型变形" class="headerlink" title="模型变形"></a>模型变形</h2><blockquote>
<p>其实还可以对模型做更多的变形，不一定要把 Hard Sigmoid 换成 Soft Sigmoid。</p>
</blockquote>
<p><img src="/img/downloaded/aHR0cHM6_fafd73da66564321890309726de67aab.png" alt="在这里插入图片描述"></p>
<blockquote>
<p><strong>HardSigmoid 可以看作是两个修正线性单元（Rectifed Linear Unit， ReLU） 的加总， ReLU 的图像有一个水平的线，走到某个地方有一个转折的点，变成一个斜坡，</strong> 其对应的公式为</p>
</blockquote>
<p>$$<br>c ∗ max(0, b + wx1)<br>$$</p>
<p><strong>输出0或b+ w1为正的。</strong></p>
<p><img src="/img/downloaded/aHR0cHM6_03de17282ec048f2863b7ab58a81f6b3.png" alt="在这里插入图片描述"></p>
<blockquote>
<p><strong>把两个 ReLU 叠起来就可以变成 Hard 的 Sigmoid</strong>，想要用 ReLU，就把 Sigmoid 的地方，换成</p>
</blockquote>
<p>$$max(0, b_i + w_{ij}x_{j})$$</p>
<blockquote>
<p>要<strong>合成 i 个 Hard Sigmoid， 需要 i 个 Sigmoid，如果 ReLU 要做到一样的事情，则需要 2i 个 ReLU</strong>，因为 2 个 ReLU 合 起来才是一个 Hard Sigmoid。因此<strong>表示一个 Hard 的 Sigmoid 不是只有一种做法</strong>。</p>
</blockquote>
<h3 id="激活函数（activation-function）"><a href="#激活函数（activation-function）" class="headerlink" title="激活函数（activation function）"></a>激活函数（activation function）</h3><blockquote>
<p>在机器学习里面， <strong>Sigmoid 或 ReLU 称为激活函数（activation function）</strong>。</p>
</blockquote>
<p><img src="/img/downloaded/aHR0cHM6_9b6073c8277646fcb7a0ef7fad256122.png" alt="在这里插入图片描述"></p>
<blockquote>
<p>Sigmoid 跟 ReLU 是最常见的激活函数，接下来的实<br>验都选择用了 ReLU，显然 ReLU 比较好，实验结果如图 1.20 所示。</p>
</blockquote>
<blockquote>
<p><strong>连续使用 10 个 ReLU作为模型，跟用线性模型的结果是差不多的</strong></p>
</blockquote>
<blockquote>
<p>但连续使用 100 个 ReLU 作为模型，结果就有显著差别了， 100 个 ReLU 在训练数据上的损失就可以从 320 降到 280，有 100 个 ReLU 就可以制造比较复杂的曲线，本来线性就是一直线，但 100 个 ReLU 就可以产生 100 个折线的函数，在测试数据上也好了一些。<br>接下来使用 1000 个 ReLU 作为模型，<strong>在训练数据上损失更低了一些，但是在没看过的数据上，损失没有变化</strong>。</p>
</blockquote>
<p><strong>Sigmoid 跟 ReLU 是最常见的激活函数</strong></p>
<blockquote>
<p>继续改模型</p>
</blockquote>
<p><img src="/img/downloaded/aHR0cHM6_4a5b5c0de372450c85459dfc8abbcad7.png" alt="在这里插入图片描述"></p>
<blockquote>
<p>如图 1.21 所示，<strong>从 x 变成 a，就是把 x 乘上 w 加 b，再通过 Sigmoid 函数</strong>。</p>
</blockquote>
<blockquote>
<p><strong>不一定要通过Sigmoid 函数</strong>，通过 ReLU 也可以得到 a，同样的事情再<strong>反复地多做几次</strong>。 所以可以把 x 做这一连串的运算产生 a，接下来把 a做这一连串的运算产生 a′。 反复地多做的次数又是另外一个超参数。<br><strong>注意， w, b 和 w′, b′ 不是同一个参数，是增加了更多 的未知的参数。</strong></p>
</blockquote>
<h3 id="深度学习"><a href="#深度学习" class="headerlink" title="深度学习"></a>深度学习</h3><p><img src="/img/downloaded/aHR0cHM6_eb720854b37f44019699e122c965429c.png" alt="在这里插入图片描述"></p>
<blockquote>
<p>如图 1.24 所示， <strong>Sigmoid 或 ReLU 称为神经元（neuron），很多的神经元称为神经网络（neural network）</strong>。<br>  <strong>每一排称为一层，称为隐藏层（hiddenlayer），很多的隐藏层就“深”，这套技术称为深度学习</strong>。</p>
</blockquote>
<blockquote>
<p>人们<strong>把神经网络越叠越多越叠越深</strong><br>残差网络（Residual Network， ResNet） 有 152 层，错误率降到 3.57%。</p>
</blockquote>
<p><img src="/img/downloaded/aHR0cHM6_5d9dd3323adf44ab9bdab196dc56227c.png" alt="在这里插入图片描述"></p>
<p>如图 1.25 所示。在训练数据和测试数据上的结果是不一致的，这种情况称为<strong>过拟合（overftting）</strong>。</p>
<h1 id="Task-3-机器学习框架-实践攻略"><a href="#Task-3-机器学习框架-实践攻略" class="headerlink" title="Task 3 机器学习框架&amp;实践攻略"></a>Task 3 机器学习框架&amp;实践攻略</h1><h2 id="机器学习框架"><a href="#机器学习框架" class="headerlink" title="机器学习框架"></a>机器学习框架</h2><h3 id="定义函数fθ-x"><a href="#定义函数fθ-x" class="headerlink" title="定义函数fθ(x)"></a>定义函数fθ(x)</h3><blockquote>
<p><strong>定义一个函数$f_θ(x)$，其中θ表示模型中的所有未知参数</strong>。该函数接收输入特征x，并根据参数θ计算输出。</p>
</blockquote>
<h3 id="定义损失函数"><a href="#定义损失函数" class="headerlink" title="定义损失函数"></a>定义损失函数</h3><blockquote>
<p><strong>定义一个损失函数，用于评估给定参数组合θ的好坏程度</strong>。损失函数的选择依赖于具体的问题和模型类型。常见的损失函数包括均方误差、交叉熵等。</p>
</blockquote>
<h3 id="优化问题求解"><a href="#优化问题求解" class="headerlink" title="优化问题求解"></a>优化问题求解</h3><blockquote>
<p>的目标是寻找一个最优的参数组合θ∗，<strong>使得损失函数的值最小化</strong>。这可以通过求解一个优化问题来实现。常用的优化算法包括梯度下降、牛顿法等。</p>
</blockquote>
<h3 id="应用于测试数据"><a href="#应用于测试数据" class="headerlink" title="应用于测试数据"></a>应用于测试数据</h3><blockquote>
<p><strong>找到最优参数θ∗后，可以将其应用于测试数据</strong>。将测试集中的输入特征x带入函数$f_θ(x)$，得到预测结果。</p>
</blockquote>
<h3 id="提交到Kaggle进行评估"><a href="#提交到Kaggle进行评估" class="headerlink" title="提交到Kaggle进行评估"></a>提交到Kaggle进行评估</h3><blockquote>
<p>将预测结果提交到Kaggle等竞赛平台进行评估。该平台会根据预测结果与真实值之间的差异进行评分，以衡量模型的性能。</p>
</blockquote>
<p>总结一下就是<strong>定义一个函数</strong>$f_θ(x)$，其中θ代表模型中的未知参数。然后，<strong>定义一个损失函数</strong>来评估参数组合的好坏程度。然后，通过<strong>优化问题求解</strong>找到最优参数<code>θ∗</code>，<strong>使损失函数最小化</strong>。然后，将<strong>最优参数应用于测试数据，得到预测结果</strong>。最后，将预测结果<strong>提交到评估平台</strong>进行性能评估。</p>
<h2 id="实践方法论"><a href="#实践方法论" class="headerlink" title="实践方法论"></a>实践方法论</h2><h3 id="为什么会出现模型偏差"><a href="#为什么会出现模型偏差" class="headerlink" title="为什么会出现模型偏差"></a>为什么会出现模型偏差</h3><blockquote>
<p><strong>模型偏差可能会影响模型训练。</strong></p>
</blockquote>
<blockquote>
<p>假设<strong>模型过于简单</strong>，一个有未知参数的函数代$θ_1$ 得到一个函数$f_θ1(x)$，同理可得到另一个函数 $f_θ2(x)$，把所有的函数集合起来得到一个函数的集合。但是该函数的集合太小了，没有包含任何一个函数，<strong>可以让损失变低的函数不在模型可以描述的范围内</strong>。</p>
</blockquote>
<p>如何解决</p>
<ol>
<li><strong>用深度学习，增加更多的灵活性</strong>。</li>
<li>所以如果模型的灵活性不够大，可以<strong>增加更多特征，可以设一个更大的模型</strong>，可以用深度学习来增加模型的灵活性，这是第一个可以的解法。</li>
<li>但是并不是训练的时候，损失大就代表一定是模型偏差，可能会遇到另外一个问题：<strong>优化做得不好</strong>。</li>
</ol>
<h3 id="优化问题"><a href="#优化问题" class="headerlink" title="优化问题"></a>优化问题</h3><blockquote>
<p><strong>一般只会用到梯度下降进行优化，这种优化的方法很多的问题。</strong></p>
</blockquote>
<blockquote>
<p>比如可能会卡在局部最小值的地方，无法找到一个真的可以让损失很低的参数，如图 2.3(a) 所示。如图 2.3(b) 所示蓝色部分是模型可以表示的函数所形成的集合，可以把 θ 代入不同的数值，形成不同的函数，把所有的函数通通集合在一起，得到这个蓝色的集合。这个蓝色的集合里面，确实包含了一些函数，这些函数它的损失是低的。<br><strong>但问题是梯度下降这一个算法无法找出损失低的函数，梯度下降是解一个优化的问题，找到 θ∗ 就结束了。但 θ∗ 的损失不够低。</strong></p>
</blockquote>
<p><img src="/img/downloaded/aHR0cHM6_97e4040032b74ea6aa21954b977d5936.png" alt="在这里插入图片描述"><br>还是可能会出现卡在局部最小值的地方,仍未能找到真正的最优解</p>
<h3 id="如何判断模型是否足够大？"><a href="#如何判断模型是否足够大？" class="headerlink" title="如何判断模型是否足够大？"></a>如何判断模型是否足够大？</h3><blockquote>
<p>一个建议判断的方法，通过比较不同的模型来判断模型现在到底够不够大。</p>
</blockquote>
<p> <img src="/img/downloaded/aHR0cHM6_2f14529755384bbbb10acdd98529b589.png" alt="在这里插入图片描述"></p>
<blockquote>
<p>很多人看到这张图认为这个代表过拟合，深度学习不奏效， 56 层太深了不奏效，根本就不需要这么深。但个不是过拟合，并不是所有的结果不好，都叫做过拟合。在训练集上， 20 层的网络损失其实是比较低的， 56 层的网络损失是比较高的，如图 2.4(b) 所示，这代表 56 层的网络的优化没有做好，它的优化不给力。</p>
</blockquote>
<p>层数多但是反而效果不好，不一定是过拟合，可能是因为它的优化没有做好。</p>
<blockquote>
<p>看到一个从来没有做过的问题，可以<strong>先跑一些比较小的、比较浅的网络，或甚至用一些非深度学习的方法，比如线性模型、支持向量机（Support Vector Machine，SVM）</strong>， SVM 可能是比较容易做优化的，它们比较不会有优化失败的问题。</p>
</blockquote>
<p>对于一个新的问题可以多种不同的模型综合考量一下。<br>先跑一个小的模型试一下，在逐步加深模型。</p>
<h3 id="过拟合"><a href="#过拟合" class="headerlink" title="过拟合"></a>过拟合</h3><h3 id="为什么会有过拟合这样的情况呢？"><a href="#为什么会有过拟合这样的情况呢？" class="headerlink" title="为什么会有过拟合这样的情况呢？"></a>为什么会有过拟合这样的情况呢？</h3><blockquote>
<p>举一个极端的例子，这是训练集。</p>
</blockquote>
<blockquote>
<p>假设根据这些训练集，某一个很废的机器学习的方法找出了一个一无是处的函数。这个一无是处的函数，只要输入 x 有出现在训练集里面，就把它对应的 y 当做输出。<strong>如果 x 没有出现在训练集里面，就输出一个随机的值。这个函数啥事也没有干，其是一个一无是处的函数，但它在训练数据上的损失是 0</strong>。把训练数据通通丢进这个函数里面，它的输出跟训练集的标签是一模一样的，<strong>所以在训练数据上面，这个函数的损失可是 0 呢，可是在测试数据上面，它的损失会变得很大，因为它其实什么都没有预测，这是一个比较极端的例子，在一般的情况下，也有可能发生类似的事情</strong>。</p>
</blockquote>
<h3 id="灵活性太大带来的问题"><a href="#灵活性太大带来的问题" class="headerlink" title="灵活性太大带来的问题"></a>灵活性太大带来的问题</h3><p><img src="/img/downloaded/aHR0cHM6_7aad017e757746fabb21ac0ff5ac08a2.png" alt="在这里插入图片描述"></p>
<blockquote>
<p>如图 2.6 所示，举例来说，假设输入的特征为 x，输出为 y， x 和 y 都是一维的。<br>x 和 y之间的关系是 2 次的曲线，<strong>曲线用虚线来表示，因为通常没有办法，直接观察到这条曲线</strong>。我们<strong>真正可以观察到的是训练集，训练集可以想像成从这条曲线上面，随机采样出来的几个点</strong>。</p>
</blockquote>
<blockquote>
<p>模型的能力非常的强，其灵活性很大，只给它这 3 个点。在这 3 个点上面，<strong>要让损失低，所以模型的这个曲线会通过这 3 个点，但是其它没有训练集做为限制的地方，因为它的灵活性很大，它灵活性很大，所以模型可以变成各式各样的函数，没有给它数据做为训练，可以产生各式各样奇怪的结果</strong>。</p>
</blockquote>
<blockquote>
<p><strong>如果再丢进测试数据，测试数据和训练数据，当然不会一模一样，它们可能是从同一个分布采样出来的，测试数据是橙色的点，训练数据是蓝色的点</strong>。</p>
</blockquote>
<h3 id="如何解决过拟合问题"><a href="#如何解决过拟合问题" class="headerlink" title="如何解决过拟合问题"></a>如何解决过拟合问题</h3><ol>
<li><p>增加数据集，数据增强<br><img src="/img/downloaded/aHR0cHM6_6710dfbe85484bd7a8324512e3cf0da5.png" alt="在这里插入图片描述"></p>
</li>
<li><p>给模型一些限制，让模型不要有过大的灵活性。</p>
<blockquote>
<p>如图 2.8 所示，要用多限制的模型才会好取决于对这个问题的理解。因为这种模型是自己设计的，设计出不同的模型，结果不同。解决过拟合的问题，要给模型一些限制，最好模型正好跟背后产生数据的过程，过程是一样的就有机会得到好的结果。</p>
</blockquote>
</li>
</ol>
<blockquote>
<p><strong>如果是深度学习的话，就给它比较少的神经元的数量，本来每层一千个神经元，改成一百个神经元之类的，或者让模型共用参数，可以让一些参数有一样的数值</strong>。 </p>
</blockquote>
<blockquote>
<p><strong>全连接网络（fully-connected network） 其实是一个比较有灵活性的架构，而卷积神经网络（Convolutional Neural Network， CNN） 是一个比较有限制的架构。</strong> <strong>CNN 是一种比较没有灵活性的模型，其是针对图像的特性来限制模型的灵活性</strong>。所以全连接神经网络，可以找出来的函数所形成的集合其实是比较大的， CNN 所找出来的函数，它形成的集合其实是比较小的，其实包含在全连接网络里面的，但是就是因为<strong>CNN 给了，比较大的限制，所以 CNN 在图像上，反而会做得比较好</strong>。</p>
</blockquote>
<p><img src="/img/downloaded/aHR0cHM6_9ba7dbf22dff42b2925ff53350d9e853.png" alt="在这里插入图片描述"><br>但也不要给太多的限制。有可能会因为模型太大的限制，大到有了模型偏差的问题。</p>
<ol start="3">
<li>比如早停（early stopping）、正则化（regularization）和丢弃法（dropout<br>method）。</li>
</ol>
<h3 id="交叉验证"><a href="#交叉验证" class="headerlink" title="交叉验证"></a>交叉验证</h3><blockquote>
<p>一种比较合理的选择模型的方法是将训练数据分成训练集和验证集，通常是将90%的数据用作训练集，剩余的10%作为验证集。训练集用于训练模型，验证集用于评估模型的性能。<br>在训练集上训练出的模型会使用验证集来衡量模型的分数。根据验证集上的分数选择最佳的模型，并将该模型的结果上传到Kaggle等平台上得到公开分数。<br>这个过程中，使用验证集来选择模型，因此公开测试集的分数可以反映私人测试集的分数。然而，如果这个过程重复太多次，根据公开测试集的结果调整模型太多次，就有可能在公开测试集上过拟合，导致在私人测试集上得到差的结果。</p>
</blockquote>
<p><img src="/img/downloaded/aHR0cHM6_d85545a530ca472686fe86b8d3ead1a0.png" alt="在这里插入图片描述"><br><strong>因此，需要在选择模型时找到平衡，避免过度拟合，并注意不要过多地根据公开测试集的结果调整模型。</strong></p>
<p><strong>在选择模型时，常用的方法是将训练数据分成训练集和验证集</strong>。训练集用于训练模型，验证集用于评估模型在未知数据上的性能。通常情况下，将大约90%的数据用作训练集，剩余的10%作为验证集。<strong>模型在验证集上的表现可以作为选择模型的依据</strong>。</p>
<p>然而，<strong>在选择模型的过程中需要注意，过多地根据公开测试集上的结果调整模型可能会导致在私人测试集上得到较差的结果，即过拟合的问题</strong>。因此，在选择模型时<strong>需要找到一个平衡，避免过度拟合模型</strong>。</p>
<blockquote>
<p><strong>最好的做法，就是用验证损失，最小的直接挑就好了，不要管公开测试集的结果</strong>。<br>在实现上，不太可能这么做，因为公开数据集的结果对模型的选择，可能还是会有些影响的。<strong>理想上就用验证集挑就好，有过比较好的基线（baseline） 算法以后，就不要再去动它了，就可以避免在测试集上面过拟合</strong>。</p>
</blockquote>
<h3 id="k-折交叉验证"><a href="#k-折交叉验证" class="headerlink" title="k 折交叉验证"></a>k 折交叉验证</h3><p><img src="/img/downloaded/aHR0cHM6_40d78d2dae2a46c5b8e7549197f66e83.png" alt="在这里插入图片描述"></p>
<blockquote>
<p>用 k 折交叉验证（k-fold cross validation），如图 2.11 所示。 k 折交叉验证就是先把训练集切成 k 等份。在这个例子，训练集被切成 3 等份，切完以后，拿其中一份当作验证集，另外两份当训练集，这件事情要重复 3 次。即第一份第 2 份当训练，第 3 份当验证；第一份第 3 份当训练，第 2 份当验证；第一份当验证，第 2 份第 3 份当训练</p>
</blockquote>
<p><img src="/img/downloaded/aHR0cHM6_7490bb6b430d460ca345df01aae6ba22.png" alt="在这里插入图片描述"></p>
<h3 id="不匹配"><a href="#不匹配" class="headerlink" title="不匹配"></a>不匹配</h3><p>反常的情况。这种情况应该算是另外一种错误的形式，这种错误的形式称为不匹配（mismatch）</p>
<p><img src="/img/downloaded/aHR0cHM6_1afc40b560d74bc99a4a5bc881d075ea.png" alt="在这里插入图片描述"></p>
<blockquote>
<p>不匹配跟过拟合其实不同，一般的过拟合可以用搜集更多的数据来克服，但是<strong>不匹配是指训练集跟测试集的分布不同，训练集再增加其实也没有帮助了</strong>。</p>
</blockquote>
<blockquote>
<p>增加数据也不能让模型做得更好，所以这种问题要怎么解决，<strong>匹不匹配要看对数据本身的理解了，我们可能要对训练集跟测试集的产生方式有一些理解，才能判断它是不是遇到了不匹配的情况</strong></p>
</blockquote>
<p><img src="/img/downloaded/aHR0cHM6_86d38b681e1a42ba93e6d7dddc1247d9.png" alt="在这里插入图片描述"></p>
<h1 id="🚩学习目标"><a href="#🚩学习目标" class="headerlink" title="🚩学习目标"></a>🚩学习目标</h1><ul>
<li><input checked="" disabled="" type="checkbox"> Task 1.1 《深度学习详解》3.1 局部极小值与鞍点</li>
<li><input checked="" disabled="" type="checkbox"> Task 1.2 《深度学习详解》3.2 批量和动量</li>
<li><input checked="" disabled="" type="checkbox"> Task 2.1 《深度学习详解》3.3&amp;4&amp;5 自适应学习率</li>
<li><input checked="" disabled="" type="checkbox"> Task 2.2 《深度学习详解》3.6 分类</li>
<li><input checked="" disabled="" type="checkbox"> Task 2.3 （实践任务）：HW3(CNN)</li>
<li><input checked="" disabled="" type="checkbox"> Task 3.1 《深度学习详解》3.7 批量归一化</li>
<li><input checked="" disabled="" type="checkbox"> Task 3.2 《深度学习详解》4.1&amp;2&amp;3&amp;4 卷积神经网络-上</li>
<li><input checked="" disabled="" type="checkbox"> Task 3.3 《深度学习详解》3.5&amp;6&amp;7&amp;8 卷积神经网络-下</li>
<li><input checked="" disabled="" type="checkbox"> （选修）《深度学习详解》6.1&amp;2 自注意力机制的原理</li>
</ul>
<hr>
<h1 id="🚩学习内容"><a href="#🚩学习内容" class="headerlink" title="🚩学习内容"></a>🚩学习内容</h1><blockquote>
<p>欢迎去大家各大电商平台选购纸质版苹果书《深度学习详解》<br>基于上述书籍拓展</p>
</blockquote>
<blockquote>
<p>引用内容为书本原话 图片基本上来源于书中<br>我以自问自答的方式输出内容</p>
</blockquote>
<hr>
<h1 id="🚩-Task1-1"><a href="#🚩-Task1-1" class="headerlink" title="🚩 Task1.1"></a>🚩 Task1.1</h1><hr>
<h2 id="🎯为什么优化会失败"><a href="#🎯为什么优化会失败" class="headerlink" title="🎯为什么优化会失败"></a>🎯为什么优化会失败</h2><blockquote>
<p>收敛在局部极限值与鞍点会导致优化失败。</p>
</blockquote>
<p>隐藏任务①：搜索资料，找到一个优化失败的案例，尝试用自己的话描述一遍情况~<br><a target="_blank" rel="noopener" href="https://www.jiqizhixin.com/articles/2018-11-07-4">深度学习并非万能，遇到这些问题会失败 | 机器之心</a></p>
<h3 id="📌因非信息梯度导致的失败"><a href="#📌因非信息梯度导致的失败" class="headerlink" title="📌因非信息梯度导致的失败"></a>📌因非信息梯度导致的失败</h3><p>原因：如果梯度中的信息很少，使用它来进行学习就无法成功。<br>例如，研究者从学习随机奇偶校验的简单问题开始，在大约d&#x3D;30这个程度之后，经过合理时间后也没有观察到优于随机的表现。<br>研究者使用两个定理对此进行了详细的分析，得出了结论：<strong>基于梯度的方法确实不能学会随机奇偶校验和线性周期函数。</strong> 此外，不管我们使用哪一类预测算法，只要使用了基于梯度的方法来进行训练，这个结果都成立。</p>
<h2 id="🎯局部极小值与鞍点"><a href="#🎯局部极小值与鞍点" class="headerlink" title="🎯局部极小值与鞍点"></a>🎯局部极小值与鞍点</h2><blockquote>
<p>我们在做优化的时候经常会发现，随着参数不断更新，训练的损失不会再下降, 但是我们对这个损失仍然不满意。’</p>
</blockquote>
<p>达到了临界点</p>
<h2 id="🎯临界点及其种类"><a href="#🎯临界点及其种类" class="headerlink" title="🎯临界点及其种类"></a>🎯临界点及其种类</h2><blockquote>
<p>过去常见的一个猜想是我们优化到某个地方，这个地方参数对损失的微分为零，如图 3.1所示。图 3.1 中的两条曲线对应两个神经网络训练的过程。当参数对损失微分为零的时候，梯度下降就不能再更新参数了，训练就停下来了，损失不再下降了。</p>
</blockquote>
<p><img src="/img/downloaded/aHR0cHM6_35ef82053d4743e1bf98cd23406436ba.png" alt="在这里插入图片描述"></p>
<blockquote>
<p>提到梯度为零的时候，大家最先想到的可能就是<strong>局部极小值（local minimum）</strong><br>所以经常<strong>有人说，做深度学习时使用梯度下降会收敛在局部极小值，梯度下降不起作用。</strong> 但其实损失不是只在局部极小值的梯度是零，还有其他可能会让梯度是零的点，比如<strong>鞍点（saddle point）</strong> 。<strong>鞍点其实就是梯度是零且区别于局部极小值和局部极大值</strong>（localmaximum）的点。图 3.2b 红色的点在 y 轴方向是比较高的，在 x 轴方向是比较低的，这就是一个鞍点。<strong>鞍点的叫法是因为其形状像马鞍。</strong> <strong>鞍点的梯度为零，但它不是局部极小值。</strong> <strong>我们把梯度为零的点统称为临界点（critical point）。</strong> 损失没有办法再下降，也许是<strong>因为收敛在了临界点，但不一定收敛在局部极小值，因为鞍点也是梯度为零的点。</strong><br><img src="/img/downloaded/aHR0cHM6_52b5824047c6400a9b58abe8a2bb880f.png" alt="在这里插入图片描述"><br>局部较小值点和鞍点都会使得梯度为零，所以梯度为零的点，临界点不一定是局部极小值。</p>
</blockquote>
<h2 id="🎯如何判断临界值种类"><a href="#🎯如何判断临界值种类" class="headerlink" title="🎯如何判断临界值种类"></a>🎯如何判断临界值种类</h2><blockquote>
<p>判断一个临界点到底是局部极小值还是鞍点需要知道损失函数的形状。<br>虽然无法完整知道整个损失函数的样子，但是如果给定某一组参数，比如$θ^{‘}$，$θ^{‘}$ 附近的 $L(θ)$ 可近似为</p>
</blockquote>
<p>$$<br>	L(θ) ≈  L(θ^{‘})+(θ − θ^{′})^T*g +\frac{1}{2}(θ − θ^{′})^T * H(θ − θ^{′}) .  (3.1)<br>$$</p>
<p>式 (3.1) 是<strong>泰勒级数近似（Tayler series appoximation）。</strong> 其中，第一项 $L(θ)$′ 告诉我们，当 θ 跟 $θ^{′}$很近的时候，$L(θ)$ 应该跟还蛮靠近的；第二项$(θ − θ^{′})^T*g$中，g 代表梯度，它是一个向量，可以弥补$L(θ^{′})$跟 $L(θ)$ 之间的差距。有时候梯度 g 会写成 $∇L(θ^{′})$。$g_i$是向量 g 的第 i 个元素，就是 L 关于 θ 的第 i 个元素的微分，即<br>$$<br>g_i &#x3D;\frac{∂L(θ^{′})}{∂θ_i}.(3.2)<br>$$</p>
<blockquote>
<p>光看 g 还是没有办法完整地描述 ，还要看式 (3.1) 的第三项$\frac{1}{2}(θ − θ^{′})^T * H(θ − θ^{′})$。第三项跟<strong>海森矩阵（Hessian matrix）H 有关</strong> 。<br>H 里面放的是 L 的二次微分，它第 i 行，第 j 列的值 $H_{ij}$ 就是把 θ 的第 i 个元素对$L(θ^{′})$作微分，再把 θ 的第 j 个元素对$\frac{∂L(θ^{′})}{∂θ_i}$作微分后的结果，即</p>
</blockquote>
<p>$$<br>	H_{ij}   &#x3D;\frac{∂^2L(θ^{′})}{∂θ_i∂θ_j}. (3.3)<br>$$</p>
<blockquote>
<p>在临界点，梯度 g 为零，因此 θ − θ′Tg 为零。所以在临界点的附近，损失函数可被近似为<br>$$<br>	L(θ) ≈  L(θ^{‘}) +\frac{1}{2}(θ − θ^{′})^T * H(θ − θ^{′}) .  (3.1)<br>$$</p>
</blockquote>
<blockquote>
<p>我们可以根据$\frac{1}{2}(θ − θ^{′})^T * H(θ − θ^{′})$来判断在 $θ^{′}$附近的<strong>误差表面（error surface）</strong> 到底长什么样子。<br><strong>知道误差表面的“地貌”，我们就可以判断 L(θ′) 是局部极小值、局部极大值，还是鞍点。</strong></p>
</blockquote>
<p>上述我们通过一系列的转化把损失函数近似的写了出来，可以根据误差表面来判断临界点。</p>
<blockquote>
<p><strong>我们用向量 v 来表示$θ − θ^{′} ,(θ − θ^{′})^T * H(θ − θ^{′})$可改写为 $v^TH_v$，有如下三种情况。</strong></p>
<ol>
<li><strong>如果对所有 v，$v^TH_v&gt; 0$ .</strong>  这意味着对任意 θ，L(θ) &gt; L(θ′). 只要 θ 在 θ′ 附近，L(θ) 都大于 L(θ′). 这代表 L(θ′) 是附近的一个最低点，所以<strong>它是局部极小值。</strong></li>
<li><strong>如果对所有 v，$v^TH_v&lt; 0$ .</strong>  这意味着对任意 θ，L(θ) &lt; L(θ′)，θ′ 是附近最高的一个点，<strong>L(θ′) 是局部极大值。</strong></li>
<li><strong>如果对于 v， $v^TH_v$ 有时候大于零，有时候小于零。</strong> 这意味着在 θ′ 附近，有时候L(θ) &gt; L(θ′)，有时候 L(θ) &lt; L(θ′). 因此在 θ′ 附近，L(θ′) 既不是局部极大值，也不是局部极小值，而 <strong>是鞍点。</strong></li>
</ol>
</blockquote>
<h3 id="📌更简便的方法来判断-v-TH-v-的正负。"><a href="#📌更简便的方法来判断-v-TH-v-的正负。" class="headerlink" title="📌更简便的方法来判断 $v^TH_v$  的正负。"></a>📌更简便的方法来判断 $v^TH_v$  的正负。</h3><blockquote>
<p>只要看 H的特征值。</p>
<ol>
<li>若 H 的<strong>所有特征值都是正的</strong>，<strong>H 为正定矩阵</strong>，则  $v^TH_v$ &gt; 0，临界点是<strong>局部极小值</strong>。</li>
<li>若 H 的<strong>所有特征值都是负的</strong>，<strong>H 为负定矩阵</strong>，则 $v^TH_v$ &lt; 0，临界点是<strong>局部极大值</strong>。</li>
<li><strong>若 H 的特征值有正有负，临界点是鞍点</strong>。<br><img src="/img/downloaded/aHR0cHM6_8e582332ef78410f83fd1d6825802032.png" alt="在这里插入图片描述"></li>
</ol>
</blockquote>
<h2 id="🎯H-怎么告诉我们怎么更新参数呢？"><a href="#🎯H-怎么告诉我们怎么更新参数呢？" class="headerlink" title="🎯H 怎么告诉我们怎么更新参数呢？"></a>🎯H 怎么告诉我们怎么更新参数呢？</h2><blockquote>
<p>设 λ 为 H 的一个特征值 λ，u 为其对应的特征向量。对于我们的优化问题，可令 $u &#x3D;θ − θ^{′}$，则</p>
</blockquote>
<p>$$<br>u^{T}Hu &#x3D; uT(λu) &#x3D; λ∥u∥^{2}.<br>$$</p>
<blockquote>
<p>若 λ &lt; 0，则 $λ∥u∥^{2}$ &lt; 0。所以$\frac{1}{2}(θ − θ^{′})^T * H(θ − θ^{′})$ &lt; 0。此时，L(θ) &lt; L(θ′)，且<strong>沿着 u 的方向更新 θ，损失就会变小。</strong></p>
</blockquote>
<blockquote>
<p>只要 θ &#x3D; θ′ + u，沿着特征向量 u 的方向去更新参数，损失就会变小，所以虽然临界点的梯度为零，<strong>如果我们是在一个鞍点，只要找出负的特征值，再找出这个特征值对应的特征向量。将其与 θ′ 相加，就可以找到一个损失更低的点。</strong><br>我们其实只要顺着 u 的方向去更新参数，就可以找到一个比鞍点的损失还要更低的点。</p>
</blockquote>
<h2 id="🎯如何逃离鞍点"><a href="#🎯如何逃离鞍点" class="headerlink" title="🎯如何逃离鞍点"></a>🎯如何逃离鞍点</h2><blockquote>
<p>我们常常会遇到两种情况：损失仍然很高，却遇到了临界点而不再下降；或者损失降得很低，才遇到临界点。</p>
</blockquote>
<p><img src="/img/downloaded/aHR0cHM6_1c71d013f49d436083c869373473fda0.png" alt="在这里插入图片描述"></p>
<blockquote>
<p>在图 3.6 所示的例子中，最小值比例（&#x3D;正特征值数量&#x2F;总特征值数量）最大也不过处于 0.5 ~ 0.6 的范围，代表只有约一半的特征值为正，另一半的特征值为负，代表<strong>在所有的维度里面有约一半的路可以让损失上升，还有约一半的路可以让损失下降。</strong></p>
</blockquote>
<p><img src="/img/downloaded/aHR0cHM6_29c52681af40426f93b8fb5ec0bb0247.png" alt="在这里插入图片描述"></p>
<blockquote>
<p>所以从经验上看起来，<strong>局部极小值并没有那么常见。</strong> 多数的时候，我们训练到一个梯度很小的地方，参数不再更新，往往只是遇到了鞍点。</p>
</blockquote>
<h1 id="🚩-Task1-2"><a href="#🚩-Task1-2" class="headerlink" title="🚩 Task1.2"></a>🚩 Task1.2</h1><h2 id="🎯什么是批量和动量"><a href="#🎯什么是批量和动量" class="headerlink" title="🎯什么是批量和动量"></a>🎯什么是批量和动量</h2><blockquote>
<p>实际计算梯度的过程中，我们将数据分成多个<strong>批次（batch）</strong>，每个批次大小为B，即包含B个数据样本。</p>
</blockquote>
<blockquote>
<p>每次更新参数时，从批次中选取数据计算损失和梯度，并更新参数。完成一次遍历所有批次的过程称为一个<strong>回合（epoch</strong>）。</p>
</blockquote>
<blockquote>
<p>为了增加样本的随机性，我们会在划分批次时进行<strong>随机打乱（shuffle）</strong>。常见的一种做法是在每个回合开始之前重新划分批次，使得每个回合的批次数据都不同。</p>
</blockquote>
<p><img src="/img/downloaded/aHR0cHM6_982c143a836e48d0a7e1310cbdbaff35.png" alt="在这里插入图片描述"></p>
<h3 id="📌批量大小对梯度下降法的影响"><a href="#📌批量大小对梯度下降法的影响" class="headerlink" title="📌批量大小对梯度下降法的影响"></a>📌批量大小对梯度下降法的影响</h3><p><img src="/img/downloaded/aHR0cHM6_06be32fe4ce44e66a0d63deb0705b6ba.png" alt="在这里插入图片描述"></p>
<h4 id="🔧批量梯度下降法（Batch-Gradient-Descent，BGD）"><a href="#🔧批量梯度下降法（Batch-Gradient-Descent，BGD）" class="headerlink" title="🔧批量梯度下降法（Batch Gradient Descent，BGD）"></a>🔧批量梯度下降法（Batch Gradient Descent，BGD）</h4><blockquote>
<p>使用<strong>全批量（fullbatch）的数据</strong>来更新参数的方法即<strong>批量梯度下降法（Batch Gradient Descent，BGD）。</strong><br> 此时模型必须把 20 笔训练数据都看完，才能够计算损失和梯度，参数才能够更新一次。</p>
</blockquote>
<h4 id="🔧随机梯度下降法（Stochastic-Gradient-Descent，SGD），也称为增量梯度下降法"><a href="#🔧随机梯度下降法（Stochastic-Gradient-Descent，SGD），也称为增量梯度下降法" class="headerlink" title="🔧随机梯度下降法（Stochastic Gradient Descent，SGD），也称为增量梯度下降法"></a>🔧随机梯度下降法（Stochastic Gradient Descent，SGD），也称为增量梯度下降法</h4><blockquote>
<p>批量大小等于 1，此时使用的方法即<strong>随机梯度下降法（Stochastic Gradient Descent，SGD）</strong>，也称为增量梯度下降法。<br><strong>批量大小等于 1 意味着只要取出一笔数据即可计算损失、更新一次参数。</strong><br>如果总共有 20 笔数据，那么在每一个回合里面，参数会更新 20 次。<br>用一笔数据算出来的损失相对带有更多噪声，因此其更新的方向如图 3.8 所示，是曲曲折折的 。</p>
</blockquote>
<h4 id="🔧批量大小与计算时间的关系"><a href="#🔧批量大小与计算时间的关系" class="headerlink" title="🔧批量大小与计算时间的关系"></a>🔧批量大小与计算时间的关系</h4><p><img src="/img/downloaded/aHR0cHM6_1c45a418839e463e84f4651712ba1f76.png" alt="在这里插入图片描述"></p>
<blockquote>
<p>当批量大小增加到10000，甚至增加到 60000 的时候，<strong>GPU 计算梯度并更新参数所耗费的时间确实随着批量大小的增加而逐渐增长。</strong></p>
</blockquote>
<blockquote>
<p><strong>大的批量更新比较稳定，小的批量的梯度的方向是比较有噪声的（noisy）。</strong></p>
</blockquote>
<p><img src="/img/downloaded/aHR0cHM6_0386e7bf7a154d88b97fa1f46538b301.png" alt="在这里插入图片描述"></p>
<blockquote>
<p><strong>同一个模型，大的批量大小往往在训练的时候，结果比较差。</strong> 这个是优化的问题，大的批量大小优化可能会有问题，<strong>小的批量大小优化的结果反而是比较好的。</strong></p>
</blockquote>
<p><img src="/img/downloaded/aHR0cHM6_9a027bea6224408b9569218a4f427438.png" alt="在这里插入图片描述"></p>
<blockquote>
<p>一个可能的解释如图 3.12 所示，批量梯度下降在更新参数的时候，沿着一个损失函数来更新参数，走到一个局部最小值或鞍点显然就停下来了。<br><strong>梯度是零，如果不看海森矩阵，梯度下降就无法再更新参数了 。</strong><br><strong>但小批量梯度下降法（mini-batch gradient descent）每次是挑一个批量计算损失，所以每一次更新参数的时候所使用的损失函数是有差异的。</strong><br>这种有噪声的更新方式反而对训练其实是有帮助的。其实小的批量也对测试有帮助。</p>
</blockquote>
<p>在模型训练的适合小批量梯度下降法的噪声反而使得梯度不容易落在临界点，而且更方便测试。</p>
<blockquote>
<p>大的批量跟小的批量的<strong>训练准确率（accuracy）</strong> 差不多，但就算是在训练的时候结果差不多，<strong>测试的时候，大的批量比小的批量差，代表过拟合。</strong></p>
</blockquote>
<p><img src="/img/downloaded/aHR0cHM6_8e0ea637d131424d9101ed3b357a8d80.png" alt="在这里插入图片描述"></p>
<blockquote>
<p><strong>大的批量大小会让我们倾向于走到“峡谷”里面，而小的批量大小倾向于让我们走到“盆地”里面。</strong> 小的批量有很多的损失，其更新方向比较随机，其每次更新的方向都不太一样。即使“峡谷”非常窄，它也可以跳出去，之后如果有一个非常宽的“盆地”，它才会停下来。</p>
</blockquote>
<p><img src="/img/downloaded/aHR0cHM6_9394eb62905742258e088a7c91f3650e.jpeg" alt="在这里插入图片描述"></p>
<blockquote>
<p>而小的批量更新的方向比较有噪声的，大的批量更新的方向比较稳定。但是有噪声的更新方向反而在优化的时候有优势，而且在测试的时候也会有优势。<br><strong>所以大的批量跟小的批量各有优缺点，批量大小是需要去调整的超参数。</strong></p>
</blockquote>
<h2 id="🎯什么是动量法"><a href="#🎯什么是动量法" class="headerlink" title="🎯什么是动量法"></a>🎯什么是动量法</h2><blockquote>
<p><strong>动量法（momentum method）是另外一个可以对抗鞍点或局部最小值的方法。</strong></p>
<p><img src="/img/downloaded/aHR0cHM6_bd3fa1b6cffc471bb731b04959f956d3.png" alt="在这里插入图片描述"></p>
</blockquote>
<blockquote>
<p>但是在物理的世界里，一个球如果从高处滚下来，就算滚到鞍点或鞍点，<strong>因为惯性</strong>的关系它还是会继续往前走。因此在物理的世界里面，一个球从高处滚下来的时候，它<strong>并不一定会被鞍点或局部最小值卡住，如果将其应用到梯度下降中，这就是动量。</strong></p>
</blockquote>
<h3 id="📌对比一般的梯度下降法和动量法"><a href="#📌对比一般的梯度下降法和动量法" class="headerlink" title="📌对比一般的梯度下降法和动量法"></a>📌对比一般的梯度下降法和动量法</h3><p><img src="/img/downloaded/aHR0cHM6_054bf06823ff4f3bad71c455e70d05e8.png" alt="在这里插入图片描述"></p>
<p><img src="/img/downloaded/aHR0cHM6_bca48a45e8f34bbe8514b794c6920a66.png" alt="在这里插入图片描述"></p>
<blockquote>
<p>引入动量后，每次在移动参数的时候，不是只往梯度的反方向来移动参数，而是<strong>根据梯度的反方向加上前一步移动的方向决定移动方向。</strong></p>
</blockquote>
<blockquote>
<p><strong>图 3.16 中红色虚线方向是梯度的反方向，蓝色虚线方向是前一次更新的方向，蓝色实线的方向是下一步要移动的方向。</strong> 把前一步指示的方向跟梯度指示的方向相加就是下一步的移动方向。</p>
</blockquote>
<p>动量法引入了动量的概念，通过累积之前的梯度信息来加速学习过程。<strong>动量法在更新参数时不仅考虑当前的梯度，还考虑了之前累积的梯度。</strong></p>
<h4 id="🔧动量法的主要优点"><a href="#🔧动量法的主要优点" class="headerlink" title="🔧动量法的主要优点"></a>🔧动量法的主要优点</h4><p>可以加速收敛速度，特别是在目标函数存在高度非均向性的情况下。<br>可以帮助跳出局部最小值，并具有一定的平滑效果。</p>
<h4 id="🔧动量法也存在一些缺点。"><a href="#🔧动量法也存在一些缺点。" class="headerlink" title="🔧动量法也存在一些缺点。"></a>🔧动量法也存在一些缺点。</h4><p>动量法引入了额外的超参数，需要人工调整。<br>如果动量系数设置过大，可能会导致震荡；如果设置过小，则可能会导致收敛速度变慢。</p>
<p><img src="/img/downloaded/aHR0cHM6_9f10f439e6a745eaafd49a3d5a497733.png" alt="在这里插入图片描述"></p>
<blockquote>
<p>动量的简单例子如图 3.17 所示。<strong>红色表示负梯度方向，蓝色虚线表示前一步的方向，蓝色实线表示真实的移动量。</strong> 一开始没有前一次更新的方向，完全按照梯度给指示往右移动参数。负梯度方向跟前一步移动的方向加起来，得到往右走的方向。一般梯度下降走到一个局部最小值或鞍点时，就被困住了。但有动量还是有办法继续走下去，因为动量不是只看梯度，还看前一步的方向。<strong>即使梯度方向往左走，但如果前一步的影响力比梯度要大，球还是有可能继续往右走</strong>，甚至翻过一个小丘，也许可以走到更好的局部最小值，这就是动量有可能带来的好处 。</p>
</blockquote>
<h1 id="🚩Task-2-1"><a href="#🚩Task-2-1" class="headerlink" title="🚩Task 2.1"></a>🚩Task 2.1</h1><h2 id="🎯什么是自适应学习率"><a href="#🎯什么是自适应学习率" class="headerlink" title="🎯什么是自适应学习率"></a>🎯什么是自适应学习率</h2><blockquote>
<p>临界点其实不一定是在训练一个网络的时候会遇到的最大的障碍。</p>
</blockquote>
<p><img src="/img/downloaded/aHR0cHM6_aa4761d1b98d42ebb39ed5783aacdcd6.png" alt="在这里插入图片描述"></p>
<blockquote>
<p>图 3.18中的横坐标代表<strong>参数更新的次数</strong>，竖坐标表示<strong>损失</strong>。</p>
</blockquote>
<blockquote>
<p><strong>一般在训练一个网络的时候，损失原来很大，随着参数不断的更新，损失会越来越小，最后就卡住了，损失不再下降。</strong></p>
</blockquote>
<p>到临界点损失不在下降。<br><img src="/img/downloaded/aHR0cHM6_81e4b491f7894b1badc43ab7b400c33a.png" alt="在这里插入图片描述"></p>
<blockquote>
<p>我们走到临界点的时候，意味着梯度非常小，但损失不再下降的时候，梯度并没有真的变得很小，图 3.19 给出了示例。</p>
</blockquote>
<blockquote>
<p>图 3.19 中横轴是<strong>迭代次数</strong>，竖轴是梯度的<strong>范数（norm）</strong>，即梯度这个向量的长度。<br><strong>随着迭代次数增多，虽然损失不再下降，但是梯度的范数并没有真的变得很小。</strong></p>
</blockquote>
<p>范数（norm）是 梯度这个向量的长度。<br><img src="/img/downloaded/aHR0cHM6_38d2774cb82042c3ba68617320ef6b9c.png" alt="在这里插入图片描述"></p>
<blockquote>
<p>图 3.20是误差表面，梯度在山谷的两个谷壁间，不断地<strong>来回“震荡”</strong>，这个时候<strong>损失不会再下降</strong>，它不是真的卡到了临界点，<strong>卡到了鞍点或局部最小值</strong>。<br>在局部最小值或鞍点，只是单纯的损失无法再下降。<br><strong>但它的梯度仍然很大，只是损失不一定再减小了</strong>。</p>
</blockquote>
<blockquote>
<p>我们可以试着把学习率设小一点</p>
</blockquote>
<p><img src="/img/downloaded/aHR0cHM6_cb3681446a064d39946a9916f168da96.png" alt="在这里插入图片描述"></p>
<blockquote>
<p>调学习率从 10−2 调到 10−7 的结果如图 3.22(b)所示，参数不再“震荡”了。<strong>参数会滑到山谷底后左转，但是这个训练永远走不到终点，因为学习率已经太小了</strong>。</p>
</blockquote>
<blockquote>
<p>AB段的坡度很陡，梯度的值很大，还能够前进一点。左拐以后，BC 段的坡度已经非常平坦了，这种小的学习率无法再让训练前进。<br>事实上在 BC 段有 10 万个点（10 万次更新），但都<strong>无法靠近局部最小值</strong>，所以<strong>显然就算是一个凸的误差表面，梯度下降也很难训练</strong>。</p>
</blockquote>
<blockquote>
<p>在梯度下降里面，所有的参数都是设同样的学习率，这显然是不够的，应该要为每一个参数定制化学习率，即引入<strong>自适应学习率（adaptive learning rate）</strong> 的方法，给每一个参数不同的学习率。</p>
</blockquote>
<h2 id="🎯AdaGrad"><a href="#🎯AdaGrad" class="headerlink" title="🎯AdaGrad"></a>🎯AdaGrad</h2><blockquote>
<p><strong>AdaGrad（Adaptive Gradient）</strong> 是<strong>典型的自适应学习率方法</strong>，其能够根据梯度大小自动调整学习率。 <strong>AdaGrad可以做到梯度比较大的时候，学习率就减小，梯度比较小的时候，学习率就放大</strong>。</p>
</blockquote>
<h3 id="📌参数更新和学习率调整"><a href="#📌参数更新和学习率调整" class="headerlink" title="📌参数更新和学习率调整"></a>📌参数更新和学习率调整</h3><h4 id="🔧基本参数更新公式"><a href="#🔧基本参数更新公式" class="headerlink" title="🔧基本参数更新公式"></a>🔧基本参数更新公式</h4><p>在第$t$个迭代中，参数$\theta^i$的更新公式为：<br>$$<br>  \theta_{t+1}^i \leftarrow \theta_t^i \eta g_t^i \quad (3.14)<br>$$<br>  其中$g_t^i$是在$\theta &#x3D; \theta_t$时，参数$\theta^i$对损失$L$的微分。</p>
<h4 id="🔧梯度计算"><a href="#🔧梯度计算" class="headerlink" title="🔧梯度计算"></a>🔧梯度计算</h4><p>梯度$g_t^i$的计算公式为：<br>$$<br>  g_t^i &#x3D; \left.\frac{\partial L}{\partial \theta^i}\right|_{\theta&#x3D;\theta_t} \quad (3.15)<br>$$</p>
<h4 id="🔧定制化学习率"><a href="#🔧定制化学习率" class="headerlink" title="🔧定制化学习率"></a>🔧定制化学习率</h4><p>将学习率$\eta$调整为参数相关的学习率$\frac{\eta}{\sigma_t^i}$：<br>$$<br>  \theta_{t+1}^i \leftarrow \theta_t^i \frac{\eta}{\sigma_t^i} g_t^i \quad (3.16)<br>$$<br>  其中$\sigma_t^i$与参数$i$和迭代$t$相关。</p>
<h4 id="🔧梯度的均方根"><a href="#🔧梯度的均方根" class="headerlink" title="🔧梯度的均方根"></a>🔧梯度的均方根</h4><p>参数的更新过程，其中$\sigma_0^i$的计算为：<br>$$<br>  \sigma_0^i &#x3D; \sqrt{\left(g_0^i\right)^2} &#x3D; \left|g_0^i\right| \quad (3.18)<br>$$</p>
<h4 id="🔧参数更新的迭代过程"><a href="#🔧参数更新的迭代过程" class="headerlink" title="🔧参数更新的迭代过程"></a>🔧参数更新的迭代过程</h4><p>第二次参数更新：<br>$$<br>  \theta_2^i \leftarrow \theta_1^i \frac{\eta}{\sigma_1^i} g_1^i \quad (3.19)<br>$$<br>  其中$\sigma_1^i$是过去所有计算出来的梯度的平方的平均再开根号。</p>
<h4 id="🔧迭代更新公式"><a href="#🔧迭代更新公式" class="headerlink" title="🔧迭代更新公式"></a>🔧迭代更新公式</h4><p>第$t+1$次更新参数的公式为：<br>$$<br>  \theta_{t+1}^i \leftarrow \theta_t^i \frac{\eta}{\sigma_t^i} g_t^i \quad \sigma_t^i &#x3D; \sqrt{\frac{1}{t+1}\sum_{i&#x3D;0}^t\left(g_t^i\right)^2} \quad (3.22)<br>$$</p>
<h4 id="🔧参数更新的动态调整"><a href="#🔧参数更新的动态调整" class="headerlink" title="🔧参数更新的动态调整"></a>🔧参数更新的动态调整</h4><p>根据梯度的不同，每一个参数的梯度的不同，自动调整学习率的大小，使得参数更新更加有效。</p>
<h4 id="🔧参数更新的可视化"><a href="#🔧参数更新的可视化" class="headerlink" title="🔧参数更新的可视化"></a>🔧参数更新的可视化</h4><p>图 3.24 展示了两个参数$\theta^1$和$\theta^2$的更新情况，其中$\theta^1$坡度小，$\theta^2$坡度大。根据公式 (3.22)，<strong>不同的梯度大小导致不同的学习率调整，从而影响参数更新的步伐。</strong></p>
<p><img src="/img/downloaded/aHR0cHM6_e0c9ce493ab549ab873ca72f7375cde3.png" alt="在这里插入图片描述"></p>
<h3 id="📌AdaGrad算法的问题"><a href="#📌AdaGrad算法的问题" class="headerlink" title="📌AdaGrad算法的问题"></a>📌AdaGrad算法的问题</h3><p><img src="/img/downloaded/aHR0cHM6_b46b6b35f6024fd3a7d77c46487347e9.png" alt="在这里插入图片描述"><br><strong>当模型接近最优点时，由于在某些方向上梯度非常小，AdaGrad算法会导致学习率变得非常大，从而可能出现“爆炸”现象，使得模型突然偏离最优路径</strong>。累积的 $\sigma_t^i$值<strong>在梯度较小的方向上会变得非常大，导致学习步伐过大。</strong></p>
<h2 id="🎯RMSProp"><a href="#🎯RMSProp" class="headerlink" title="🎯RMSProp"></a>🎯RMSProp</h2><p><strong>RMSprop 是一种自适应学习率的优化算法</strong>，由 Geoffrey Hinton 在 Coursera 深度学习课程中提出。</p>
<h3 id="📌算法的步骤"><a href="#📌算法的步骤" class="headerlink" title="📌算法的步骤"></a>📌算法的步骤</h3><h4 id="🔧初始梯度的均方根"><a href="#🔧初始梯度的均方根" class="headerlink" title="🔧初始梯度的均方根"></a>🔧初始梯度的均方根</h4><p>RMSprop 的第一步与 Adagrad 相同，<strong>计算初始梯度的绝对值作为均方根</strong>：<br>$$<br>  \sigma_0^i &#x3D; \sqrt{\left(g_0^i\right)^2} &#x3D; \left|g_0^i\right| \quad (3.23)<br>$$</p>
<h3 id="📌参数更新公式"><a href="#📌参数更新公式" class="headerlink" title="📌参数更新公式"></a>📌参数更新公式</h3><p>第二次更新参数的公式，<strong>引入超参数 $\alpha$来调整梯度的重要性</strong>：<br>$$<br>  \theta_2^i \leftarrow \theta_1^i \frac{\eta}{\sigma_1^i} g_1^i \quad \sigma_1^i &#x3D; \sqrt{\alpha \left(\sigma_0^i\right)^2 + (1-\alpha) \left(g_1^i\right)^2} \quad (3.24)<br>$$<br>  其中 $0 &lt; \alpha &lt; 1$。</p>
<h4 id="🔧迭代更新过程"><a href="#🔧迭代更新过程" class="headerlink" title="🔧迭代更新过程"></a>🔧迭代更新过程</h4><p>后续的参数更新过程，<strong>通过递归方式计算 $\sigma_t^i$来动态调整学习率</strong>：<br>$$<br>  \begin{aligned}<br>  \theta_3^i &amp;\leftarrow \theta_2^i \frac{\eta}{\sigma_2^i} g_2^i \quad \sigma_2^i &#x3D; \sqrt{\alpha \left(\sigma_1^i\right)^2 + (1-\alpha) \left(g_2^i\right)^2} \<br>  \vdots \<br>  \theta_{t+1}^i &amp;\leftarrow \theta_t^i \frac{\eta}{\sigma_t^i} g_t^i \quad \sigma_t^i &#x3D; \sqrt{\alpha \left(\sigma_{t-1}^i\right)^2 + (1-\alpha) \left(g_t^i\right)^2}<br>  \end{aligned}<br>$$<br><img src="/img/downloaded/aHR0cHM6_cf67b006a5fc42629d9e68e9ac35cc37.png" alt="在这里插入图片描述"></p>
<h3 id="📌算法特性"><a href="#📌算法特性" class="headerlink" title="📌算法特性"></a>📌算法特性</h3><p>RMSprop <strong>通过超参数 $\alpha$来决定当前梯度 $g_t^i$相较于之前梯度</strong>的重要性。<strong>这使得算法能够快速响应梯度的变化，实现更灵活的参数更新。</strong><br>在误差表面的不同区域，例如从 A 到 B 的平坦区域，<strong>RMSprop 允许较大的学习步伐</strong>；而在 B 到 C 的陡峭区域，通过增加 $\alpha$值，可以<strong>快速减小学习步伐，实现“踩刹车”的效果。</strong></p>
<p><img src="/img/downloaded/aHR0cHM6_3f693ea3ebba414e83fcc79cd5d7df59.png" alt="在这里插入图片描述"></p>
<h2 id="🎯Adam"><a href="#🎯Adam" class="headerlink" title="🎯Adam"></a>🎯Adam</h2><blockquote>
<p><strong>最常用的优化的策略或者优化器（optimizer） 是Adam（Adaptive moment estimation）</strong> 。<br>Adam 可以看作 <strong>RMSprop 加上动量</strong>，其使用动量作为参数更新方向，并且能够自适应调整学习率。PyTorch 里面已经写好了 Adam 优化器。</p>
</blockquote>
<h2 id="🎯学习率调度"><a href="#🎯学习率调度" class="headerlink" title="🎯学习率调度"></a>🎯学习率调度</h2><p>学习率调度是一种策略，它使得学习率 $\eta$<strong>随着时间或训练的迭代次数逐渐减小</strong>。<strong>学习率衰减（learning rate decay）或学习率退火（learning rate annealing）是学习率调度中的一种常见策略</strong>。</p>
<h3 id="📌学习率衰减"><a href="#📌学习率衰减" class="headerlink" title="📌学习率衰减"></a>📌学习率衰减</h3><p>通过引入学习率衰减，可以<strong>避免在训练后期由于学习率过大导致的不稳定现象</strong>。学习率调度<strong>允许模型在训练初期快速收敛，在训练后期则通过减小学习率，使模型能够稳定地接近最优点</strong>。<br><img src="/img/downloaded/aHR0cHM6_1a64095d74924d7bb04096c9235d4a6b.png" alt="在这里插入图片描述"></p>
<h3 id="📌学习率调度的公式"><a href="#📌学习率调度的公式" class="headerlink" title="📌学习率调度的公式"></a>📌学习率调度的公式</h3><p>引入学习率调度后的参数更新公式为：<br>$$<br>\theta_{t+1}^i \leftarrow \theta_t^i \frac{\eta_t}{\sigma_t^i} g_t^i \quad (3.26)<br>$$<br>其中 $\eta_t$是随时间变化的学习率。</p>
<h3 id="📌预热"><a href="#📌预热" class="headerlink" title="📌预热"></a>📌预热</h3><blockquote>
<p>除了学习率下降以外，还有另外一个经典的学习率调度的方式———预热。</p>
</blockquote>
<blockquote>
<p><strong>预热的方法是让学习率先变大后变小</strong>，至于变到多大、变大的速度、变小的速度是超参数。<br>除了残差网络， <strong>BERT 和 Transformer 的训练也都使用了预<br>热</strong>。</p>
</blockquote>
<p><img src="/img/downloaded/aHR0cHM6_0c1596cc34d14a77833076d2db96d869.png" alt="在这里插入图片描述"></p>
<h3 id="📌RAdam"><a href="#📌RAdam" class="headerlink" title="📌RAdam"></a>📌RAdam</h3><blockquote>
<p>RAdam的应用场景非常广泛，<strong>尤其适用于那些对学习率预热敏感的模型和任务</strong>。<br>RAdam 是 Ranger 优化器的重要组成部分。Ranger 优化器结合了 RAdam 和 LookAhead，在深度学习中表现出色，能够提升模型的能力和收敛速度。</p>
</blockquote>
<h4 id="🔧无需预热"><a href="#🔧无需预热" class="headerlink" title="🔧无需预热"></a>🔧无需预热</h4><p>RAdam 无需预热，就能避免模型收敛至“局部最优解”。 </p>
<h4 id="🔧优于手动预热"><a href="#🔧优于手动预热" class="headerlink" title="🔧优于手动预热"></a>🔧优于手动预热</h4><p>RAdam 自动提供方差缩减，在各种预热长度和各种学习率下都优于传统的手动预热调整。 </p>
<h4 id="🔧RAdam与Adam性能对比"><a href="#🔧RAdam与Adam性能对比" class="headerlink" title="🔧RAdam与Adam性能对比"></a>🔧RAdam与Adam性能对比</h4><table>
<thead>
<tr>
<th>性能指标</th>
<th>RAdam</th>
<th>Adam</th>
</tr>
</thead>
<tbody><tr>
<td>收敛速度</td>
<td>有望为几乎所有 AI 应用提供更好的收敛速度。</td>
<td>在前期的表现一般不好，前期数据少，很难总结出一个靠谱的初始动量，也更容易陷入局部最优，所以一般需要几个 batch 的预热阶段让自适应动量更靠谱。</td>
</tr>
<tr>
<td>训练稳定性</td>
<td>对不同的学习速度具有鲁棒性，同时仍具有更好的训练稳定性（对选择的学习率不那么敏感）。</td>
<td>在没有预热的情况下使用时，在初始迭代期间，梯度具有较大的方差。这种较大的差异会导致最小值的过冲，从而导致较差的最优值。</td>
</tr>
<tr>
<td>准确性和泛化性</td>
<td>可立即提高 AI 准确度和泛化性。</td>
<td>存在很多问题，效果甚至没有简单的 SGD+Momentum 好。</td>
</tr>
<tr>
<td>以下是对您提供的文件内容的总结，使用Markdown格式：</td>
<td></td>
<td></td>
</tr>
</tbody></table>
<hr>
<h2 id="🎯学习率调度与优化器变形"><a href="#🎯学习率调度与优化器变形" class="headerlink" title="🎯学习率调度与优化器变形"></a>🎯学习率调度与优化器变形</h2><h3 id="📌动量与均方根的计算差异"><a href="#📌动量与均方根的计算差异" class="headerlink" title="📌动量与均方根的计算差异"></a>📌动量与均方根的计算差异</h3><p><strong>动量 $m_{t}^{i}$</strong>：<strong>考虑了过去所有梯度的方向和大小，通过将所有梯度直接相加来计算</strong>，因此保留了梯度的方向信息。<br><strong>均方根 $\sigma_{t}^{i}$</strong>：仅考虑了梯度的大小，<strong>通过计算梯度的平方和的平方根来得出，忽略了梯度的方向</strong>。</p>
<p>尽管 $m_{t}^{i}$ 和 $\sigma_{t}^{i}$ 都使用了过去所有的梯度，但由于<strong>计算方式的不同，它们并不会相互抵消</strong>。</p>
<h3 id="📌优化总结"><a href="#📌优化总结" class="headerlink" title="📌优化总结"></a>📌优化总结</h3><p>从最基本的梯度下降法演化至包含动量的优化版本，如式(3.27)所示：<br>  $$<br>  \theta_{t+1}^i \leftarrow \theta_t^i \frac{\eta_t}{\sigma_t^i} m_t^i \qquad (3.27)<br>  $$<br>  其中，$m_t^i$ 表示动量。</p>
<p>动量 $m_t^i$ 不仅考虑了某一时刻的梯度方向，而是<strong>对所有梯度方向进行了加权总和，作为参数更新的方向</strong>。<br><strong>更新步伐的大小</strong>由 $\frac{m_t^i}{\sigma_t^i}$ 决定。</p>
<h1 id="🚩Task-2-2"><a href="#🚩Task-2-2" class="headerlink" title="🚩Task 2.2"></a>🚩Task 2.2</h1><h2 id="🎯什么是分类"><a href="#🎯什么是分类" class="headerlink" title="🎯什么是分类"></a>🎯什么是分类</h2><blockquote>
<p><strong>分类与回归是深度学习最常见的两种问题</strong></p>
</blockquote>
<h2 id="🎯回归与分类的区别和联系"><a href="#🎯回归与分类的区别和联系" class="headerlink" title="🎯回归与分类的区别和联系"></a>🎯回归与分类的区别和联系</h2><blockquote>
<p>回归是输入一个向量 x，输出 yˆ，我们希望 yˆ 跟某一个标签 y 越接近越好， y 是要学习的目标。而分类可当作回归来看，输入 x 后，输出仍然是一个标量 yˆ，要让它跟正确答案的那个类越接近越好。 </p>
</blockquote>
<h3 id="📌回归"><a href="#📌回归" class="headerlink" title="📌回归"></a>📌回归</h3><p>回归任务涉及输入一个向量 $x$ 并预测一个连续值 $y$。<br>目标是使得预测值 $y$ 尽可能接近真实标签值。</p>
<h3 id="📌分类"><a href="#📌分类" class="headerlink" title="📌分类"></a>📌分类</h3><p>分类可以视为一种特殊的回归问题，其中输入 $x$ 后，输出是一个标量 $y$。<br>目的是让输出 $y$ 与正确类别的编号尽可能接近。</p>
<h2 id="🎯使用数字表示类别会出现的问题"><a href="#🎯使用数字表示类别会出现的问题" class="headerlink" title="🎯使用数字表示类别会出现的问题"></a>🎯使用数字表示类别会出现的问题</h2><pre><code>引出独热编码
</code></pre>
<p><strong>直接使用数字来表示类别可能会导致问题，尤其是当类别之间存在某种关系时。</strong></p>
<blockquote>
<p>例如，根据身高和体重预测年级时，一年级和二年级在逻辑上比一年级和三年级更接近。</p>
</blockquote>
<h2 id="🎯什么是独热编码（One-Hot-Encoding）"><a href="#🎯什么是独热编码（One-Hot-Encoding）" class="headerlink" title="🎯什么是独热编码（One-Hot Encoding）"></a>🎯什么是独热编码（One-Hot Encoding）</h2><p><strong>当类别之间没有固有的顺序或数值关系时，使用独热编码来表示类别是一种常见做法。</strong><br>独热编码通过<strong>为每个类别分配一个唯一的二进制向量，避免了类别之间不恰当的数值关系。</strong><br><strong>在分类问题中尤其有用，因为它允许模型更准确地学习类别之间的关系。</strong></p>
<p><img src="/img/downloaded/aHR0cHM6_d14bc7f21bd8480684b1cbeba4820441.png" alt="在这里插入图片描述"></p>
<blockquote>
<p>图3.30展示了如何使用数字和独热编码来表示类别。类别编号（如1、2、3）可能暗示它们之间的某种关系，而独热编码则不包含这样的预设关系。</p>
</blockquote>
<blockquote>
<p><strong>如果用独热向量计算距离的话，类两两之间的距离都是一样的</strong><br>以下是对您提供的文件内容的总结：</p>
</blockquote>
<h3 id="📌多输出神经网络结构"><a href="#📌多输出神经网络结构" class="headerlink" title="📌多输出神经网络结构"></a>📌多输出神经网络结构</h3><p><img src="/img/downloaded/aHR0cHM6_6cd73e45d00c42f5b842704208c7e90a.png" alt="在这里插入图片描述"></p>
<p><strong>计算 $\hat{y}_1$</strong>:<br>将输入特征 $x_1$ 与权重相乘，加上偏置，得到 $a_1$。<br>将 $a_1$ 与另一组权重相乘，加上偏置，得到 $\hat{y}_1$。</p>
<p><strong>计算 $\hat{y}_2$</strong>:<br>将输入特征 $x_2$ 与权重相乘，加上偏置，得到 $a_2$。<br>将 $a_2$ 与另一组权重相乘，加上偏置，得到 $\hat{y}_2$。</p>
<p> <strong>计算 $\hat{y}_3$</strong>:<br>将输入特征 $x_3$ 与权重相乘，加上偏置，得到 $a_3$。<br> 将 $a_3$ 与另一组权重相乘，加上偏置，得到 $\hat{y}_3$。</p>
<p>每个输出 $\hat{y}$ 是通过对输入特征的不同线性组合并加上偏置来计算的。使得这些输出 $\hat{y}_1, \hat{y}_2, \hat{y}_3$ 尽可能接近它们各自的目标值。<br><strong>偏置使得这些输出尽可能接近它们的目标值，以实现最佳的预测性能。</strong></p>
<h2 id="🎯带有-softmax-的分类"><a href="#🎯带有-softmax-的分类" class="headerlink" title="🎯带有 softmax 的分类"></a>🎯带有 softmax 的分类</h2><blockquote>
<p>按照上述的设定，分类实际过程是：输入 x，乘上 W，加上 b，通过激活函数 σ，乘上W ′，再加上 b′ 得到向量 yˆ。<br>但实际做分类的时候，往往会把 yˆ 通过 softmax 函数得到 y′，才去计算 y′ 跟 yˆ 之间的距离。</p>
</blockquote>
<p><img src="/img/downloaded/aHR0cHM6_58a16b2b20d04e2ca70e7dbe121410e0.png" alt="在这里插入图片描述"></p>
<h3 id="📌为什么分类过程中要加上softmax函数"><a href="#📌为什么分类过程中要加上softmax函数" class="headerlink" title="📌为什么分类过程中要加上softmax函数"></a>📌为什么分类过程中要加上softmax函数</h3><p><img src="/img/downloaded/aHR0cHM6_d0aaab477265406899e7be95004fd606.png" alt="在这里插入图片描述"></p>
<blockquote>
<p>softmax 的计算如式 (3.28) 所示，先把所有的 y 取一个指数（负数取指数后也会变成正的），再对其做归一化（除掉所有 y 的指数值的和）得到 y′。</p>
</blockquote>
<h3 id="📌Softmax-函数及其特性"><a href="#📌Softmax-函数及其特性" class="headerlink" title="📌Softmax 函数及其特性"></a>📌Softmax 函数及其特性</h3><p><img src="/img/downloaded/aHR0cHM6_0d75ef9509af4272b2626f83b1d5379a.png" alt="在这里插入图片描述"><br>在考虑三个类别的情况下，Softmax 函数的应用(图3.33）</p>
<p><strong>Softmax 函数</strong> 用于将一个向量或一组实数转换成另一个向量，其中转换后的向量元素值在 0 到 1 之间，并且所有元素的和为 1。<br>公式为：<br>  $$<br>  y_i^{\prime} &#x3D; \frac{\exp\left(y_i\right)}{\sum_j \exp\left(y_i\right)} \qquad (3.28)<br>  $$<br>  其中$y_i$ 是输入向量中的第$i$ 个元素，$y_i^{\prime}$ 是输出向量中的第$i$ 个元素。</p>
<h4 id="🔧特性"><a href="#🔧特性" class="headerlink" title="🔧特性"></a>🔧特性</h4><ol>
<li><strong>Softmax 函数除了进行归一化，使得输出值在 0 到 1 之间并总和为 1 之外，还有将大数值与小数值的差距进一步拉大的效果</strong>。</li>
<li>输出值$y_i^{\prime}$ 满足$1 &gt; y_i^{\prime} &gt; 0$，并且所有输出值之和为 1。</li>
</ol>
<h3 id="📌Sigmoid-函数与-Softmax-函数的比较"><a href="#📌Sigmoid-函数与-Softmax-函数的比较" class="headerlink" title="📌Sigmoid 函数与 Softmax 函数的比较"></a>📌Sigmoid 函数与 Softmax 函数的比较</h3><h4 id="🔧两分类问题"><a href="#🔧两分类问题" class="headerlink" title="🔧两分类问题"></a>🔧两分类问题</h4><p>在处理两个类别的问题时，通常直接使用 <strong>Sigmoid 函数</strong> 而不是 Softmax。<br><strong>当只有两个类别时，Sigmoid 函数和 Softmax 函数是等价的</strong>。</p>
<h4 id="🔧多分类问题"><a href="#🔧多分类问题" class="headerlink" title="🔧多分类问题"></a>🔧多分类问题</h4><p><strong>在涉及三个或更多类别的情况下，Softmax 函数是首选</strong>，因为它可以处理多类别的输出，并保证输出值的总和为 1。</p>
<h2 id="🎯什么是分类损失"><a href="#🎯什么是分类损失" class="headerlink" title="🎯什么是分类损失"></a>🎯什么是分类损失</h2><h3 id="📌损失函数"><a href="#📌损失函数" class="headerlink" title="📌损失函数"></a>📌损失函数</h3><p>在分类问题中，损失函数用于衡量模型预测值$y’$ 与实际标签$y$ 之间的差异。</p>
<h3 id="📌均方误差-MSE"><a href="#📌均方误差-MSE" class="headerlink" title="📌均方误差 (MSE)"></a>📌均方误差 (MSE)</h3><p><strong>公式</strong>：<br>  $$<br>  e &#x3D; \sum_i \left(y_i y_i’\right)^2 \quad (3.29)<br>  $$<br><strong>计算预测值与实际值之间差的平方和。</strong></p>
<h3 id="📌交叉熵-Cross-Entropy"><a href="#📌交叉熵-Cross-Entropy" class="headerlink" title="📌交叉熵 (Cross-Entropy)"></a>📌交叉熵 (Cross-Entropy)</h3><p><strong>公式</strong>：<br>  $$<br>  e &#x3D; -\sum_i y_i \ln y_i’ \quad (3.30)<br>  $$<br><strong>衡量实际标签与通过 softmax 转换后的预测值之间的差异。</strong></p>
<p><strong>优点</strong>：当预测值与实际值相同时，交叉熵最小化，此时均方误差也最小。</p>
<h3 id="📌使用-softmax-的好处"><a href="#📌使用-softmax-的好处" class="headerlink" title="📌使用 softmax 的好处"></a>📌使用 softmax 的好处</h3><blockquote>
<p>如图 3.35 所示，有一个三类的分类，网络先输出 y1、 y2 和 y3，在通过 softmax 以后，产生 y1′ 、 y2′ 和 y3′ 。<br>假设正确答案是 [1, 0, 0]T，要计算 [1, 0, 0]T 跟 y1′ 、 y2′ 和 y3′ 之间的距离 e， e 可以是均方误差或交叉熵。<br>假设 y1 的变化是从-10 到 10， y2 的变化也是从-10 到 10， y3 就固定设成-1000。因为 y3 的值很小，通过 softmax 以后， y3′ 非常趋近于 0，它跟正确答案非常接近，且它对结果影响很少。</p>
</blockquote>
<p><img src="/img/downloaded/aHR0cHM6_139ec3c735a6474886cc99d4e3e1ff4e.png" alt="在这里插入图片描述"><br>Softmax 将网络输出转换为概率分布，使得每个类别的预测值在 0 到 1 之间，并且总和为 1。</p>
<h2 id="🎯损失函数的选择对优化的影响"><a href="#🎯损失函数的选择对优化的影响" class="headerlink" title="🎯损失函数的选择对优化的影响"></a>🎯损失函数的选择对优化的影响</h2><h3 id="📌交叉熵"><a href="#📌交叉熵" class="headerlink" title="📌交叉熵"></a>📌交叉熵</h3><ol>
<li><strong>优点</strong>：在分类问题中，交叉熵比均方误差更常用，因为它在优化过程中表现更好，尤其是在参数初始化远离最优值时。</li>
<li><strong>优化难度</strong>：使用交叉熵时，即使在损失较大的区域，梯度仍然存在，使得模型可以通过梯度下降法有效地优化。</li>
</ol>
<h3 id="📌均方误差"><a href="#📌均方误差" class="headerlink" title="📌均方误差"></a>📌均方误差</h3><ol>
<li><strong>缺点</strong>：在损失较大的区域，均方误差可能导致梯度非常小，使得梯度下降法难以优化。</li>
<li><strong>优化难度</strong>：如果没有好的优化器，使用均方误差可能导致模型训练困难。</li>
</ol>
<p><img src="/img/downloaded/aHR0cHM6_92f0eeeb07fc45e8a578459dfd39832f.png" alt="在这里插入图片描述"></p>
<p>图3.35比较了均方误差和交叉熵在损失表面上的差异，以及它们对优化过程的影响。</p>
<h3 id="📌总结"><a href="#📌总结" class="headerlink" title="📌总结"></a>📌总结</h3><ol>
<li>在分类问题中，交叉熵是首选的损失函数，因为它在优化过程中提供了更好的梯度信息。</li>
<li>均方误差可能在某些情况下导致优化困难，尤其是在模型初始化远离最优值时。</li>
<li>选择合适的损失函数对模型的训练效果和优化效率至关重要。</li>
</ol>
<h1 id="🚩Task-2-3"><a href="#🚩Task-2-3" class="headerlink" title="🚩Task 2.3"></a>🚩Task 2.3</h1><blockquote>
<p>（实践任务）：HW3(CNN)</p>
</blockquote>
<h2 id="🎯一键运行Notebook"><a href="#🎯一键运行Notebook" class="headerlink" title="🎯一键运行Notebook"></a>🎯一键运行Notebook</h2><blockquote>
<p>通过在卷积神经网络（CNN）模型的验证集上实现t-SNE（t分布随机邻域嵌入），可视化学习到的视觉表示，包括顶层和中间层的输出。<br>绘制特定类别的t-SNE可视化图</p>
</blockquote>
<h2 id="🎯实验结果"><a href="#🎯实验结果" class="headerlink" title="🎯实验结果"></a>🎯实验结果</h2><p><img src="/img/downloaded/aHR0cHM6_e380b21460ef4d5f9276a6d7432384bb.png" alt="在这里插入图片描述"><br><img src="/img/downloaded/aHR0cHM6_4986163e25cd4ae79d05633b94d84b39.png" alt="在这里插入图片描述"></p>
<h1 id="🚩-Task-3-1"><a href="#🚩-Task-3-1" class="headerlink" title="🚩 Task 3.1"></a>🚩 Task 3.1</h1><h2 id="🎯批量归一化（Batch-Normalization，-BN）"><a href="#🎯批量归一化（Batch-Normalization，-BN）" class="headerlink" title="🎯批量归一化（Batch Normalization， BN）"></a>🎯批量归一化（Batch Normalization， BN）</h2><p><strong>直接改误差表面的地貌，“把山铲平”，让它变得比较好训练</strong></p>
<blockquote>
<p><strong>一个“把山铲平”的想法</strong>。</p>
</blockquote>
<blockquote>
<p>不要小看优化这个问题，有时候就算误差表面是凸（convex）的，它就是一个碗的形状，都不一定很好训练。如图 3.37 所示，假设两个参数对损失的斜率差别非常大，在 w1 这个方向上面，斜率变化很小，在 w2 这个方向上面斜率变化很大。</p>
</blockquote>
<p><img src="/img/downloaded/aHR0cHM6_6b26886c33f5437da7d9e462d028a26c.png" alt="在这里插入图片描述"></p>
<h2 id="🎯学习率和误差表面优化"><a href="#🎯学习率和误差表面优化" class="headerlink" title="🎯学习率和误差表面优化"></a>🎯学习率和误差表面优化</h2><p>在机器学习模型训练中，<strong>固定的学习率可能导致难以获得理想的结果。为了解决这一问题，引入了自适应学习率和高级优化算法</strong>，如Adam，这些方法能够更好地调整学习率，从而优化训练过程。</p>
<h3 id="📌误差表面分析"><a href="#📌误差表面分析" class="headerlink" title="📌误差表面分析"></a>📌误差表面分析</h3><p><strong>误差表面可能因为参数$w_1$和$w_2$的斜率差异而变得难以优化。</strong><br><strong>通过修改误差表面，尝试使其更易于优化。</strong></p>
<h3 id="📌线性模型示例"><a href="#📌线性模型示例" class="headerlink" title="📌线性模型示例"></a>📌线性模型示例</h3><p>考虑一个简单的线性模型，输入为$x_1$和$x_2$，参数为$w_1$和$w_2$。<br>模型输出$\hat{y}$计算为$\hat{y} &#x3D; w_1 \cdot x_1 + w_2 \cdot x_2 + b$。<br>损失函数$L$定义为所有训练数据误差$e$的总和，即$L &#x3D; \sum e$，其中$e &#x3D; \hat{y} y$。</p>
<h2 id="🎯特征归一化的重要性"><a href="#🎯特征归一化的重要性" class="headerlink" title="🎯特征归一化的重要性"></a>🎯特征归一化的重要性</h2><p>在训练模型时，<strong>如果输入特征的数值范围差异很大，可能会导致难以训练的误差表面</strong>。为了解决这个问题，<strong>引入了特征归一化技术，如Z值归一化（也称为标准化），它有助于使误差表面更加平滑，从而优化训练过程</strong>。</p>
<h3 id="📌Z值归一化（标准化）"><a href="#📌Z值归一化（标准化）" class="headerlink" title="📌Z值归一化（标准化）"></a>📌Z值归一化（标准化）</h3><p>归一化处理通过计算每个特征维度的平均值$m_i$和标准差$\sigma_i$，然后应用以下公式进行归一化：<br>$$\tilde{x}_i^r \leftarrow \frac{x_i^r m_i}{\sigma_i}$$<br>归一化后，特征的平均值为0，方差为1，有助于梯度下降算法更有效地收敛。</p>
<h2 id="🎯深度学习中的特征归一化"><a href="#🎯深度学习中的特征归一化" class="headerlink" title="🎯深度学习中的特征归一化"></a>🎯深度学习中的特征归一化</h2><p><strong>在深度学习中，即使输入特征已经归一化</strong>，<strong>中间层的特征</strong>$z$<strong>也可能需要进一步的归一化处理，以确保模型训练的稳定性和效率</strong>。</p>
<h3 id="📌归一化中间层特征"><a href="#📌归一化中间层特征" class="headerlink" title="📌归一化中间层特征"></a>📌归一化中间层特征</h3><p>对于中间层的特征$z$，计算其平均值$\mu$和标准差$\sigma$，然后应用以下公式进行归一化：<br>$$\mu &#x3D; \frac{1}{N} \sum_{i&#x3D;1}^N z_i$$<br>$$\sigma &#x3D; \sqrt{\frac{1}{N} \sum_{i&#x3D;1}^N (z_i - \mu)^2}$$<br>$$\tilde{z}_i &#x3D; \frac{z_i - \mu}{\sigma}$$<br>归一化可以放在激活函数之前或之后，具体取决于所使用的激活函数类型。</p>
<p>通过这些方法，可以有效地优化深度学习模型的训练过程，提高模型的性能和泛化能力。</p>
<p><img src="/img/downloaded/aHR0cHM6_505f8d8aa22c4fe39bffa4d2ec923c63.png" alt="## 考虑深度学习"><br>其中，除号代表逐元素的除，即分子分母两个向量对应元素相除。</p>
<p><img src="/img/downloaded/aHR0cHM6_6578751fd45d475ab9ed1a60c18b844f.png" alt="在这里插入图片描述"><br><img src="/img/downloaded/aHR0cHM6_7a4c7c98a30f4460b319e836792a4a10.png" alt="在这里插入图片描述"><br><img src="/img/downloaded/aHR0cHM6_f8acc678b366409bb4a0913923d7ff4b.png" alt="在这里插入图片描述"></p>
<p><img src="/img/downloaded/aHR0cHM6_e3559848a8154d44bb9bce913c587bed.png" alt="在这里插入图片描述"></p>
<p>如果做归一化以后， z˜ 的平均值一定是 0，如果平均值是 0 的话，这会给网络一些限制，这个限制可能会带来负面的影响，所以需要<strong>把 β, γ 加回去，让网络隐藏层的输出平均值不是 0。让网络学习 β, γ 来调整一下输出的分布，从而来调整 zˆ 的分布</strong></p>
<blockquote>
<p><strong>以上说的都是训练的部分，测试有时候又称为推断（inference）</strong>。</p>
</blockquote>
<h2 id="🎯批量归一化（Batch-Normalization）在测试阶段的应用"><a href="#🎯批量归一化（Batch-Normalization）在测试阶段的应用" class="headerlink" title="🎯批量归一化（Batch Normalization）在测试阶段的应用"></a>🎯批量归一化（Batch Normalization）在测试阶段的应用</h2><h3 id="📌使用移动平均和方差"><a href="#📌使用移动平均和方差" class="headerlink" title="📌使用移动平均和方差"></a>📌使用移动平均和方差</h3><p>在训练阶段，批量归一化通过计算每个批次的均值和方差来归一化数据。在测试阶段，<strong>利用训练期间累积的移动平均均值和方差来处理测试数据，确保模型的稳定性</strong>。</p>
<p><img src="/img/downloaded/aHR0cHM6_e34f524966b14ad1a4b1e4c66c216ace.png" alt="在这里插入图片描述"><br>使用训练过程中计算的移动平均均值 $\mu_{\text{moving}}$ 和方差 $\sigma^2_{\text{moving}}$ 替代批次统计量：</p>
<p>$$<br>\hat{x}<em>{\text{test}} &#x3D; \frac{x \mu</em>{\text{moving}}}{\sqrt{\sigma^2_{\text{moving}} + \epsilon}}<br>$$</p>
<p>$$<br>y_{\text{test}} &#x3D; \gamma \hat{x}_{\text{test}} + \beta<br>$$</p>
<p>这样，即使在<strong>测试时单个样本的分布可能与训练时的批次分布不同，模型仍然能够以一种稳定的方式进行推理</strong>。</p>
<h4 id="🔧不依赖批次统计"><a href="#🔧不依赖批次统计" class="headerlink" title="🔧不依赖批次统计"></a>🔧不依赖批次统计</h4><p>测试阶段不计算新的统计量，而是<strong>直接使用训练阶段得到的全局均值和方差，独立处理每个测试样本</strong>。</p>
<h4 id="🔧减少内部协变量偏移"><a href="#🔧减少内部协变量偏移" class="headerlink" title="🔧减少内部协变量偏移"></a>🔧减少内部协变量偏移</h4><p>批量归一化通过<strong>使用固定的全局统计量</strong>，继续在测试阶段减少网络各层激活分布的变化，从而<strong>提高模型的稳定性</strong>。</p>
<h4 id="🔧提高泛化能力"><a href="#🔧提高泛化能力" class="headerlink" title="🔧提高泛化能力"></a>🔧提高泛化能力</h4><p>测试阶段应用批量归一化有助于<strong>提升模型对新数据的泛化能力</strong>，<strong>减少对训练数据特定特征的过度拟合</strong>。</p>
<h4 id="🔧无需调整学习率"><a href="#🔧无需调整学习率" class="headerlink" title="🔧无需调整学习率"></a>🔧无需调整学习率</h4><p><strong>由于测试阶段不涉及梯度更新，因此不需要调整学习率</strong>，简化了模型的应用过程。</p>
<h4 id="🔧计算效率"><a href="#🔧计算效率" class="headerlink" title="🔧计算效率"></a>🔧计算效率</h4><p><strong>在测试阶段，批量归一化避免了复杂的统计计算</strong>，从而提高了推理过程的计算效率。</p>
<h4 id="🔧模型部署"><a href="#🔧模型部署" class="headerlink" title="🔧模型部署"></a>🔧模型部署</h4><p>批量归一化在模型部署时作为一个固定步骤，确保了模型在不同环境中的一致性和可靠性。</p>
<h2 id="🎯内部协变量偏移"><a href="#🎯内部协变量偏移" class="headerlink" title="🎯内部协变量偏移"></a>🎯内部协变量偏移</h2><p><strong>批量归一化最初被提出来是为了解决神经网络中的内部协变量偏移问题</strong>。这个问题说的是，网络中每一层的输入数据分布会随着前面层的参数更新而变化，这可能会让训练过程变得不太稳定。<br><strong>批量归一化通过规范化层间的输出，减少这种分布变化，帮助训练过程更稳定。</strong></p>
<p><img src="/img/downloaded/aHR0cHM6_31171673034c427c8dc262d4922a7356.png" alt="在这里插入图片描述"></p>
<h3 id="📌对优化的帮助"><a href="#📌对优化的帮助" class="headerlink" title="📌对优化的帮助"></a>📌对优化的帮助</h3><p><strong>论文《How Does Batch Normalization Help Optimization?》提出了不同的看法，质疑内部协变量偏移是否真的是训练网络的主要障碍</strong>。研究发现，<strong>即使存在内部协变量偏移，也不一定会对训练产生负面影响</strong>。实验显示，无论是否使用批量归一化，网络层输出的分布变化对训练的影响都不大，梯度方向的变化也不显著。这表明<strong>批量归一化的有效性可能并非仅仅因为它解决了内部协变量偏移</strong>。</p>
<h3 id="📌误差表面平滑化"><a href="#📌误差表面平滑化" class="headerlink" title="📌误差表面平滑化"></a>📌误差表面平滑化</h3><p><strong>论文还提出了另一个观点，即批量归一化可能通过改变网络的误差表面，使其变得更加平滑，从而有助于优化过程。这个观点得到了理论和实验的支持</strong>。论文还指出还有其他方法也可以使误差表面平滑化，效果可能与批量归一化相似或更好。</p>
<h3 id="📌归一化方法的多样性"><a href="#📌归一化方法的多样性" class="headerlink" title="📌归一化方法的多样性"></a>📌归一化方法的多样性</h3><p><strong>批量归一化不是唯一的归一化技术。实际上，存在多种归一化方法，包括批量重归一化、层归一化、实例归一化、组归一化、权重归一化和谱归一化等</strong>。可以根据具体的需求和场景选择合适的方法。</p>
<h1 id="🚩-Task-3-2-Task-3-3"><a href="#🚩-Task-3-2-Task-3-3" class="headerlink" title="🚩 Task 3.2 &amp;&amp; Task 3.3"></a>🚩 Task 3.2 &amp;&amp; Task 3.3</h1><blockquote>
<p>卷积神经网络(CNN)</p>
</blockquote>
<h2 id="🎯如何把图像输入到计算机里面"><a href="#🎯如何把图像输入到计算机里面" class="headerlink" title="🎯如何把图像输入到计算机里面"></a>🎯如何把图像输入到计算机里面</h2><blockquote>
<p>一张图像是一个<strong>三维的张量</strong>，其中<strong>一维代表图像的宽，另外一维代表图像的高，还有一维代表图像的通道（channel） 的数目</strong>。</p>
</blockquote>
<h2 id="🎯什么是卷积神经网络架构"><a href="#🎯什么是卷积神经网络架构" class="headerlink" title="🎯什么是卷积神经网络架构"></a>🎯什么是卷积神经网络架构</h2><blockquote>
<p>卷积神经网络（CNN）是一种经典的网络架构，常用于图像处理等计算机视觉任务。</p>
</blockquote>
<p><strong>通过卷积层提取图像特征，实现图像识别</strong>。</p>
<h2 id="🎯图像表示"><a href="#🎯图像表示" class="headerlink" title="🎯图像表示"></a>🎯图像表示</h2><p><img src="/img/downloaded/aHR0cHM6_b77708ce050146eea74588d076ce5af4.png" alt="在这里插入图片描述"></p>
<p><strong>图像可以被机器识别，机器通过图像识别图中的对象</strong>（如狗、飞机、汽车等）。<br><strong>彩色图像由红色、绿色、蓝色三种颜色的组合构成，每种颜色的强度不同</strong>。</p>
<h2 id="🎯向量化处理"><a href="#🎯向量化处理" class="headerlink" title="🎯向量化处理"></a>🎯向量化处理</h2><p>网络处理时，需要<strong>将图像的三维数据（宽度、高度、颜色通道）拉直成一维向量</strong>。<br><strong>每个像素点的颜色值被展开成一个向量，作为网络的输入</strong>。</p>
<h2 id="🎯标准化处理"><a href="#🎯标准化处理" class="headerlink" title="🎯标准化处理"></a>🎯标准化处理</h2><p>不同图像大小不一，通常将所有图像调整为相同大小再输入网络。<br><strong>标准化处理有助于统一网络输入，简化图像识别过程</strong>。</p>
<h2 id="🎯卷积层的作用"><a href="#🎯卷积层的作用" class="headerlink" title="🎯卷积层的作用"></a>🎯卷积层的作用</h2><p>卷积层通过卷积操作提取图像特征。<br><strong>卷积层的输出是特征图，特征图上的每个点代表图像中某种特征的存在概率</strong>。</p>
<h2 id="🎯特征图的解释"><a href="#🎯特征图的解释" class="headerlink" title="🎯特征图的解释"></a>🎯特征图的解释</h2><p>特征图上的<strong>每个点（激活值）代表网络对输入图像中特定特征的响应</strong>。<br><strong>激活值高表示相应特征在图像中出现的可能性大</strong>。</p>
<h2 id="🎯目标检测"><a href="#🎯目标检测" class="headerlink" title="🎯目标检测"></a>🎯目标检测</h2><p><img src="/img/downloaded/aHR0cHM6_f61051d5d2c54f8da474201771a4aba2.png" alt="在这里插入图片描述"></p>
<p>卷积神经网络也可用于目标检测，即<strong>识别图像中是否存在特定对象</strong>。<br><strong>通过卷积层提取的特征，可以用于判断图像中是否包含某些特定模式或对象</strong>。</p>
<h2 id="🎯感受野"><a href="#🎯感受野" class="headerlink" title="🎯感受野"></a>🎯感受野</h2><p>感受野是指网络中每个神经元“关心”的输入图像区域大小。<br><img src="/img/downloaded/aHR0cHM6_05ade20ce2b74995a9e1d16df8810bb9.png" alt="在这里插入图片描述"><br>卷积层的神经元只关注输入图像的一小部分区域，通过这种方式提取局部特征。</p>
<p> <strong>感受野是输入图像中影响特定神经元输出的局部区域。</strong></p>
<p> 每个神经元<strong>只关注输入图像的一个特定区域，这个区域称为它的感受野。</strong></p>
<h3 id="📌神经元和权重"><a href="#📌神经元和权重" class="headerlink" title="📌神经元和权重"></a>📌神经元和权重</h3><p><strong>每个神经元接收一个多维输入向量，并对每个维度赋予权重</strong>。<br>例如，一个神经元可能有 $3 \times 3 \times 3 &#x3D; 27$个权重，对应于3x3x3的输入数据。<br><img src="/img/downloaded/aHR0cHM6_c3be881dad944c0086ff8bca71937633.png" alt="在这里插入图片描述"></p>
<h3 id="📌偏置（Bias）"><a href="#📌偏置（Bias）" class="headerlink" title="📌偏置（Bias）"></a>📌偏置（Bias）</h3><p><strong>神经元除了权重外，还有一个偏置项，用于调整输出。</strong></p>
<h3 id="📌感受野的重叠"><a href="#📌感受野的重叠" class="headerlink" title="📌感受野的重叠"></a>📌感受野的重叠</h3><p><strong>不同神经元的感受野可以重叠，允许多个神经元同时响应图像中的同一个局部区域</strong>。<br>重叠的感受野有助于网络捕捉图像中的复杂特征。</p>
<p><img src="/img/downloaded/aHR0cHM6_42e7293167fe4f4e9f4538f5d1e65bfb.png" alt="在这里插入图片描述"></p>
<h3 id="📌感受野的大小和形状"><a href="#📌感受野的大小和形状" class="headerlink" title="📌感受野的大小和形状"></a>📌感受野的大小和形状</h3><p><strong>感受野的大小可以根据需要调整，不仅限于正方形，也可以是长方形或其他形状</strong>。<br>有的模式可能在小范围内就能被检测到，而有的则需要更大的范围。</p>
<h3 id="📌通道的选择性"><a href="#📌通道的选择性" class="headerlink" title="📌通道的选择性"></a>📌通道的选择性</h3><p><strong>感受野不仅可以覆盖所有颜色通道（如RGB），也可以只关注特定的颜色通道</strong>。<br>这允许网络对某些颜色通道中的特征更加敏感。</p>
<h3 id="📌设计感受野"><a href="#📌设计感受野" class="headerlink" title="📌设计感受野"></a>📌设计感受野</h3><p>设计感受野时，可以根据任务的需求和图像的特性来决定其大小、形状和覆盖的通道。<br><strong>经典的感受野安排方式会考虑如何有效地覆盖和响应图像的不同区域</strong>。</p>
<h2 id="🎯卷积操作"><a href="#🎯卷积操作" class="headerlink" title="🎯卷积操作"></a>🎯卷积操作</h2><p><img src="/img/downloaded/aHR0cHM6_f87e535e84e04b14928b66f91fcc9f14.png" alt="在这里插入图片描述"></p>
<p>卷积操作通过滑动窗口（卷积核或滤波器）在图像上移动，计算窗口内像素与卷积核值的点积。</p>
<h3 id="📌步幅"><a href="#📌步幅" class="headerlink" title="📌步幅"></a>📌步幅</h3><p><img src="/img/downloaded/aHR0cHM6_f8afb1ce9b574f77867b34dc192fdd68.png" alt="在这里插入图片描述"></p>
<blockquote>
<p>我们把左上角的感受野往右移一个步幅，就制造出一个新的守备范围，即新的感受野。移动的量称为步幅（stride） ，图 4.9 中的这个例子里面，步幅就等于 2。</p>
</blockquote>
<p>感受野移动的距离</p>
<p><strong>每次移动的步长称为步幅，步幅影响感受野的重叠程度。</strong></p>
<h3 id="📌填充"><a href="#📌填充" class="headerlink" title="📌填充"></a>📌填充</h3><p>为了<strong>处理图像边缘，避免卷积后图像尺寸过小，可以在图像边缘添加填充（padding）</strong>。<br><strong>填充通常使用零值，也可以使用其他策略。</strong></p>
<p><img src="/img/downloaded/aHR0cHM6_65f7e2df7e5d439bbaf7a72525fa52b9.png" alt="在这里插入图片描述"></p>
<blockquote>
<p>如图 4.10 所示，超出范围就做填充（padding） ，填充就是补值，一般使用零填充（zero padding），超出范围就补 0，如果感受野有一部分超出图像的范围之外，就当做那个里面的值都是 0。其实也有别的补值的方法，比如补整张图像里面所有值的平均值或者把边界的这些数字拿出来补没有值的地方。</p>
</blockquote>
<blockquote>
<p>而感受野加上参数共享就是卷积层（convolutional layer），用到卷积层的网络就叫卷积神经网络。</p>
</blockquote>
<h3 id="📌卷积层的参数共享"><a href="#📌卷积层的参数共享" class="headerlink" title="📌卷积层的参数共享"></a>📌卷积层的参数共享</h3><blockquote>
<p>如果不同的守备范围都要有一个检测鸟嘴的神经元，参数量会太多了，因此需要做出相应的简化</p>
</blockquote>
<p><img src="/img/downloaded/aHR0cHM6_f7fb8c83c2d14467a90efd3a6ff58881.png" alt="在这里插入图片描述"></p>
<blockquote>
<p>让不同感受野的神经元共享参数，也就是做<strong>参数共享（parameter sharing）</strong>，如图 4.13 所示。所谓参数共享就是两个神经元的权重完全是一样的。</p>
</blockquote>
<p><img src="/img/downloaded/aHR0cHM6_d5b91176e5bc43e79597eea40b46dd91.png" alt="在这里插入图片描述"></p>
<p>卷积层的参数（卷积核）在整个图像上共享，减少模型参数数量，提高训练效率。<br>参数共享使得网络能够学习到图像中普遍存在的模式。</p>
<h3 id="📌全连接层（Fully-Connected-Layer）"><a href="#📌全连接层（Fully-Connected-Layer）" class="headerlink" title="📌全连接层（Fully Connected Layer）"></a>📌全连接层（Fully Connected Layer）</h3><blockquote>
<p>拓展</p>
</blockquote>
<p>全连接层是神经网络中的一种层，其中每个神经元都与前一层的所有神经元相连。<br><strong>全连接层主要用于整合前一层的特征，进行最终的分类或回归分析。</strong><br>它们<strong>通常位于卷积神经网络的末尾，用于处理卷积层和池化层提取的特征</strong>。</p>
<p><strong>全连接层广泛应用于图像识别、语音处理和自然语言处理等多种任务中。<br>它们是构建深度学习模型的关键组件之一</strong>。</p>
<h4 id="🔧特点"><a href="#🔧特点" class="headerlink" title="🔧特点"></a>🔧特点</h4><p><strong>在全连接层中，每个神经元对所有输入数据进行加权求和，然后通过激活函数</strong>。</p>
<p><strong>这些层通常包含大量的参数，因为每个输入都与每个神经元相连。</strong></p>
<h4 id="🔧计算过程"><a href="#🔧计算过程" class="headerlink" title="🔧计算过程"></a>🔧计算过程</h4><p>每个神经元的输出计算公式为：$$\text{output} &#x3D; \text{activation}(\text{weights} \times \text{input} + \text{bias})$$<br>其中，weights 是权重矩阵，input 是前一层的输出，bias 是偏置项，activation 是激活函数。</p>
<h4 id="🔧激活函数"><a href="#🔧激活函数" class="headerlink" title="🔧激活函数"></a>🔧激活函数</h4><p><strong>激活函数用于引入非线性，使网络能够学习复杂的模式。</strong><br>常用的激活函数包括 <strong>ReLU、sigmoid 和 tanh。</strong></p>
<h4 id="🔧训练"><a href="#🔧训练" class="headerlink" title="🔧训练"></a>🔧训练</h4><p><strong>全连接层的权重和偏置通过反向传播算法和梯度下降进行优化。</strong><br>训练过程中，网络通过调整这些参数来最小化损失函数。</p>
<h4 id="🔧输出"><a href="#🔧输出" class="headerlink" title="🔧输出"></a>🔧输出</h4><p><strong>在分类任务中，全连接层的输出通常是一个概率分布，表示不同类别的预测概率。<br>在回归任务中，全连接层可能只有一个输出节点，直接预测连续值。</strong></p>
<p><img src="/img/downloaded/aHR0cHM6_09043dd87ad44ba983a100f8a07232d7.png" alt="在这里插入图片描述"></p>
<h3 id="📌特征映射"><a href="#📌特征映射" class="headerlink" title="📌特征映射"></a>📌特征映射</h3><p><img src="/img/downloaded/aHR0cHM6_fb4f563f365641ec981ed5682de92c22.png" alt="在这里插入图片描述"></p>
<blockquote>
<p>如果有 64 个滤波器，就可以得到 64 组的数字。这组数字称为<strong>特征映射（feature map） 。当一张图像通过一个卷积层里面一堆滤波器的时候，就会产生一个特征映射</strong>。</p>
</blockquote>
<p><img src="/img/downloaded/aHR0cHM6_5169beb0ddfe4b61a157b23c22e69a44.png" alt="在这里插入图片描述"><br><img src="/img/downloaded/aHR0cHM6_4df27931c9eb4473ad3f2c20e5994bd3.png" alt="在这里插入图片描述"></p>
<h3 id="📌多卷积核"><a href="#📌多卷积核" class="headerlink" title="📌多卷积核"></a>📌多卷积核</h3><p><strong>卷积层通常包含多个卷积核，每个卷积核负责提取图像中的不同特征。</strong><br>多个卷积核的输出可以组合成新的特征图，提供更丰富的图像表示。<br><img src="/img/downloaded/aHR0cHM6_f3addfe5f8e346dfb674ef225e61882e.png" alt="在这里插入图片描述"></p>
<blockquote>
<p>如图 4.22 所示，第 2 层的卷积里面也有一堆的滤波器，每个滤波器的大小设成 3 × 3。<strong>其高度必须设为 64，因为滤波器的高度就是它要处理的图像的通道</strong>。如果输入的图像是黑白的，通道是 1，滤波器的高度就是 1。</p>
</blockquote>
<blockquote>
<p><strong>而共享权重其实就是用滤波器扫过一张图像，这个过程就是卷积</strong>。这就是卷积层名字的由来。把滤波器扫过图像就相当于不同的感受野神经元可以共用参数，这组共用的参数就叫做一个滤波器。</p>
</blockquote>
<h3 id="📌采样"><a href="#📌采样" class="headerlink" title="📌采样"></a>📌采样</h3><blockquote>
<p>把一张比较大的图像做下采样（downsampling），把图像偶数的列都拿掉，奇数的行都拿掉，图像变成为原来的 1&#x2F;4，但是不会影响里面是什么东西。</p>
</blockquote>
<p><img src="/img/downloaded/aHR0cHM6_b9534762754e4a06b55b63892a438da3.png" alt="在这里插入图片描述"></p>
<h3 id="📌汇聚"><a href="#📌汇聚" class="headerlink" title="📌汇聚"></a>📌汇聚</h3><blockquote>
<p><strong>做完卷积以后，往往后面还会搭配汇聚</strong>。汇聚就是把图像变小。做完卷积以后会得到一张图像，这张图像里面有很多的通道。做完汇聚以后，这张图像的通道不变。</p>
</blockquote>
<p><img src="/img/downloaded/aHR0cHM6_50148c5c6437431b9e25f402d3f803b4.png" alt="在这里插入图片描述"></p>
<blockquote>
<p>图 4.27 中的例子是 2 × 2 个一组。汇聚有很多不同的版本，以最大汇聚（max pooling） 为例。<strong>最大汇聚在每一组里面选一个代表，选的代表就是最大的一个</strong>，如图 4.28 所示。<strong>除了最大汇聚，还有平均汇聚（mean pooling），平均汇聚是取每一组的平均值</strong>。</p>
</blockquote>
<p><strong>汇聚可能会对模型的性能造成一定的损害。</strong><br><strong>特别是在检测非常细微的特征时，进行下采样可能会使性能稍微降低</strong>。</p>
<h4 id="🔧全卷积网络的趋势"><a href="#🔧全卷积网络的趋势" class="headerlink" title="🔧全卷积网络的趋势"></a>🔧全卷积网络的趋势</h4><p>近年来，图像网络设计趋向于舍弃汇聚层，转而使用全卷积网络。全卷积网络中整个网络结构都是卷积层，不使用汇聚层，这种设计可以保持更多的空间信息，有助于特征的提取。</p>
<h4 id="🔧汇聚的主要作用"><a href="#🔧汇聚的主要作用" class="headerlink" title="🔧汇聚的主要作用"></a>🔧汇聚的主要作用</h4><p><strong>汇聚的主要作用是减少运算量。</strong> 通过下采样缩小图像尺寸，从而减少计算量，这在<strong>资源有限的情况下非常有用</strong>。</p>
<h4 id="🔧运算能力的提升"><a href="#🔧运算能力的提升" class="headerlink" title="🔧运算能力的提升"></a>🔧运算能力的提升</h4><p>随着运算能力的提升，如果有足够的运算资源，<strong>很多网络架构设计选择不使用汇聚</strong>。<strong>采用全卷积设计</strong>，从开始到结束都使用卷积层，探索是否可以取得更好的效果。</p>
<h4 id="🔧一般网络架构"><a href="#🔧一般网络架构" class="headerlink" title="🔧一般网络架构"></a>🔧一般网络架构</h4><p><strong>传统的网络架构通常包括卷积层和汇聚层。汇聚层是可有可无的，许多设计选择不使用汇聚层，以避免可能的性能损失。</strong></p>
<h4 id="🔧架构示例"><a href="#🔧架构示例" class="headerlink" title="🔧架构示例"></a>🔧架构示例</h4><p><strong>在完成卷积和汇聚后，通常将汇聚的输出扁平化，形成一维向量。然后将这个向量输入到全连接层中，最终通过 softmax 层得到图像识别的结果</strong>。<br><img src="/img/downloaded/aHR0cHM6_9917eead742f4276987292fbbe7a1862.png" alt="在这里插入图片描述"><br>这是一个经典的图像识别网络，包括卷积层、汇聚层、扁平化处理，以及全连接层或 softmax 层。</p>
<h2 id="🎯卷积神经网络的应用"><a href="#🎯卷积神经网络的应用" class="headerlink" title="🎯卷积神经网络的应用"></a>🎯卷积神经网络的应用</h2><p>卷积神经网络广泛应用于图像识别、目标检测等领域。<br>通过学习图像特征，卷积神经网络能够识别和分类图像中的不同对象。</p>
<h2 id="🎯围棋落子预测"><a href="#🎯围棋落子预测" class="headerlink" title="🎯围棋落子预测"></a>🎯围棋落子预测</h2><p>卷积神经网络也可用于围棋等策略游戏，预测下一步最佳落子位置。<br>通过分析棋盘状态，网络可以评估每个位置的重要性，指导决策。</p>
<h1 id="🚩自注意力机制"><a href="#🚩自注意力机制" class="headerlink" title="🚩自注意力机制"></a>🚩自注意力机制</h1><h2 id="🎯自注意力模型（Self-Attention-Model）"><a href="#🎯自注意力模型（Self-Attention-Model）" class="headerlink" title="🎯自注意力模型（Self-Attention Model）"></a>🎯自注意力模型（Self-Attention Model）</h2><blockquote>
<p>自注意力模型是深度学习中处理序列数据的一种重要架构，尤其适用于处理输入序列长度可变的问题。</p>
</blockquote>
<h3 id="📌输入与输出"><a href="#📌输入与输出" class="headerlink" title="📌输入与输出"></a>📌输入与输出</h3><p><img src="/img/downloaded/aHR0cHM6_9e2d9e86cb494283bff6748c3fd9494b.png" alt="在这里插入图片描述"></p>
<p><strong>输入通常是向量序列，输出可以是标量、类别或另一个向量序列。</strong></p>
<p>自注意力模型能够处理输入序列长度不一的情况，适用于文本、语音、图数据等多种序列任务。</p>
<h3 id="📌序列处理的挑战"><a href="#📌序列处理的挑战" class="headerlink" title="📌序列处理的挑战"></a>📌序列处理的挑战</h3><p>传统的卷积或全连接网络在处理序列数据时，可能<strong>因固定窗口大小或不考虑序列间长距离依赖而受限</strong>。</p>
<h3 id="📌自注意力机制"><a href="#📌自注意力机制" class="headerlink" title="📌自注意力机制"></a>📌自注意力机制</h3><p><strong>自注意力模型通过计算序列中每个元素对其他所有元素的关联程度（注意力分数），来捕捉序列内的长距离依赖关系</strong>。</p>
<p>模型不需要预设固定大小的窗口，能够动态地关注序列中任意距离的依赖。</p>
<h3 id="📌独热编码与词嵌入"><a href="#📌独热编码与词嵌入" class="headerlink" title="📌独热编码与词嵌入"></a>📌独热编码与词嵌入</h3><p><strong>独热编码是一种将词汇表示为向量的方法，但这种方法无法表达词汇之间的语义关系</strong>。</p>
<p><strong>词嵌入（Word Embedding）通过将词汇映射到包含语义信息的向量空间，能够更好地捕捉词汇之间的关系</strong>。</p>
<h3 id="📌序列到序列的任务"><a href="#📌序列到序列的任务" class="headerlink" title="📌序列到序列的任务"></a>📌序列到序列的任务</h3><p>一些任务如机器翻译，输入和输出序列的长度可能不同，自注意力模型能够灵活处理这类序列到序列的任务。</p>
<h3 id="📌注意力分数的计算"><a href="#📌注意力分数的计算" class="headerlink" title="📌注意力分数的计算"></a>📌注意力分数的计算</h3><p><img src="/img/downloaded/aHR0cHM6_5e66d30130d64898bd5b22a77df6763e.png" alt="在这里插入图片描述"></p>
<p><strong>注意力分数通过查询（Query）、键（Key）和值（Value）的机制计算得出。</strong></p>
<p><img src="/img/downloaded/aHR0cHM6_9846e5485ece4f7aacf74e1b9f4d264f.png" alt="在这里插入图片描述"></p>
<p><strong>常见的计算方法包括点积（Dot Product）和相加（Additive）等</strong>。</p>
<h3 id="📌多头注意力"><a href="#📌多头注意力" class="headerlink" title="📌多头注意力"></a>📌多头注意力</h3><p><strong>多头注意力（Multi-Head Attention）是自注意力的一种扩展，它将输入向量分割成多个头，每个头计算不同的注意力，最终再将结果合并，以捕获不同子空间的信息</strong>。</p>
<h3 id="📌Transformer-架构"><a href="#📌Transformer-架构" class="headerlink" title="📌Transformer 架构"></a>📌Transformer 架构</h3><p>Transformer 是一种<strong>完全基于自注意力机制的网络架构，广泛应用于自然语言处理任务</strong>。<br>Transformer 通过<strong>堆叠多个自注意力层和前馈神经网络层，并通过残差连接和层归一化来提高训练效率和性能</strong>。</p>
<h3 id="📌应用实例"><a href="#📌应用实例" class="headerlink" title="📌应用实例"></a>📌应用实例</h3><p><strong>文本处理：如情感分析、词性标注、机器翻译。<br>语音处理：如语音识别、语音合成。<br>图数据：如社交网络分析、药物分子发现。</strong></p>
<blockquote>
<p><strong>自注意力模型通过其灵活的注意力机制，为深度学习在序列数据处理方面提供了强大的工具，推动了多个领域的发展。</strong></p>
</blockquote>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="https://canjisam.github.io">CanJisam</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="https://canjisam.github.io/2025/03/10/%E6%9D%8E%E5%AE%8F%E6%AF%85%E8%8B%B9%E6%9E%9C%E4%B9%A6%E8%AF%BB%E4%B9%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">https://canjisam.github.io/2025/03/10/%E6%9D%8E%E5%AE%8F%E6%AF%85%E8%8B%B9%E6%9E%9C%E4%B9%A6%E8%AF%BB%E4%B9%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来源 <a href="https://canjisam.github.io" target="_blank">诒森的博客</a>！</span></div></div><div class="tag_share"><div class="post-share"><div class="social-share" data-image="/img/cover/9f13ef7ffc274ea1974a8c4bd3849b26_2.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related" href="/2025/03/10/408%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E8%80%83%E7%A0%94%E5%A4%A7%E7%BA%B2%E8%AF%A6%E8%A7%A3/" title="408数据结构考研大纲详解"><img class="cover" src="/img/cover/eca45645431b46fea6db1634a45287b2_2.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="info"><div class="info-1"><div class="info-item-1">上一篇</div><div class="info-item-2">408数据结构考研大纲详解</div></div><div class="info-2"><div class="info-item-1">408数据结构考研大纲详解本文根据计算机专业考研408数据结构大纲，系统地整理了数据结构的核心知识点，包括基本概念、线性表、栈与队列、树与二叉树、图、查找和排序等内容。每个部分都包含了定义、性质、基本操作及其算法实现、时间复杂度分析和典型应用场景，帮助考生全面掌握数据结构的重要知识点。 一、绪论1. 基本概念数据结构是计算机存储、组织数据的方式。数据结构是指相互之间存在一种或多种特定关系的数据元素的集合。 基本术语：  数据：描述客观事物的符号，是计算机中可以操作的对象 数据元素：数据的基本单位 数据项：构成数据元素的不可分割的最小单位 数据对象：性质相同的数据元素的集合 数据类型：一组性质相同的值的集合及定义在此集合上的一组操作 抽象数据类型(ADT)：一个数学模型及定义在该模型上的一组操作  2....</div></div></div></a><a class="pagination-related" href="/2025/03/10/%E5%A6%82%E4%BD%95%E6%9B%B4%E6%96%B0GitHub%E5%8D%9A%E5%AE%A2/" title="如何更新GitHub博客"><img class="cover" src="/img/cover/eca45645431b46fea6db1634a45287b2_3.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="info text-right"><div class="info-1"><div class="info-item-1">下一篇</div><div class="info-item-2">如何更新GitHub博客</div></div><div class="info-2"><div class="info-item-1">如何更新GitHub博客本文将介绍如何使用Hexo框架更新部署在GitHub Pages上的博客。 准备工作在开始之前，确保你已经安装了以下工具：  Node.js和npm Git Hexo CLI  更新博客的步骤1. 创建新文章使用以下命令创建一篇新文章： 1$ hexo new &quot;文章标题&quot;  这将在source/_posts目录下创建一个新的Markdown文件。 2. 编辑文章使用你喜欢的文本编辑器打开新创建的Markdown文件，编辑文章内容。Markdown文件的开头是文章的前置信息，包括标题、日期、标签等。 12345678---title: 文章标题date: 2023-01-01 12:00:00tags: [标签1, 标签2]categories: [分类]---这里是文章内容...  3. 本地预览编辑完成后，可以在本地预览效果： 12$ hexo clean   # 清除之前生成的文件$ hexo server  # 启动本地服务器  访问 http://localhost:4000 查看效果。 4....</div></div></div></a></nav><hr class="custom-hr"/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div id="gitalk-container"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="/img/cover/avator.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">CanJisam</div><div class="author-info-description"></div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">57</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">101</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">29</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/canjisam"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons"><a class="social-icon" href="https://github.com/canjisam" target="_blank" title="Github"><i class="fab fa-github" style="color: #24292e;"></i></a><a class="social-icon" href="mailto:canjisam@qq.com" target="_blank" title="Email"><i class="fas fa-envelope" style="color: #4a7dbe;"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">欢迎关注微信公众号：昨宵梦寐与君语（国内访问更稳定）</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content is-expand"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%AD%A6%E4%B9%A0%E7%9B%AE%E6%A0%87%EF%BC%9A"><span class="toc-number">1.</span> <span class="toc-text">学习目标：</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%AD%A6%E4%B9%A0%E5%86%85%E5%AE%B9%EF%BC%9A"><span class="toc-number">2.</span> <span class="toc-text">学习内容：</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Task-1-%E9%80%9A%E8%BF%87%E6%A1%88%E4%BE%8B%E4%BA%86%E8%A7%A3%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0"><span class="toc-number">3.</span> <span class="toc-text">Task 1 通过案例了解机器学习</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88Machine-Learning%EF%BC%8CML%EF%BC%89%E5%92%8C%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%EF%BC%88Deep-Learning%EF%BC%8CDL%EF%BC%89%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5"><span class="toc-number">3.1.</span> <span class="toc-text">机器学习（Machine Learning，ML）和深度学习（Deep Learning，DL）的基本概念</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BB%80%E4%B9%88%E6%98%AF%E5%9B%9E%E5%BD%92%EF%BC%88regression%EF%BC%89"><span class="toc-number">3.2.</span> <span class="toc-text">什么是回归（regression）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BB%80%E4%B9%88%E6%98%AF%E5%88%86%E7%B1%BB%EF%BC%88classification%EF%BC%89"><span class="toc-number">3.3.</span> <span class="toc-text">什么是分类（classification）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BB%80%E4%B9%88%E6%98%AF%E7%BB%93%E6%9E%84%E5%8C%96%E5%AD%A6%E4%B9%A0"><span class="toc-number">3.4.</span> <span class="toc-text">什么是结构化学习</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%89%BE%E5%87%BD%E6%95%B0%E7%9A%84%E4%B8%89%E4%B8%AA%E6%AD%A5%E9%AA%A4"><span class="toc-number">3.5.</span> <span class="toc-text">机器学习找函数的三个步骤</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AC%AC1%E4%B8%AA%E6%AD%A5%E9%AA%A4%E6%98%AF%E5%86%99%E5%87%BA%E4%B8%80%E4%B8%AA%E5%B8%A6%E6%9C%89%E6%9C%AA%E7%9F%A5%E5%8F%82%E6%95%B0%E7%9A%84%E5%87%BD%E6%95%B0f%EF%BC%8C%E5%85%B6%E8%83%BD%E9%A2%84%E6%B5%8B%E6%9C%AA%E6%9D%A5%E8%A7%82%E7%9C%8B%E6%AC%A1%E6%95%B0%E3%80%82"><span class="toc-number">3.5.1.</span> <span class="toc-text">第1个步骤是写出一个带有未知参数的函数f，其能预测未来观看次数。</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AC%AC2%E4%B8%AA%E6%AD%A5%E9%AA%A4%E6%98%AF%E5%AE%9A%E4%B9%89%E6%8D%9F%E5%A4%B1%EF%BC%88loss%EF%BC%89%EF%BC%8C%E6%8D%9F%E5%A4%B1%E4%B9%9F%E6%98%AF%E4%B8%80%E4%B8%AA%E5%87%BD%E6%95%B0%E3%80%82"><span class="toc-number">3.5.2.</span> <span class="toc-text">第2个步骤是定义损失（loss），损失也是一个函数。</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E7%AC%AC-3-%E6%AD%A5%EF%BC%9A%E8%A7%A3%E4%B8%80%E4%B8%AA%E6%9C%80%E4%BC%98%E5%8C%96%E7%9A%84%E9%97%AE%E9%A2%98%E3%80%82"><span class="toc-number">3.5.3.</span> <span class="toc-text">机器学习的第 3 步：解一个最优化的问题。</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88%E6%8D%9F%E5%A4%B1%E5%8F%AF%E4%BB%A5%E6%98%AF%E8%B4%9F%E7%9A%84%EF%BC%9F"><span class="toc-number">3.6.</span> <span class="toc-text">为什么损失可以是负的？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%9C%89%E4%B8%80%E4%B8%AA%E5%BE%88%E5%A4%A7%E7%9A%84%E9%97%AE%E9%A2%98"><span class="toc-number">3.7.</span> <span class="toc-text">梯度下降有一个很大的问题</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Task-2-%E4%BA%86%E8%A7%A3%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B"><span class="toc-number">4.</span> <span class="toc-text">Task 2  了解线性模型</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B"><span class="toc-number">4.1.</span> <span class="toc-text">线性模型</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%86%E6%AE%B5%E7%BA%BF%E6%80%A7%E6%9B%B2%E7%BA%BF"><span class="toc-number">4.1.1.</span> <span class="toc-text">分段线性曲线</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A6%82%E4%BD%95%E8%A1%A8%E7%A4%BA%E6%96%B9%E7%A8%8B"><span class="toc-number">4.1.2.</span> <span class="toc-text">如何表示方程</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BC%98%E5%8C%96%E6%98%AF%E6%89%BE%E4%B8%80%E4%B8%AA%E5%8F%AF%E4%BB%A5%E8%AE%A9%E6%8D%9F%E5%A4%B1%E6%9C%80%E5%B0%8F%E7%9A%84%E5%8F%82%E6%95%B0%EF%BC%8C%E6%98%AF%E5%90%A6%E5%8F%AF%E4%BB%A5%E7%A9%B7%E4%B8%BE%E6%89%80%E6%9C%89%E5%8F%AF%E8%83%BD%E7%9A%84%E6%9C%AA%E7%9F%A5%E5%8F%82%E6%95%B0%E7%9A%84%E5%80%BC%EF%BC%9F"><span class="toc-number">4.1.2.1.</span> <span class="toc-text">优化是找一个可以让损失最小的参数，是否可以穷举所有可能的未知参数的值？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%88%9A%E6%89%8D%E7%9A%84%E4%BE%8B%E5%AD%90%E9%87%8C%E9%9D%A2%E6%9C%89-3-%E4%B8%AA-Sigmoid%EF%BC%8C%E4%B8%BA%E4%BB%80%E4%B9%88%E6%98%AF-3-%E4%B8%AA%EF%BC%8C%E8%83%BD%E4%B8%8D%E8%83%BD-4-%E4%B8%AA%E6%88%96%E6%9B%B4%E5%A4%9A%EF%BC%9F"><span class="toc-number">4.1.2.2.</span> <span class="toc-text">刚才的例子里面有 3 个 Sigmoid，为什么是 3 个，能不能 4 个或更多？</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AE%9A%E4%B9%89%E6%8D%9F%E5%A4%B1"><span class="toc-number">4.1.3.</span> <span class="toc-text">定义损失</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AE%9E%E7%8E%B0%E4%B8%8A%E7%9A%84%E7%BB%86%E8%8A%82"><span class="toc-number">4.1.4.</span> <span class="toc-text">实现上的细节</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%89%B9%E9%87%8F%EF%BC%88batch%EF%BC%89"><span class="toc-number">4.1.4.1.</span> <span class="toc-text">批量（batch）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%9B%9E%E5%90%88%EF%BC%88epoch%EF%BC%89"><span class="toc-number">4.1.4.2.</span> <span class="toc-text">回合（epoch）</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E5%8F%98%E5%BD%A2"><span class="toc-number">4.2.</span> <span class="toc-text">模型变形</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%EF%BC%88activation-function%EF%BC%89"><span class="toc-number">4.2.1.</span> <span class="toc-text">激活函数（activation function）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0"><span class="toc-number">4.2.2.</span> <span class="toc-text">深度学习</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Task-3-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A1%86%E6%9E%B6-%E5%AE%9E%E8%B7%B5%E6%94%BB%E7%95%A5"><span class="toc-number">5.</span> <span class="toc-text">Task 3 机器学习框架&amp;实践攻略</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A1%86%E6%9E%B6"><span class="toc-number">5.1.</span> <span class="toc-text">机器学习框架</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AE%9A%E4%B9%89%E5%87%BD%E6%95%B0f%CE%B8-x"><span class="toc-number">5.1.1.</span> <span class="toc-text">定义函数fθ(x)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AE%9A%E4%B9%89%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="toc-number">5.1.2.</span> <span class="toc-text">定义损失函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BC%98%E5%8C%96%E9%97%AE%E9%A2%98%E6%B1%82%E8%A7%A3"><span class="toc-number">5.1.3.</span> <span class="toc-text">优化问题求解</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%BA%94%E7%94%A8%E4%BA%8E%E6%B5%8B%E8%AF%95%E6%95%B0%E6%8D%AE"><span class="toc-number">5.1.4.</span> <span class="toc-text">应用于测试数据</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%8F%90%E4%BA%A4%E5%88%B0Kaggle%E8%BF%9B%E8%A1%8C%E8%AF%84%E4%BC%B0"><span class="toc-number">5.1.5.</span> <span class="toc-text">提交到Kaggle进行评估</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%9E%E8%B7%B5%E6%96%B9%E6%B3%95%E8%AE%BA"><span class="toc-number">5.2.</span> <span class="toc-text">实践方法论</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88%E4%BC%9A%E5%87%BA%E7%8E%B0%E6%A8%A1%E5%9E%8B%E5%81%8F%E5%B7%AE"><span class="toc-number">5.2.1.</span> <span class="toc-text">为什么会出现模型偏差</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BC%98%E5%8C%96%E9%97%AE%E9%A2%98"><span class="toc-number">5.2.2.</span> <span class="toc-text">优化问题</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A6%82%E4%BD%95%E5%88%A4%E6%96%AD%E6%A8%A1%E5%9E%8B%E6%98%AF%E5%90%A6%E8%B6%B3%E5%A4%9F%E5%A4%A7%EF%BC%9F"><span class="toc-number">5.2.3.</span> <span class="toc-text">如何判断模型是否足够大？</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%BF%87%E6%8B%9F%E5%90%88"><span class="toc-number">5.2.4.</span> <span class="toc-text">过拟合</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88%E4%BC%9A%E6%9C%89%E8%BF%87%E6%8B%9F%E5%90%88%E8%BF%99%E6%A0%B7%E7%9A%84%E6%83%85%E5%86%B5%E5%91%A2%EF%BC%9F"><span class="toc-number">5.2.5.</span> <span class="toc-text">为什么会有过拟合这样的情况呢？</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%81%B5%E6%B4%BB%E6%80%A7%E5%A4%AA%E5%A4%A7%E5%B8%A6%E6%9D%A5%E7%9A%84%E9%97%AE%E9%A2%98"><span class="toc-number">5.2.6.</span> <span class="toc-text">灵活性太大带来的问题</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A6%82%E4%BD%95%E8%A7%A3%E5%86%B3%E8%BF%87%E6%8B%9F%E5%90%88%E9%97%AE%E9%A2%98"><span class="toc-number">5.2.7.</span> <span class="toc-text">如何解决过拟合问题</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BA%A4%E5%8F%89%E9%AA%8C%E8%AF%81"><span class="toc-number">5.2.8.</span> <span class="toc-text">交叉验证</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#k-%E6%8A%98%E4%BA%A4%E5%8F%89%E9%AA%8C%E8%AF%81"><span class="toc-number">5.2.9.</span> <span class="toc-text">k 折交叉验证</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%8D%E5%8C%B9%E9%85%8D"><span class="toc-number">5.2.10.</span> <span class="toc-text">不匹配</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%F0%9F%9A%A9%E5%AD%A6%E4%B9%A0%E7%9B%AE%E6%A0%87"><span class="toc-number">6.</span> <span class="toc-text">🚩学习目标</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%F0%9F%9A%A9%E5%AD%A6%E4%B9%A0%E5%86%85%E5%AE%B9"><span class="toc-number">7.</span> <span class="toc-text">🚩学习内容</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%F0%9F%9A%A9-Task1-1"><span class="toc-number">8.</span> <span class="toc-text">🚩 Task1.1</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%F0%9F%8E%AF%E4%B8%BA%E4%BB%80%E4%B9%88%E4%BC%98%E5%8C%96%E4%BC%9A%E5%A4%B1%E8%B4%A5"><span class="toc-number">8.1.</span> <span class="toc-text">🎯为什么优化会失败</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%F0%9F%93%8C%E5%9B%A0%E9%9D%9E%E4%BF%A1%E6%81%AF%E6%A2%AF%E5%BA%A6%E5%AF%BC%E8%87%B4%E7%9A%84%E5%A4%B1%E8%B4%A5"><span class="toc-number">8.1.1.</span> <span class="toc-text">📌因非信息梯度导致的失败</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%F0%9F%8E%AF%E5%B1%80%E9%83%A8%E6%9E%81%E5%B0%8F%E5%80%BC%E4%B8%8E%E9%9E%8D%E7%82%B9"><span class="toc-number">8.2.</span> <span class="toc-text">🎯局部极小值与鞍点</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%F0%9F%8E%AF%E4%B8%B4%E7%95%8C%E7%82%B9%E5%8F%8A%E5%85%B6%E7%A7%8D%E7%B1%BB"><span class="toc-number">8.3.</span> <span class="toc-text">🎯临界点及其种类</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%F0%9F%8E%AF%E5%A6%82%E4%BD%95%E5%88%A4%E6%96%AD%E4%B8%B4%E7%95%8C%E5%80%BC%E7%A7%8D%E7%B1%BB"><span class="toc-number">8.4.</span> <span class="toc-text">🎯如何判断临界值种类</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%F0%9F%93%8C%E6%9B%B4%E7%AE%80%E4%BE%BF%E7%9A%84%E6%96%B9%E6%B3%95%E6%9D%A5%E5%88%A4%E6%96%AD-v-TH-v-%E7%9A%84%E6%AD%A3%E8%B4%9F%E3%80%82"><span class="toc-number">8.4.1.</span> <span class="toc-text">📌更简便的方法来判断 $v^TH_v$  的正负。</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%F0%9F%8E%AFH-%E6%80%8E%E4%B9%88%E5%91%8A%E8%AF%89%E6%88%91%E4%BB%AC%E6%80%8E%E4%B9%88%E6%9B%B4%E6%96%B0%E5%8F%82%E6%95%B0%E5%91%A2%EF%BC%9F"><span class="toc-number">8.5.</span> <span class="toc-text">🎯H 怎么告诉我们怎么更新参数呢？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%F0%9F%8E%AF%E5%A6%82%E4%BD%95%E9%80%83%E7%A6%BB%E9%9E%8D%E7%82%B9"><span class="toc-number">8.6.</span> <span class="toc-text">🎯如何逃离鞍点</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%F0%9F%9A%A9-Task1-2"><span class="toc-number">9.</span> <span class="toc-text">🚩 Task1.2</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%F0%9F%8E%AF%E4%BB%80%E4%B9%88%E6%98%AF%E6%89%B9%E9%87%8F%E5%92%8C%E5%8A%A8%E9%87%8F"><span class="toc-number">9.1.</span> <span class="toc-text">🎯什么是批量和动量</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%F0%9F%93%8C%E6%89%B9%E9%87%8F%E5%A4%A7%E5%B0%8F%E5%AF%B9%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95%E7%9A%84%E5%BD%B1%E5%93%8D"><span class="toc-number">9.1.1.</span> <span class="toc-text">📌批量大小对梯度下降法的影响</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%F0%9F%94%A7%E6%89%B9%E9%87%8F%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95%EF%BC%88Batch-Gradient-Descent%EF%BC%8CBGD%EF%BC%89"><span class="toc-number">9.1.1.1.</span> <span class="toc-text">🔧批量梯度下降法（Batch Gradient Descent，BGD）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%F0%9F%94%A7%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95%EF%BC%88Stochastic-Gradient-Descent%EF%BC%8CSGD%EF%BC%89%EF%BC%8C%E4%B9%9F%E7%A7%B0%E4%B8%BA%E5%A2%9E%E9%87%8F%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95"><span class="toc-number">9.1.1.2.</span> <span class="toc-text">🔧随机梯度下降法（Stochastic Gradient Descent，SGD），也称为增量梯度下降法</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%F0%9F%94%A7%E6%89%B9%E9%87%8F%E5%A4%A7%E5%B0%8F%E4%B8%8E%E8%AE%A1%E7%AE%97%E6%97%B6%E9%97%B4%E7%9A%84%E5%85%B3%E7%B3%BB"><span class="toc-number">9.1.1.3.</span> <span class="toc-text">🔧批量大小与计算时间的关系</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%F0%9F%8E%AF%E4%BB%80%E4%B9%88%E6%98%AF%E5%8A%A8%E9%87%8F%E6%B3%95"><span class="toc-number">9.2.</span> <span class="toc-text">🎯什么是动量法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%F0%9F%93%8C%E5%AF%B9%E6%AF%94%E4%B8%80%E8%88%AC%E7%9A%84%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95%E5%92%8C%E5%8A%A8%E9%87%8F%E6%B3%95"><span class="toc-number">9.2.1.</span> <span class="toc-text">📌对比一般的梯度下降法和动量法</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%F0%9F%94%A7%E5%8A%A8%E9%87%8F%E6%B3%95%E7%9A%84%E4%B8%BB%E8%A6%81%E4%BC%98%E7%82%B9"><span class="toc-number">9.2.1.1.</span> <span class="toc-text">🔧动量法的主要优点</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%F0%9F%94%A7%E5%8A%A8%E9%87%8F%E6%B3%95%E4%B9%9F%E5%AD%98%E5%9C%A8%E4%B8%80%E4%BA%9B%E7%BC%BA%E7%82%B9%E3%80%82"><span class="toc-number">9.2.1.2.</span> <span class="toc-text">🔧动量法也存在一些缺点。</span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%F0%9F%9A%A9Task-2-1"><span class="toc-number">10.</span> <span class="toc-text">🚩Task 2.1</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%F0%9F%8E%AF%E4%BB%80%E4%B9%88%E6%98%AF%E8%87%AA%E9%80%82%E5%BA%94%E5%AD%A6%E4%B9%A0%E7%8E%87"><span class="toc-number">10.1.</span> <span class="toc-text">🎯什么是自适应学习率</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%F0%9F%8E%AFAdaGrad"><span class="toc-number">10.2.</span> <span class="toc-text">🎯AdaGrad</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%F0%9F%93%8C%E5%8F%82%E6%95%B0%E6%9B%B4%E6%96%B0%E5%92%8C%E5%AD%A6%E4%B9%A0%E7%8E%87%E8%B0%83%E6%95%B4"><span class="toc-number">10.2.1.</span> <span class="toc-text">📌参数更新和学习率调整</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%F0%9F%94%A7%E5%9F%BA%E6%9C%AC%E5%8F%82%E6%95%B0%E6%9B%B4%E6%96%B0%E5%85%AC%E5%BC%8F"><span class="toc-number">10.2.1.1.</span> <span class="toc-text">🔧基本参数更新公式</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%F0%9F%94%A7%E6%A2%AF%E5%BA%A6%E8%AE%A1%E7%AE%97"><span class="toc-number">10.2.1.2.</span> <span class="toc-text">🔧梯度计算</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%F0%9F%94%A7%E5%AE%9A%E5%88%B6%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%8E%87"><span class="toc-number">10.2.1.3.</span> <span class="toc-text">🔧定制化学习率</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%F0%9F%94%A7%E6%A2%AF%E5%BA%A6%E7%9A%84%E5%9D%87%E6%96%B9%E6%A0%B9"><span class="toc-number">10.2.1.4.</span> <span class="toc-text">🔧梯度的均方根</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%F0%9F%94%A7%E5%8F%82%E6%95%B0%E6%9B%B4%E6%96%B0%E7%9A%84%E8%BF%AD%E4%BB%A3%E8%BF%87%E7%A8%8B"><span class="toc-number">10.2.1.5.</span> <span class="toc-text">🔧参数更新的迭代过程</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%F0%9F%94%A7%E8%BF%AD%E4%BB%A3%E6%9B%B4%E6%96%B0%E5%85%AC%E5%BC%8F"><span class="toc-number">10.2.1.6.</span> <span class="toc-text">🔧迭代更新公式</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%F0%9F%94%A7%E5%8F%82%E6%95%B0%E6%9B%B4%E6%96%B0%E7%9A%84%E5%8A%A8%E6%80%81%E8%B0%83%E6%95%B4"><span class="toc-number">10.2.1.7.</span> <span class="toc-text">🔧参数更新的动态调整</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%F0%9F%94%A7%E5%8F%82%E6%95%B0%E6%9B%B4%E6%96%B0%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96"><span class="toc-number">10.2.1.8.</span> <span class="toc-text">🔧参数更新的可视化</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%F0%9F%93%8CAdaGrad%E7%AE%97%E6%B3%95%E7%9A%84%E9%97%AE%E9%A2%98"><span class="toc-number">10.2.2.</span> <span class="toc-text">📌AdaGrad算法的问题</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%F0%9F%8E%AFRMSProp"><span class="toc-number">10.3.</span> <span class="toc-text">🎯RMSProp</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%F0%9F%93%8C%E7%AE%97%E6%B3%95%E7%9A%84%E6%AD%A5%E9%AA%A4"><span class="toc-number">10.3.1.</span> <span class="toc-text">📌算法的步骤</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%F0%9F%94%A7%E5%88%9D%E5%A7%8B%E6%A2%AF%E5%BA%A6%E7%9A%84%E5%9D%87%E6%96%B9%E6%A0%B9"><span class="toc-number">10.3.1.1.</span> <span class="toc-text">🔧初始梯度的均方根</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%F0%9F%93%8C%E5%8F%82%E6%95%B0%E6%9B%B4%E6%96%B0%E5%85%AC%E5%BC%8F"><span class="toc-number">10.3.2.</span> <span class="toc-text">📌参数更新公式</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%F0%9F%94%A7%E8%BF%AD%E4%BB%A3%E6%9B%B4%E6%96%B0%E8%BF%87%E7%A8%8B"><span class="toc-number">10.3.2.1.</span> <span class="toc-text">🔧迭代更新过程</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%F0%9F%93%8C%E7%AE%97%E6%B3%95%E7%89%B9%E6%80%A7"><span class="toc-number">10.3.3.</span> <span class="toc-text">📌算法特性</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%F0%9F%8E%AFAdam"><span class="toc-number">10.4.</span> <span class="toc-text">🎯Adam</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%F0%9F%8E%AF%E5%AD%A6%E4%B9%A0%E7%8E%87%E8%B0%83%E5%BA%A6"><span class="toc-number">10.5.</span> <span class="toc-text">🎯学习率调度</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%F0%9F%93%8C%E5%AD%A6%E4%B9%A0%E7%8E%87%E8%A1%B0%E5%87%8F"><span class="toc-number">10.5.1.</span> <span class="toc-text">📌学习率衰减</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%F0%9F%93%8C%E5%AD%A6%E4%B9%A0%E7%8E%87%E8%B0%83%E5%BA%A6%E7%9A%84%E5%85%AC%E5%BC%8F"><span class="toc-number">10.5.2.</span> <span class="toc-text">📌学习率调度的公式</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%F0%9F%93%8C%E9%A2%84%E7%83%AD"><span class="toc-number">10.5.3.</span> <span class="toc-text">📌预热</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%F0%9F%93%8CRAdam"><span class="toc-number">10.5.4.</span> <span class="toc-text">📌RAdam</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%F0%9F%94%A7%E6%97%A0%E9%9C%80%E9%A2%84%E7%83%AD"><span class="toc-number">10.5.4.1.</span> <span class="toc-text">🔧无需预热</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%F0%9F%94%A7%E4%BC%98%E4%BA%8E%E6%89%8B%E5%8A%A8%E9%A2%84%E7%83%AD"><span class="toc-number">10.5.4.2.</span> <span class="toc-text">🔧优于手动预热</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%F0%9F%94%A7RAdam%E4%B8%8EAdam%E6%80%A7%E8%83%BD%E5%AF%B9%E6%AF%94"><span class="toc-number">10.5.4.3.</span> <span class="toc-text">🔧RAdam与Adam性能对比</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%F0%9F%8E%AF%E5%AD%A6%E4%B9%A0%E7%8E%87%E8%B0%83%E5%BA%A6%E4%B8%8E%E4%BC%98%E5%8C%96%E5%99%A8%E5%8F%98%E5%BD%A2"><span class="toc-number">10.6.</span> <span class="toc-text">🎯学习率调度与优化器变形</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%F0%9F%93%8C%E5%8A%A8%E9%87%8F%E4%B8%8E%E5%9D%87%E6%96%B9%E6%A0%B9%E7%9A%84%E8%AE%A1%E7%AE%97%E5%B7%AE%E5%BC%82"><span class="toc-number">10.6.1.</span> <span class="toc-text">📌动量与均方根的计算差异</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%F0%9F%93%8C%E4%BC%98%E5%8C%96%E6%80%BB%E7%BB%93"><span class="toc-number">10.6.2.</span> <span class="toc-text">📌优化总结</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%F0%9F%9A%A9Task-2-2"><span class="toc-number">11.</span> <span class="toc-text">🚩Task 2.2</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%F0%9F%8E%AF%E4%BB%80%E4%B9%88%E6%98%AF%E5%88%86%E7%B1%BB"><span class="toc-number">11.1.</span> <span class="toc-text">🎯什么是分类</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%F0%9F%8E%AF%E5%9B%9E%E5%BD%92%E4%B8%8E%E5%88%86%E7%B1%BB%E7%9A%84%E5%8C%BA%E5%88%AB%E5%92%8C%E8%81%94%E7%B3%BB"><span class="toc-number">11.2.</span> <span class="toc-text">🎯回归与分类的区别和联系</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%F0%9F%93%8C%E5%9B%9E%E5%BD%92"><span class="toc-number">11.2.1.</span> <span class="toc-text">📌回归</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%F0%9F%93%8C%E5%88%86%E7%B1%BB"><span class="toc-number">11.2.2.</span> <span class="toc-text">📌分类</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%F0%9F%8E%AF%E4%BD%BF%E7%94%A8%E6%95%B0%E5%AD%97%E8%A1%A8%E7%A4%BA%E7%B1%BB%E5%88%AB%E4%BC%9A%E5%87%BA%E7%8E%B0%E7%9A%84%E9%97%AE%E9%A2%98"><span class="toc-number">11.3.</span> <span class="toc-text">🎯使用数字表示类别会出现的问题</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%F0%9F%8E%AF%E4%BB%80%E4%B9%88%E6%98%AF%E7%8B%AC%E7%83%AD%E7%BC%96%E7%A0%81%EF%BC%88One-Hot-Encoding%EF%BC%89"><span class="toc-number">11.4.</span> <span class="toc-text">🎯什么是独热编码（One-Hot Encoding）</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%F0%9F%93%8C%E5%A4%9A%E8%BE%93%E5%87%BA%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84"><span class="toc-number">11.4.1.</span> <span class="toc-text">📌多输出神经网络结构</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%F0%9F%8E%AF%E5%B8%A6%E6%9C%89-softmax-%E7%9A%84%E5%88%86%E7%B1%BB"><span class="toc-number">11.5.</span> <span class="toc-text">🎯带有 softmax 的分类</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%F0%9F%93%8C%E4%B8%BA%E4%BB%80%E4%B9%88%E5%88%86%E7%B1%BB%E8%BF%87%E7%A8%8B%E4%B8%AD%E8%A6%81%E5%8A%A0%E4%B8%8Asoftmax%E5%87%BD%E6%95%B0"><span class="toc-number">11.5.1.</span> <span class="toc-text">📌为什么分类过程中要加上softmax函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%F0%9F%93%8CSoftmax-%E5%87%BD%E6%95%B0%E5%8F%8A%E5%85%B6%E7%89%B9%E6%80%A7"><span class="toc-number">11.5.2.</span> <span class="toc-text">📌Softmax 函数及其特性</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%F0%9F%94%A7%E7%89%B9%E6%80%A7"><span class="toc-number">11.5.2.1.</span> <span class="toc-text">🔧特性</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%F0%9F%93%8CSigmoid-%E5%87%BD%E6%95%B0%E4%B8%8E-Softmax-%E5%87%BD%E6%95%B0%E7%9A%84%E6%AF%94%E8%BE%83"><span class="toc-number">11.5.3.</span> <span class="toc-text">📌Sigmoid 函数与 Softmax 函数的比较</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%F0%9F%94%A7%E4%B8%A4%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98"><span class="toc-number">11.5.3.1.</span> <span class="toc-text">🔧两分类问题</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%F0%9F%94%A7%E5%A4%9A%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98"><span class="toc-number">11.5.3.2.</span> <span class="toc-text">🔧多分类问题</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%F0%9F%8E%AF%E4%BB%80%E4%B9%88%E6%98%AF%E5%88%86%E7%B1%BB%E6%8D%9F%E5%A4%B1"><span class="toc-number">11.6.</span> <span class="toc-text">🎯什么是分类损失</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%F0%9F%93%8C%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="toc-number">11.6.1.</span> <span class="toc-text">📌损失函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%F0%9F%93%8C%E5%9D%87%E6%96%B9%E8%AF%AF%E5%B7%AE-MSE"><span class="toc-number">11.6.2.</span> <span class="toc-text">📌均方误差 (MSE)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%F0%9F%93%8C%E4%BA%A4%E5%8F%89%E7%86%B5-Cross-Entropy"><span class="toc-number">11.6.3.</span> <span class="toc-text">📌交叉熵 (Cross-Entropy)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%F0%9F%93%8C%E4%BD%BF%E7%94%A8-softmax-%E7%9A%84%E5%A5%BD%E5%A4%84"><span class="toc-number">11.6.4.</span> <span class="toc-text">📌使用 softmax 的好处</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%F0%9F%8E%AF%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E7%9A%84%E9%80%89%E6%8B%A9%E5%AF%B9%E4%BC%98%E5%8C%96%E7%9A%84%E5%BD%B1%E5%93%8D"><span class="toc-number">11.7.</span> <span class="toc-text">🎯损失函数的选择对优化的影响</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%F0%9F%93%8C%E4%BA%A4%E5%8F%89%E7%86%B5"><span class="toc-number">11.7.1.</span> <span class="toc-text">📌交叉熵</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%F0%9F%93%8C%E5%9D%87%E6%96%B9%E8%AF%AF%E5%B7%AE"><span class="toc-number">11.7.2.</span> <span class="toc-text">📌均方误差</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%F0%9F%93%8C%E6%80%BB%E7%BB%93"><span class="toc-number">11.7.3.</span> <span class="toc-text">📌总结</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%F0%9F%9A%A9Task-2-3"><span class="toc-number">12.</span> <span class="toc-text">🚩Task 2.3</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%F0%9F%8E%AF%E4%B8%80%E9%94%AE%E8%BF%90%E8%A1%8CNotebook"><span class="toc-number">12.1.</span> <span class="toc-text">🎯一键运行Notebook</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%F0%9F%8E%AF%E5%AE%9E%E9%AA%8C%E7%BB%93%E6%9E%9C"><span class="toc-number">12.2.</span> <span class="toc-text">🎯实验结果</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%F0%9F%9A%A9-Task-3-1"><span class="toc-number">13.</span> <span class="toc-text">🚩 Task 3.1</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%F0%9F%8E%AF%E6%89%B9%E9%87%8F%E5%BD%92%E4%B8%80%E5%8C%96%EF%BC%88Batch-Normalization%EF%BC%8C-BN%EF%BC%89"><span class="toc-number">13.1.</span> <span class="toc-text">🎯批量归一化（Batch Normalization， BN）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%F0%9F%8E%AF%E5%AD%A6%E4%B9%A0%E7%8E%87%E5%92%8C%E8%AF%AF%E5%B7%AE%E8%A1%A8%E9%9D%A2%E4%BC%98%E5%8C%96"><span class="toc-number">13.2.</span> <span class="toc-text">🎯学习率和误差表面优化</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%F0%9F%93%8C%E8%AF%AF%E5%B7%AE%E8%A1%A8%E9%9D%A2%E5%88%86%E6%9E%90"><span class="toc-number">13.2.1.</span> <span class="toc-text">📌误差表面分析</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%F0%9F%93%8C%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B%E7%A4%BA%E4%BE%8B"><span class="toc-number">13.2.2.</span> <span class="toc-text">📌线性模型示例</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%F0%9F%8E%AF%E7%89%B9%E5%BE%81%E5%BD%92%E4%B8%80%E5%8C%96%E7%9A%84%E9%87%8D%E8%A6%81%E6%80%A7"><span class="toc-number">13.3.</span> <span class="toc-text">🎯特征归一化的重要性</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%F0%9F%93%8CZ%E5%80%BC%E5%BD%92%E4%B8%80%E5%8C%96%EF%BC%88%E6%A0%87%E5%87%86%E5%8C%96%EF%BC%89"><span class="toc-number">13.3.1.</span> <span class="toc-text">📌Z值归一化（标准化）</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%F0%9F%8E%AF%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E7%89%B9%E5%BE%81%E5%BD%92%E4%B8%80%E5%8C%96"><span class="toc-number">13.4.</span> <span class="toc-text">🎯深度学习中的特征归一化</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%F0%9F%93%8C%E5%BD%92%E4%B8%80%E5%8C%96%E4%B8%AD%E9%97%B4%E5%B1%82%E7%89%B9%E5%BE%81"><span class="toc-number">13.4.1.</span> <span class="toc-text">📌归一化中间层特征</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%F0%9F%8E%AF%E6%89%B9%E9%87%8F%E5%BD%92%E4%B8%80%E5%8C%96%EF%BC%88Batch-Normalization%EF%BC%89%E5%9C%A8%E6%B5%8B%E8%AF%95%E9%98%B6%E6%AE%B5%E7%9A%84%E5%BA%94%E7%94%A8"><span class="toc-number">13.5.</span> <span class="toc-text">🎯批量归一化（Batch Normalization）在测试阶段的应用</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%F0%9F%93%8C%E4%BD%BF%E7%94%A8%E7%A7%BB%E5%8A%A8%E5%B9%B3%E5%9D%87%E5%92%8C%E6%96%B9%E5%B7%AE"><span class="toc-number">13.5.1.</span> <span class="toc-text">📌使用移动平均和方差</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%F0%9F%94%A7%E4%B8%8D%E4%BE%9D%E8%B5%96%E6%89%B9%E6%AC%A1%E7%BB%9F%E8%AE%A1"><span class="toc-number">13.5.1.1.</span> <span class="toc-text">🔧不依赖批次统计</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%F0%9F%94%A7%E5%87%8F%E5%B0%91%E5%86%85%E9%83%A8%E5%8D%8F%E5%8F%98%E9%87%8F%E5%81%8F%E7%A7%BB"><span class="toc-number">13.5.1.2.</span> <span class="toc-text">🔧减少内部协变量偏移</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%F0%9F%94%A7%E6%8F%90%E9%AB%98%E6%B3%9B%E5%8C%96%E8%83%BD%E5%8A%9B"><span class="toc-number">13.5.1.3.</span> <span class="toc-text">🔧提高泛化能力</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%F0%9F%94%A7%E6%97%A0%E9%9C%80%E8%B0%83%E6%95%B4%E5%AD%A6%E4%B9%A0%E7%8E%87"><span class="toc-number">13.5.1.4.</span> <span class="toc-text">🔧无需调整学习率</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%F0%9F%94%A7%E8%AE%A1%E7%AE%97%E6%95%88%E7%8E%87"><span class="toc-number">13.5.1.5.</span> <span class="toc-text">🔧计算效率</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%F0%9F%94%A7%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2"><span class="toc-number">13.5.1.6.</span> <span class="toc-text">🔧模型部署</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%F0%9F%8E%AF%E5%86%85%E9%83%A8%E5%8D%8F%E5%8F%98%E9%87%8F%E5%81%8F%E7%A7%BB"><span class="toc-number">13.6.</span> <span class="toc-text">🎯内部协变量偏移</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%F0%9F%93%8C%E5%AF%B9%E4%BC%98%E5%8C%96%E7%9A%84%E5%B8%AE%E5%8A%A9"><span class="toc-number">13.6.1.</span> <span class="toc-text">📌对优化的帮助</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%F0%9F%93%8C%E8%AF%AF%E5%B7%AE%E8%A1%A8%E9%9D%A2%E5%B9%B3%E6%BB%91%E5%8C%96"><span class="toc-number">13.6.2.</span> <span class="toc-text">📌误差表面平滑化</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%F0%9F%93%8C%E5%BD%92%E4%B8%80%E5%8C%96%E6%96%B9%E6%B3%95%E7%9A%84%E5%A4%9A%E6%A0%B7%E6%80%A7"><span class="toc-number">13.6.3.</span> <span class="toc-text">📌归一化方法的多样性</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%F0%9F%9A%A9-Task-3-2-Task-3-3"><span class="toc-number">14.</span> <span class="toc-text">🚩 Task 3.2 &amp;&amp; Task 3.3</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%F0%9F%8E%AF%E5%A6%82%E4%BD%95%E6%8A%8A%E5%9B%BE%E5%83%8F%E8%BE%93%E5%85%A5%E5%88%B0%E8%AE%A1%E7%AE%97%E6%9C%BA%E9%87%8C%E9%9D%A2"><span class="toc-number">14.1.</span> <span class="toc-text">🎯如何把图像输入到计算机里面</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%F0%9F%8E%AF%E4%BB%80%E4%B9%88%E6%98%AF%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%9E%B6%E6%9E%84"><span class="toc-number">14.2.</span> <span class="toc-text">🎯什么是卷积神经网络架构</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%F0%9F%8E%AF%E5%9B%BE%E5%83%8F%E8%A1%A8%E7%A4%BA"><span class="toc-number">14.3.</span> <span class="toc-text">🎯图像表示</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%F0%9F%8E%AF%E5%90%91%E9%87%8F%E5%8C%96%E5%A4%84%E7%90%86"><span class="toc-number">14.4.</span> <span class="toc-text">🎯向量化处理</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%F0%9F%8E%AF%E6%A0%87%E5%87%86%E5%8C%96%E5%A4%84%E7%90%86"><span class="toc-number">14.5.</span> <span class="toc-text">🎯标准化处理</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%F0%9F%8E%AF%E5%8D%B7%E7%A7%AF%E5%B1%82%E7%9A%84%E4%BD%9C%E7%94%A8"><span class="toc-number">14.6.</span> <span class="toc-text">🎯卷积层的作用</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%F0%9F%8E%AF%E7%89%B9%E5%BE%81%E5%9B%BE%E7%9A%84%E8%A7%A3%E9%87%8A"><span class="toc-number">14.7.</span> <span class="toc-text">🎯特征图的解释</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%F0%9F%8E%AF%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B"><span class="toc-number">14.8.</span> <span class="toc-text">🎯目标检测</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%F0%9F%8E%AF%E6%84%9F%E5%8F%97%E9%87%8E"><span class="toc-number">14.9.</span> <span class="toc-text">🎯感受野</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%F0%9F%93%8C%E7%A5%9E%E7%BB%8F%E5%85%83%E5%92%8C%E6%9D%83%E9%87%8D"><span class="toc-number">14.9.1.</span> <span class="toc-text">📌神经元和权重</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%F0%9F%93%8C%E5%81%8F%E7%BD%AE%EF%BC%88Bias%EF%BC%89"><span class="toc-number">14.9.2.</span> <span class="toc-text">📌偏置（Bias）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%F0%9F%93%8C%E6%84%9F%E5%8F%97%E9%87%8E%E7%9A%84%E9%87%8D%E5%8F%A0"><span class="toc-number">14.9.3.</span> <span class="toc-text">📌感受野的重叠</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%F0%9F%93%8C%E6%84%9F%E5%8F%97%E9%87%8E%E7%9A%84%E5%A4%A7%E5%B0%8F%E5%92%8C%E5%BD%A2%E7%8A%B6"><span class="toc-number">14.9.4.</span> <span class="toc-text">📌感受野的大小和形状</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%F0%9F%93%8C%E9%80%9A%E9%81%93%E7%9A%84%E9%80%89%E6%8B%A9%E6%80%A7"><span class="toc-number">14.9.5.</span> <span class="toc-text">📌通道的选择性</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%F0%9F%93%8C%E8%AE%BE%E8%AE%A1%E6%84%9F%E5%8F%97%E9%87%8E"><span class="toc-number">14.9.6.</span> <span class="toc-text">📌设计感受野</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%F0%9F%8E%AF%E5%8D%B7%E7%A7%AF%E6%93%8D%E4%BD%9C"><span class="toc-number">14.10.</span> <span class="toc-text">🎯卷积操作</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%F0%9F%93%8C%E6%AD%A5%E5%B9%85"><span class="toc-number">14.10.1.</span> <span class="toc-text">📌步幅</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%F0%9F%93%8C%E5%A1%AB%E5%85%85"><span class="toc-number">14.10.2.</span> <span class="toc-text">📌填充</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%F0%9F%93%8C%E5%8D%B7%E7%A7%AF%E5%B1%82%E7%9A%84%E5%8F%82%E6%95%B0%E5%85%B1%E4%BA%AB"><span class="toc-number">14.10.3.</span> <span class="toc-text">📌卷积层的参数共享</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%F0%9F%93%8C%E5%85%A8%E8%BF%9E%E6%8E%A5%E5%B1%82%EF%BC%88Fully-Connected-Layer%EF%BC%89"><span class="toc-number">14.10.4.</span> <span class="toc-text">📌全连接层（Fully Connected Layer）</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%F0%9F%94%A7%E7%89%B9%E7%82%B9"><span class="toc-number">14.10.4.1.</span> <span class="toc-text">🔧特点</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%F0%9F%94%A7%E8%AE%A1%E7%AE%97%E8%BF%87%E7%A8%8B"><span class="toc-number">14.10.4.2.</span> <span class="toc-text">🔧计算过程</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%F0%9F%94%A7%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0"><span class="toc-number">14.10.4.3.</span> <span class="toc-text">🔧激活函数</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%F0%9F%94%A7%E8%AE%AD%E7%BB%83"><span class="toc-number">14.10.4.4.</span> <span class="toc-text">🔧训练</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%F0%9F%94%A7%E8%BE%93%E5%87%BA"><span class="toc-number">14.10.4.5.</span> <span class="toc-text">🔧输出</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%F0%9F%93%8C%E7%89%B9%E5%BE%81%E6%98%A0%E5%B0%84"><span class="toc-number">14.10.5.</span> <span class="toc-text">📌特征映射</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%F0%9F%93%8C%E5%A4%9A%E5%8D%B7%E7%A7%AF%E6%A0%B8"><span class="toc-number">14.10.6.</span> <span class="toc-text">📌多卷积核</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%F0%9F%93%8C%E9%87%87%E6%A0%B7"><span class="toc-number">14.10.7.</span> <span class="toc-text">📌采样</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%F0%9F%93%8C%E6%B1%87%E8%81%9A"><span class="toc-number">14.10.8.</span> <span class="toc-text">📌汇聚</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%F0%9F%94%A7%E5%85%A8%E5%8D%B7%E7%A7%AF%E7%BD%91%E7%BB%9C%E7%9A%84%E8%B6%8B%E5%8A%BF"><span class="toc-number">14.10.8.1.</span> <span class="toc-text">🔧全卷积网络的趋势</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%F0%9F%94%A7%E6%B1%87%E8%81%9A%E7%9A%84%E4%B8%BB%E8%A6%81%E4%BD%9C%E7%94%A8"><span class="toc-number">14.10.8.2.</span> <span class="toc-text">🔧汇聚的主要作用</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%F0%9F%94%A7%E8%BF%90%E7%AE%97%E8%83%BD%E5%8A%9B%E7%9A%84%E6%8F%90%E5%8D%87"><span class="toc-number">14.10.8.3.</span> <span class="toc-text">🔧运算能力的提升</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%F0%9F%94%A7%E4%B8%80%E8%88%AC%E7%BD%91%E7%BB%9C%E6%9E%B6%E6%9E%84"><span class="toc-number">14.10.8.4.</span> <span class="toc-text">🔧一般网络架构</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%F0%9F%94%A7%E6%9E%B6%E6%9E%84%E7%A4%BA%E4%BE%8B"><span class="toc-number">14.10.8.5.</span> <span class="toc-text">🔧架构示例</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%F0%9F%8E%AF%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%BA%94%E7%94%A8"><span class="toc-number">14.11.</span> <span class="toc-text">🎯卷积神经网络的应用</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%F0%9F%8E%AF%E5%9B%B4%E6%A3%8B%E8%90%BD%E5%AD%90%E9%A2%84%E6%B5%8B"><span class="toc-number">14.12.</span> <span class="toc-text">🎯围棋落子预测</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%F0%9F%9A%A9%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6"><span class="toc-number">15.</span> <span class="toc-text">🚩自注意力机制</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%F0%9F%8E%AF%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%A8%A1%E5%9E%8B%EF%BC%88Self-Attention-Model%EF%BC%89"><span class="toc-number">15.1.</span> <span class="toc-text">🎯自注意力模型（Self-Attention Model）</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%F0%9F%93%8C%E8%BE%93%E5%85%A5%E4%B8%8E%E8%BE%93%E5%87%BA"><span class="toc-number">15.1.1.</span> <span class="toc-text">📌输入与输出</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%F0%9F%93%8C%E5%BA%8F%E5%88%97%E5%A4%84%E7%90%86%E7%9A%84%E6%8C%91%E6%88%98"><span class="toc-number">15.1.2.</span> <span class="toc-text">📌序列处理的挑战</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%F0%9F%93%8C%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6"><span class="toc-number">15.1.3.</span> <span class="toc-text">📌自注意力机制</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%F0%9F%93%8C%E7%8B%AC%E7%83%AD%E7%BC%96%E7%A0%81%E4%B8%8E%E8%AF%8D%E5%B5%8C%E5%85%A5"><span class="toc-number">15.1.4.</span> <span class="toc-text">📌独热编码与词嵌入</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%F0%9F%93%8C%E5%BA%8F%E5%88%97%E5%88%B0%E5%BA%8F%E5%88%97%E7%9A%84%E4%BB%BB%E5%8A%A1"><span class="toc-number">15.1.5.</span> <span class="toc-text">📌序列到序列的任务</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%F0%9F%93%8C%E6%B3%A8%E6%84%8F%E5%8A%9B%E5%88%86%E6%95%B0%E7%9A%84%E8%AE%A1%E7%AE%97"><span class="toc-number">15.1.6.</span> <span class="toc-text">📌注意力分数的计算</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%F0%9F%93%8C%E5%A4%9A%E5%A4%B4%E6%B3%A8%E6%84%8F%E5%8A%9B"><span class="toc-number">15.1.7.</span> <span class="toc-text">📌多头注意力</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%F0%9F%93%8CTransformer-%E6%9E%B6%E6%9E%84"><span class="toc-number">15.1.8.</span> <span class="toc-text">📌Transformer 架构</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%F0%9F%93%8C%E5%BA%94%E7%94%A8%E5%AE%9E%E4%BE%8B"><span class="toc-number">15.1.9.</span> <span class="toc-text">📌应用实例</span></a></li></ol></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2025/09/02/PostgreSQL%E6%8A%80%E6%9C%AF%E5%85%A8%E6%A0%88%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97/" title="PostgreSQL技术全栈实践指南"><img src="/img/cover/9f13ef7ffc274ea1974a8c4bd3849b26_0.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="PostgreSQL技术全栈实践指南"/></a><div class="content"><a class="title" href="/2025/09/02/PostgreSQL%E6%8A%80%E6%9C%AF%E5%85%A8%E6%A0%88%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97/" title="PostgreSQL技术全栈实践指南">PostgreSQL技术全栈实践指南</a><time datetime="2025-09-01T19:46:44.000Z" title="发表于 2025-09-02 03:46:44">2025-09-02</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/09/02/JUnit5%E5%85%A8%E9%9D%A2%E6%8A%80%E6%9C%AF%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97/" title="JUnit5全面技术实践指南"><img src="/img/cover/9f13ef7ffc274ea1974a8c4bd3849b26_3.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="JUnit5全面技术实践指南"/></a><div class="content"><a class="title" href="/2025/09/02/JUnit5%E5%85%A8%E9%9D%A2%E6%8A%80%E6%9C%AF%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97/" title="JUnit5全面技术实践指南">JUnit5全面技术实践指南</a><time datetime="2025-09-01T19:46:32.000Z" title="发表于 2025-09-02 03:46:32">2025-09-02</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/08/18/Linux%E7%B3%BB%E7%BB%9F%E7%AE%A1%E7%90%86%E6%A0%B8%E5%BF%83%E7%9F%A5%E8%AF%86%E7%82%B9%E8%AF%A6%E8%A7%A3/" title="Linux系统管理核心知识点详解"><img src="/img/cover/eca45645431b46fea6db1634a45287b2_2.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Linux系统管理核心知识点详解"/></a><div class="content"><a class="title" href="/2025/08/18/Linux%E7%B3%BB%E7%BB%9F%E7%AE%A1%E7%90%86%E6%A0%B8%E5%BF%83%E7%9F%A5%E8%AF%86%E7%82%B9%E8%AF%A6%E8%A7%A3/" title="Linux系统管理核心知识点详解">Linux系统管理核心知识点详解</a><time datetime="2025-08-18T15:46:10.000Z" title="发表于 2025-08-18 23:46:10">2025-08-18</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/08/18/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E6%A0%B8%E5%BF%83%E7%9F%A5%E8%AF%86%E7%82%B9%E8%AF%A6%E8%A7%A3/" title="计算机网络核心知识点详解"><img src="/img/cover/eca45645431b46fea6db1634a45287b2_3.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="计算机网络核心知识点详解"/></a><div class="content"><a class="title" href="/2025/08/18/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E6%A0%B8%E5%BF%83%E7%9F%A5%E8%AF%86%E7%82%B9%E8%AF%A6%E8%A7%A3/" title="计算机网络核心知识点详解">计算机网络核心知识点详解</a><time datetime="2025-08-18T15:33:50.000Z" title="发表于 2025-08-18 23:33:50">2025-08-18</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/08/18/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%E7%AC%94%E8%AF%95%E6%A0%B8%E5%BF%83%E7%9F%A5%E8%AF%86%E7%82%B9%E8%AF%A6%E8%A7%A3/" title="数据结构与算法笔试核心知识点详解"><img src="/img/cover/eca45645431b46fea6db1634a45287b2_1.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="数据结构与算法笔试核心知识点详解"/></a><div class="content"><a class="title" href="/2025/08/18/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%E7%AC%94%E8%AF%95%E6%A0%B8%E5%BF%83%E7%9F%A5%E8%AF%86%E7%82%B9%E8%AF%A6%E8%A7%A3/" title="数据结构与算法笔试核心知识点详解">数据结构与算法笔试核心知识点详解</a><time datetime="2025-08-18T14:54:27.000Z" title="发表于 2025-08-18 22:54:27">2025-08-18</time></div></div></div></div></div></div></main><footer id="footer" style="background-image: url(/img/cover/footer-bg.jpg);"><div id="footer-wrap"><div class="copyright">&copy;2019 - 2025 By CanJisam</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo 7.3.0</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly 5.3.5</a></div><div class="footer_custom_text"><div class="footer-custom">感谢您的访问，希望这里的内容对您有所帮助！<br><a href="https://beian.miit.gov.cn/" target="_blank">备案号：粤ICP备XXXXXXXX号</a></div></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="简繁转换">繁</button><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="前往评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><div class="js-pjax"><script>(() => {
  const runMermaid = ele => {
    window.loadMermaid = true
    const theme = document.documentElement.getAttribute('data-theme') === 'dark' ? 'dark' : 'default'

    ele.forEach((item, index) => {
      const mermaidSrc = item.firstElementChild
      const mermaidThemeConfig = `%%{init:{ 'theme':'${theme}'}}%%\n`
      const mermaidID = `mermaid-${index}`
      const mermaidDefinition = mermaidThemeConfig + mermaidSrc.textContent

      const renderFn = mermaid.render(mermaidID, mermaidDefinition)
      const renderMermaid = svg => {
        mermaidSrc.insertAdjacentHTML('afterend', svg)
      }

      // mermaid v9 and v10 compatibility
      typeof renderFn === 'string' ? renderMermaid(renderFn) : renderFn.then(({ svg }) => renderMermaid(svg))
    })
  }

  const codeToMermaid = () => {
    const codeMermaidEle = document.querySelectorAll('pre > code.mermaid')
    if (codeMermaidEle.length === 0) return

    codeMermaidEle.forEach(ele => {
      const preEle = document.createElement('pre')
      preEle.className = 'mermaid-src'
      preEle.hidden = true
      preEle.textContent = ele.textContent
      const newEle = document.createElement('div')
      newEle.className = 'mermaid-wrap'
      newEle.appendChild(preEle)
      ele.parentNode.replaceWith(newEle)
    })
  }

  const loadMermaid = () => {
    if (true) codeToMermaid()
    const $mermaid = document.querySelectorAll('#article-container .mermaid-wrap')
    if ($mermaid.length === 0) return

    const runMermaidFn = () => runMermaid($mermaid)
    btf.addGlobalFn('themeChange', runMermaidFn, 'mermaid')
    window.loadMermaid ? runMermaidFn() : btf.getScript('https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js').then(runMermaidFn)
  }

  btf.addGlobalFn('encrypt', loadMermaid, 'mermaid')
  window.pjax ? loadMermaid() : document.addEventListener('DOMContentLoaded', loadMermaid)
})()</script><script>(() => {
  const isShuoshuo = GLOBAL_CONFIG_SITE.pageType === 'shuoshuo'
  const option = {"distractionFreeMode":true}

  const commentCount = n => {
    const isCommentCount = document.querySelector('#post-meta .gitalk-comment-count')
    if (isCommentCount) {
      isCommentCount.textContent= n
    }
  }

  const initGitalk = (el, path) => {
    if (isShuoshuo) {
      window.shuoshuoComment.destroyGitalk = () => {
        if (el.children.length) {
          el.innerHTML = ''
          el.classList.add('no-comment')
        }
      }
    }

    const gitalk = new Gitalk({
      clientID: 'Ov23lifFntnoMvOg1Fir',
      clientSecret: '0596c9896ad9c699f2188c823bce11b2d3d231e6',
      repo: 'canjisam.github.io',
      owner: 'canjisam',
      admin: ['canjisam'],
      updateCountCallback: commentCount,
      ...option,
      id: isShuoshuo ? path : (option && option.id) || 'cdb99dcdedb33fcfa9bb2e61d16ac262'
    })

    gitalk.render('gitalk-container')
  }

  const loadGitalk = async(el, path) => {
    if (typeof Gitalk === 'function') initGitalk(el, path)
    else {
      await btf.getCSS('https://cdn.jsdelivr.net/npm/gitalk/dist/gitalk.min.css')
      await btf.getScript('https://cdn.jsdelivr.net/npm/gitalk/dist/gitalk.min.js')
      initGitalk(el, path)
    }
  }

  if (isShuoshuo) {
    'Gitalk' === 'Gitalk'
      ? window.shuoshuoComment = { loadComment: loadGitalk }
      : window.loadOtherComment = loadGitalk
    return
  }

  if ('Gitalk' === 'Gitalk' || !false) {
    if (false) btf.loadComment(document.getElementById('gitalk-container'), loadGitalk)
    else loadGitalk()
  } else {
    window.loadOtherComment = loadGitalk
  }
})()</script></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.js"></script><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/metingjs/dist/Meting.min.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="text-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章..." type="text"/></div></div><hr/><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js"></script></div></div></body></html>